{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXFKWclwj1k1"
      },
      "source": [
        "# 1. Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install eli5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghpFbC8E5G0m",
        "outputId": "4613f828-f9f1-4e5d-c65e-2b450bcf8788"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting eli5\n",
            "  Downloading eli5-0.13.0.tar.gz (216 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/216.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m194.6/216.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>17.1.0 in /usr/local/lib/python3.10/dist-packages (from eli5) (23.1.0)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from eli5) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from eli5) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from eli5) (1.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from eli5) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from eli5) (1.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from eli5) (0.20.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from eli5) (0.8.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.0->eli5) (2.1.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->eli5) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->eli5) (3.1.0)\n",
            "Building wheels for collected packages: eli5\n",
            "  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for eli5: filename=eli5-0.13.0-py2.py3-none-any.whl size=107730 sha256=92b94c27cb4801325ef8141427c93ab937f0eec395412cd2f133e9b062b351bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/58/ef/2cf4c306898c2338d51540e0922c8e0d6028e07007085c0004\n",
            "Successfully built eli5\n",
            "Installing collected packages: eli5\n",
            "Successfully installed eli5-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lscK3B8NDsz5"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2CzYx6yHWB8"
      },
      "source": [
        "df = pd.read_csv(\"patientml.csv\")\n",
        "df = df.rename(columns = {'DEGREE_CENTRALITY':'Degree Centrality', 'EIGENVECTOR':'Eigenvector Centrality ', 'CLOSENESS_CENTRALITY': 'Closeness Centrality', 'BETWEENNESS_CENTRALITY' :'Betweenness Centrality', 'CLUSTCOEF': 'Clustering Coefficient', 'gender':'Gender', 'age':'Age', 'smoker':'Smoking'})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdoO8QqYe7tF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d894c4d0-86d8-4d79-b0bd-a93f962f794c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ID  Degree centrality  Eigenvector  Closness centrality  \\\n",
              "0  200132255           0.000973     0.000005             0.263413   \n",
              "1       7673           0.004380     0.000421             0.333830   \n",
              "2  200066270           0.056448     0.004889             0.415296   \n",
              "3     140445           0.008759     0.000841             0.365491   \n",
              "4     115693           0.006813     0.000098             0.319032   \n",
              "\n",
              "   Betweeness Centrality  Clustering coefficient  Gender       Age  Smoking  \\\n",
              "0               0.000000                1.000000       1  0.504348        1   \n",
              "1               0.000000                1.000000       1  0.573913        1   \n",
              "2               0.000659                0.666417       0  0.695652        0   \n",
              "3               0.000047                0.777778       1  0.347826        0   \n",
              "4               0.002725                0.560440       0  0.391304        1   \n",
              "\n",
              "   Outcome  \n",
              "0        0  \n",
              "1        1  \n",
              "2        0  \n",
              "3        0  \n",
              "4        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ccabd50-8f30-4df1-bcf3-aad078c0e9b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Degree centrality</th>\n",
              "      <th>Eigenvector</th>\n",
              "      <th>Closness centrality</th>\n",
              "      <th>Betweeness Centrality</th>\n",
              "      <th>Clustering coefficient</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Smoking</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200132255</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.263413</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.504348</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7673</td>\n",
              "      <td>0.004380</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.333830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.573913</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>200066270</td>\n",
              "      <td>0.056448</td>\n",
              "      <td>0.004889</td>\n",
              "      <td>0.415296</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.666417</td>\n",
              "      <td>0</td>\n",
              "      <td>0.695652</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>140445</td>\n",
              "      <td>0.008759</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.365491</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>1</td>\n",
              "      <td>0.347826</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>115693</td>\n",
              "      <td>0.006813</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.319032</td>\n",
              "      <td>0.002725</td>\n",
              "      <td>0.560440</td>\n",
              "      <td>0</td>\n",
              "      <td>0.391304</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ccabd50-8f30-4df1-bcf3-aad078c0e9b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ccabd50-8f30-4df1-bcf3-aad078c0e9b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ccabd50-8f30-4df1-bcf3-aad078c0e9b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNpFlxsRj476"
      },
      "source": [
        "# 2. Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POw_zerYeeGA"
      },
      "source": [
        "X = df.iloc[:, 1:-1]\n",
        "y = df.iloc[:, -1:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuRCm_33kDtE"
      },
      "source": [
        "## 2.1 Train Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgNqpKYDk0Kg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=2)\n",
        "feature_names = X_test.columns.tolist()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuEqc2KYoxHb"
      },
      "source": [
        "\n",
        "\n",
        "# 3 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPLqMeM0oxHc"
      },
      "source": [
        "## 3.1 LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loX9s55JduPJ",
        "outputId": "23885616-8c85-446e-9a79-4c6b474c0157"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=10, random_state=2, shuffle=True)\n",
        "# create model\n",
        "LR = LogisticRegression()\n",
        "# evaluate model\n",
        "scores = cross_val_score(LR, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy for training: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "LR.fit(X_train,y_train)\n",
        "y_pred = LR.predict(X_test)\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_LR = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "#plot_confusion_matrix(LR, X_test, y_test)\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for training: 0.7348 (0.0375)\n",
            "Accuracy for testing  0.7427184466019418\n",
            "[[151  62]\n",
            " [ 44 155]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7744    0.7089    0.7402       213\n",
            "           1     0.7143    0.7789    0.7452       199\n",
            "\n",
            "    accuracy                         0.7427       412\n",
            "   macro avg     0.7443    0.7439    0.7427       412\n",
            "weighted avg     0.7453    0.7427    0.7426       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlV7tZ6hLLtB"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF2cJbYFEPUi",
        "outputId": "7f84415a-abae-44fd-9cbb-f3ec9ac36c30"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "r = permutation_importance(LR, X_test, y_test,\n",
        "                        n_repeats=30,\n",
        "                         random_state=0)\n",
        "\n",
        "r.importances_mean"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.50323625e-02,  1.16504854e-02,  4.86245955e-02,  0.00000000e+00,\n",
              "        1.35841424e-01,  2.28964401e-02,  5.13754045e-02, -8.09061489e-05])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "hPdMfdN9FPc7",
        "outputId": "026f5881-40fe-482f-9a97-106c344cccc4"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm_LR = PermutationImportance(LR, random_state=0).fit(X_test, y_test)\n",
        "eli5.show_weights(perm_LR, feature_names = X_test.columns.tolist())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1398\n",
              "                \n",
              "                    &plusmn; 0.0181\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 89.87%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0529\n",
              "                \n",
              "                    &plusmn; 0.0244\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0447\n",
              "                \n",
              "                    &plusmn; 0.0231\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.56%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0408\n",
              "                \n",
              "                    &plusmn; 0.0169\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 94.29%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0233\n",
              "                \n",
              "                    &plusmn; 0.0139\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.09%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0136\n",
              "                \n",
              "                    &plusmn; 0.0109\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 99.62%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0005\n",
              "                \n",
              "                    &plusmn; 0.0036\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzfquKrEoxHd"
      },
      "source": [
        "# 3.2 KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-5ckumxAZRC",
        "outputId": "1ce852c8-70c4-4e5b-9cda-24d56ef40fae"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import neighbors\n",
        "params = {'n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],\n",
        "          'weights':['uniform','distance'],\n",
        "         }\n",
        "\n",
        "knn = neighbors.KNeighborsRegressor()\n",
        "\n",
        "model = GridSearchCV(knn, params, cv=10)\n",
        "model.fit(X_train,y_train)\n",
        "model.best_params_"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_neighbors': 12, 'weights': 'uniform'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APuH08kUfnWv",
        "outputId": "1fe9c794-c0fa-417e-db8a-31e0352ac0e0"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "cv = KFold(n_splits=10, random_state=2, shuffle=True)\n",
        "# create model\n",
        "KNN = KNeighborsClassifier(n_neighbors=12, weights = \"uniform\")\n",
        "# evaluate model\n",
        "scores = cross_val_score(KNN, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "KNN.fit(X_train,y_train)\n",
        "y_pred = KNN.predict(X_test)\n",
        "\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_KNN = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7738 (0.0323)\n",
            "Accuracy for testing  0.8131067961165048\n",
            "[[193  20]\n",
            " [ 57 142]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7720    0.9061    0.8337       213\n",
            "           1     0.8765    0.7136    0.7867       199\n",
            "\n",
            "    accuracy                         0.8131       412\n",
            "   macro avg     0.8243    0.8098    0.8102       412\n",
            "weighted avg     0.8225    0.8131    0.8110       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXG2QhNeL6S1"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "uJKtp5cXG8P3",
        "outputId": "bb8c0a44-53d6-40c9-c96d-66c869def4b3"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm_KNN = PermutationImportance(KNN, random_state=0).fit(X_test, y_test)\n",
        "eli5.show_weights(perm_KNN, feature_names = X_test.columns.tolist())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1602\n",
              "                \n",
              "                    &plusmn; 0.0226\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 84.47%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1117\n",
              "                \n",
              "                    &plusmn; 0.0251\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 88.48%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0728\n",
              "                \n",
              "                    &plusmn; 0.0213\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.07%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0427\n",
              "                \n",
              "                    &plusmn; 0.0205\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 94.96%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0223\n",
              "                \n",
              "                    &plusmn; 0.0108\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.44%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0136\n",
              "                \n",
              "                    &plusmn; 0.0136\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 99.09%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0019\n",
              "                \n",
              "                    &plusmn; 0.0036\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R0uLP_5oxHd"
      },
      "source": [
        "## 3.3 SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsUS0LXVgSxJ",
        "outputId": "fe1b2045-6c75-43b4-8b8d-33218b77ef88"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "\n",
        "cv = KFold(n_splits=10, random_state=2, shuffle=True)\n",
        "# create model\n",
        "SVM = svm.SVC(probability=True)\n",
        "# evaluate model\n",
        "scores = cross_val_score(SVM, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "SVM.fit(X_train,y_train)\n",
        "y_pred = SVM.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_SVM = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7512 (0.0316)\n",
            "Accuracy for testing  0.7864077669902912\n",
            "[[169  44]\n",
            " [ 44 155]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7934    0.7934    0.7934       213\n",
            "           1     0.7789    0.7789    0.7789       199\n",
            "\n",
            "    accuracy                         0.7864       412\n",
            "   macro avg     0.7862    0.7862    0.7862       412\n",
            "weighted avg     0.7864    0.7864    0.7864       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "n2hcCq2RHK7H",
        "outputId": "8b60aef9-baf9-430a-a84a-4e73aa21e25e"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm_SVM = PermutationImportance(SVM, random_state=0).fit(X_test, y_test)\n",
        "eli5.show_weights(perm_SVM, feature_names = X_test.columns.tolist())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1607\n",
              "                \n",
              "                    &plusmn; 0.0207\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 85.12%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1053\n",
              "                \n",
              "                    &plusmn; 0.0343\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 88.56%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0723\n",
              "                \n",
              "                    &plusmn; 0.0099\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.21%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0417\n",
              "                \n",
              "                    &plusmn; 0.0135\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 94.23%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0272\n",
              "                \n",
              "                    &plusmn; 0.0155\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.10%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0155\n",
              "                \n",
              "                    &plusmn; 0.0050\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.81%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0117\n",
              "                \n",
              "                    &plusmn; 0.0149\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 100.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0\n",
              "                \n",
              "                    &plusmn; 0.0000\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BREZ8p-oxHe"
      },
      "source": [
        "## 3.4 NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG2Kb2d_gf4C",
        "outputId": "83c0a223-a614-4b2f-aaa6-d34a4afdaf4f"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "cv = KFold(n_splits=10, random_state=2, shuffle=True)\n",
        "\n",
        "\n",
        "NB = GaussianNB()\n",
        "# evaluate model\n",
        "scores = cross_val_score(NB, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "NB.fit(X_train,y_train)\n",
        "y_pred = NB.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_NB = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6473 (0.0396)\n",
            "Accuracy for testing  0.6529126213592233\n",
            "[[192  21]\n",
            " [122  77]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6115    0.9014    0.7287       213\n",
            "           1     0.7857    0.3869    0.5185       199\n",
            "\n",
            "    accuracy                         0.6529       412\n",
            "   macro avg     0.6986    0.6442    0.6236       412\n",
            "weighted avg     0.6956    0.6529    0.6272       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "Af5xq9mwHakn",
        "outputId": "00648bd7-a7e1-4f31-e47c-e0e852f359ba"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm_NB= PermutationImportance(NB, random_state=0).fit(X_test, y_test)\n",
        "eli5.show_weights(perm_NB, feature_names = X_test.columns.tolist())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 90.21%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0107\n",
              "                \n",
              "                    &plusmn; 0.0265\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.16%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0092\n",
              "                \n",
              "                    &plusmn; 0.0120\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.16%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0078\n",
              "                \n",
              "                    &plusmn; 0.0078\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.86%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0068\n",
              "                \n",
              "                    &plusmn; 0.0190\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 93.97%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0053\n",
              "                \n",
              "                    &plusmn; 0.0036\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 92.16%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                -0.0078\n",
              "                \n",
              "                    &plusmn; 0.0036\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 90.52%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                -0.0102\n",
              "                \n",
              "                    &plusmn; 0.0099\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                -0.0296\n",
              "                \n",
              "                    &plusmn; 0.0104\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4_tSn83oxHe"
      },
      "source": [
        "# 3.5 DT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIYB4KS3hR9A",
        "outputId": "15004b8c-f06d-46f4-fb18-e082bf260f8f"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "cv = KFold(n_splits=10, random_state=2, shuffle=True)\n",
        "DT = DecisionTreeClassifier(random_state=1)\n",
        "scores = cross_val_score(DT, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "DT.fit(X_train,y_train)\n",
        "y_pred = DT.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_DT = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7464 (0.0329)\n",
            "Accuracy for testing  0.808252427184466\n",
            "[[172  41]\n",
            " [ 38 161]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8190    0.8075    0.8132       213\n",
            "           1     0.7970    0.8090    0.8030       199\n",
            "\n",
            "    accuracy                         0.8083       412\n",
            "   macro avg     0.8080    0.8083    0.8081       412\n",
            "weighted avg     0.8084    0.8083    0.8083       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "etlBJHUrHmRp",
        "outputId": "53c3fa4f-3c88-4af6-eb32-3141037df437"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm_DT= PermutationImportance(DT, random_state=0).fit(X_test, y_test)\n",
        "eli5.show_weights(perm_DT, feature_names = X_test.columns.tolist())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.2092\n",
              "                \n",
              "                    &plusmn; 0.0520\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 84.34%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1476\n",
              "                \n",
              "                    &plusmn; 0.0224\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 85.18%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1364\n",
              "                \n",
              "                    &plusmn; 0.0265\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 88.23%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0981\n",
              "                \n",
              "                    &plusmn; 0.0368\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 90.81%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0689\n",
              "                \n",
              "                    &plusmn; 0.0225\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 95.89%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0218\n",
              "                \n",
              "                    &plusmn; 0.0248\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 96.08%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0204\n",
              "                \n",
              "                    &plusmn; 0.0139\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 98.28%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0063\n",
              "                \n",
              "                    &plusmn; 0.0050\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPFdizoRoxHf"
      },
      "source": [
        "# 3.6 RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ZicgXGDBck"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc=RandomForestClassifier(random_state=2)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0VAYflRC36o"
      },
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth' : [6,7,8,9],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqh8P9IIC-W0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30a232d8-bfaa-4122-83e4-a04b3c95063a"
      },
      "source": [
        "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10)\n",
        "CV_rfc.fit(X_train, y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, estimator=RandomForestClassifier(random_state=2),\n",
              "             param_grid={'criterion': ['gini', 'entropy'],\n",
              "                         'max_depth': [6, 7, 8, 9],\n",
              "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
              "                         'n_estimators': [100, 200, 500]})"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=RandomForestClassifier(random_state=2),\n",
              "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
              "                         &#x27;max_depth&#x27;: [6, 7, 8, 9],\n",
              "                         &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
              "                         &#x27;n_estimators&#x27;: [100, 200, 500]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=RandomForestClassifier(random_state=2),\n",
              "             param_grid={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;],\n",
              "                         &#x27;max_depth&#x27;: [6, 7, 8, 9],\n",
              "                         &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
              "                         &#x27;n_estimators&#x27;: [100, 200, 500]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=2)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=2)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuO52AFug3JR",
        "outputId": "5004f36c-6761-4ca9-8484-0d9310c7cf07"
      },
      "source": [
        "cv = KFold(n_splits=10, random_state=2, shuffle=True)\n",
        "\n",
        "RF = RandomForestClassifier(criterion='gini', max_depth=8,\n",
        "                                              max_features='auto',\n",
        "                                              n_estimators=200, random_state=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(RF,X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "RF.fit(X_train,y_train)\n",
        "y_pred = RF.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_RF = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8145 (0.0266)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for testing  0.8519417475728155\n",
            "[[201  12]\n",
            " [ 49 150]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8040    0.9437    0.8683       213\n",
            "           1     0.9259    0.7538    0.8310       199\n",
            "\n",
            "    accuracy                         0.8519       412\n",
            "   macro avg     0.8650    0.8487    0.8496       412\n",
            "weighted avg     0.8629    0.8519    0.8503       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0evCbv7D_dds"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "elABqDioH7OB",
        "outputId": "4d095e3e-824f-4614-b51d-de1bd44c713e"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm_RF= PermutationImportance(RF, random_state=0).fit(X_test, y_test)\n",
        "eli5.show_weights(perm_RF, feature_names = X_test.columns.tolist())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1306\n",
              "                \n",
              "                    &plusmn; 0.0117\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 86.34%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0757\n",
              "                \n",
              "                    &plusmn; 0.0207\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 88.90%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0563\n",
              "                \n",
              "                    &plusmn; 0.0207\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.82%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0364\n",
              "                \n",
              "                    &plusmn; 0.0087\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.13%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0345\n",
              "                \n",
              "                    &plusmn; 0.0166\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.84%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0301\n",
              "                \n",
              "                    &plusmn; 0.0198\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 98.95%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0019\n",
              "                \n",
              "                    &plusmn; 0.0135\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 99.60%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0005\n",
              "                \n",
              "                    &plusmn; 0.0019\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdAdo6xI5v0L",
        "outputId": "ef58f741-481c-401c-9fc1-f0ab593ba455"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "\n",
        "rf=RandomForestClassifier(criterion='entropy', max_depth=8,\n",
        "                                              max_features='auto',\n",
        "                                              n_estimators=200, random_state=1)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print('Training set metrics:')\n",
        "print('Accuracy:', accuracy_score(y_train, rf.predict(X_train)))\n",
        "print('Precision:', precision_score(y_train, rf.predict(X_train)))\n",
        "print('Recall:', recall_score(y_train, rf.predict(X_train)))\n",
        "\n",
        "print('Test set metrics:')\n",
        "print('Accuracy:', accuracy_score(y_test, rf.predict(X_test)))\n",
        "print('Precision:', precision_score(y_test, rf.predict(X_test)))\n",
        "print('Recall:', recall_score(y_test, rf.predict(X_test)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set metrics:\n",
            "Accuracy: 0.8777372262773723\n",
            "Precision: 0.9422535211267605\n",
            "Recall: 0.8069963811821471\n",
            "Test set metrics:\n",
            "Accuracy: 0.8495145631067961\n",
            "Precision: 0.9202453987730062\n",
            "Recall: 0.7537688442211056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtOcWQ2wu1xd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Calculate feature importances\n",
        "importances = RF.feature_importances_\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Rearrange feature names so they match the sorted feature importances\n",
        "names = [X.columns[i] for i in indices]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "u6OktzAcR5M6",
        "outputId": "76f865d9-d444-4fa7-8e0d-5197d0eb26d2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Calculate feature importances\n",
        "importances = RF.feature_importances_\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Rearrange feature names so they match the sorted feature importances\n",
        "names = [X.columns[i] for i in indices]\n",
        "\n",
        "# Barplot: Add bars\n",
        "plt.barh(range(X.shape[1]), importances[indices], color='grey')\n",
        "# Add feature names as x-axis labels\n",
        "plt.yticks(range(X.shape[1]), names, fontsize = 10)\n",
        "# Create plot title\n",
        "#plt.title(\"Feature Importance\")\n",
        "plt.savefig('Figure5.png', dpi=300, bbox_inches = 'tight')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inzbkyyxoxHi"
      },
      "source": [
        "# 3.7 XGboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQgS35w9oxHi"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvLXDndpoxHj"
      },
      "source": [
        "from xgboost.sklearn import XGBClassifier\n",
        "import scipy.stats as st\n",
        "\n",
        "one_to_left = st.beta(10, 1)\n",
        "from_zero_positive = st.expon(0, 50)\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\": st.randint(3, 40),\n",
        "    \"max_depth\": st.randint(3, 40),\n",
        "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
        "    \"colsample_bytree\": one_to_left,\n",
        "    \"subsample\": one_to_left,\n",
        "    \"gamma\": st.uniform(0, 10),\n",
        "    'reg_alpha': from_zero_positive,\n",
        "    \"min_child_weight\": from_zero_positive,\n",
        "}\n",
        "\n",
        "xgb = XGBClassifier(nthreads=-1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPcoX01IyYdF"
      },
      "source": [
        "X = df.iloc[:, 1:-1]\n",
        "y = df.iloc[:, -1:]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHON559QswsP"
      },
      "source": [
        "X_train = X_train.values\n",
        "y_train = y_train.values\n",
        "X_test = X_test.values\n",
        "y_test = y_test.values"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CFx_RfB1oxHj",
        "outputId": "48c38322-1b61-4e0f-93b0-b00c68aba056"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "gs = RandomizedSearchCV(xgb, params, n_jobs=1, random_state=1)\n",
        "gs.fit(X_train, y_train)\n",
        "gs.best_estimator_"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[07:02:35] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:36] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:36] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:36] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:36] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:37] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:37] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:37] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:38] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:38] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:38] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:38] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:38] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:38] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:38] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:39] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:40] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n",
            "[07:02:41] WARNING: ../src/learner.cc:767: \n",
            "Parameters: { \"nthreads\" } are not used.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.989870219416199, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=4.140559878195683, gpu_id=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.32776006309109806, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
              "              min_child_weight=38.89175481553912, missing=nan,\n",
              "              monotone_constraints=None, n_estimators=16, n_jobs=None,\n",
              "              nthreads=-1, num_parallel_tree=None, predictor=None, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.989870219416199, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=4.140559878195683, gpu_id=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.32776006309109806, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
              "              min_child_weight=38.89175481553912, missing=nan,\n",
              "              monotone_constraints=None, n_estimators=16, n_jobs=None,\n",
              "              nthreads=-1, num_parallel_tree=None, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=0.989870219416199, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=4.140559878195683, gpu_id=None, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=0.32776006309109806, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
              "              min_child_weight=38.89175481553912, missing=nan,\n",
              "              monotone_constraints=None, n_estimators=16, n_jobs=None,\n",
              "              nthreads=-1, num_parallel_tree=None, predictor=None, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOecBfHQEtWE",
        "outputId": "633f629f-cb20-4b05-9aa7-e15cd2bfd075"
      },
      "source": [
        "\"\"\"\n",
        "####### witout network feature\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=0.9692206821226456,\n",
        "              gamma=0.3142918568673425, learning_rate=0.30456416450551216,\n",
        "              max_delta_step=0, max_depth=34,\n",
        "              min_child_weight=59.456851110666285, missing=None,\n",
        "              n_estimators=39, n_jobs=1, nthread=None, nthreads=-1,\n",
        "              objective='binary:logistic', random_state=0,\n",
        "              reg_alpha=14.336941149164916, reg_lambda=1, scale_pos_weight=1,\n",
        "              seed=None, silent=None, subsample=0.898377733005299, verbosity=1)\n",
        "# evaluate model\n",
        "scores = cross_val_score(model,X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_XGBoost = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6637 (0.0341)\n",
            "Accuracy for testing  0.6586538461538461\n",
            "[[106 106]\n",
            " [ 36 168]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7465    0.5000    0.5989       212\n",
            "           1     0.6131    0.8235    0.7029       204\n",
            "\n",
            "    accuracy                         0.6587       416\n",
            "   macro avg     0.6798    0.6618    0.6509       416\n",
            "weighted avg     0.6811    0.6587    0.6499       416\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20AnIMU1oxHk",
        "outputId": "5c90ec98-8282-4c9d-f387-58958df37819"
      },
      "source": [
        "\n",
        "cv = KFold(n_splits=10, random_state=2, shuffle=True)\n",
        "\n",
        "xgb = XGBClassifier()\n",
        "# evaluate model\n",
        "scores = cross_val_score(xgb,X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.4f (%.4f)' % (mean(scores), std(scores)))\n",
        "\n",
        "y_train=np.ravel(y_train)\n",
        "xgb.fit(X_train,y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Accuracy for testing ', accuracy_score(y_test, y_pred))\n",
        "\n",
        "cf_matrix_XGBoost = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8017 (0.0308)\n",
            "Accuracy for testing  0.8398058252427184\n",
            "[[183  30]\n",
            " [ 36 163]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8356    0.8592    0.8472       213\n",
            "           1     0.8446    0.8191    0.8316       199\n",
            "\n",
            "    accuracy                         0.8398       412\n",
            "   macro avg     0.8401    0.8391    0.8394       412\n",
            "weighted avg     0.8399    0.8398    0.8397       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "2gfNGeb_PDTX",
        "outputId": "3be994c1-c70e-4499-af4c-199b14c5a394"
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "perm_xgb= PermutationImportance(xgb, random_state=0).fit(X_test, y_test)\n",
        "perm_xgb.fit(X_test, y_test)\n",
        "eli5.show_weights(perm_xgb, feature_names=feature_names)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.2063\n",
              "                \n",
              "                    &plusmn; 0.0323\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.65%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0592\n",
              "                \n",
              "                    &plusmn; 0.0105\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 91.99%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0558\n",
              "                \n",
              "                    &plusmn; 0.0269\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.58%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0500\n",
              "                \n",
              "                    &plusmn; 0.0223\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 92.94%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0466\n",
              "                \n",
              "                    &plusmn; 0.0169\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 95.91%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0214\n",
              "                \n",
              "                    &plusmn; 0.0149\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.40%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0112\n",
              "                \n",
              "                    &plusmn; 0.0225\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 98.99%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0029\n",
              "                \n",
              "                    &plusmn; 0.0019\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRZYl-EooxHk"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_tree\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OffUpkeyoxHm",
        "outputId": "1b17faa8-6984-4970-bb2c-a826700e88de"
      },
      "source": [
        "xgb.feature_importances_"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03598718, 0.2191312 , 0.07815997, 0.2679952 , 0.24476038,\n",
              "       0.04504747, 0.10891863, 0.        ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "hQHmhaqosIi3",
        "outputId": "0d6b1ef1-9356-4417-c111-dbba128e32c5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Calculate feature importances\n",
        "importances = xgb.feature_importances_\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Rearrange feature names so they match the sorted feature importances\n",
        "names = [X.columns[i] for i in indices]\n",
        "\n",
        "# Barplot: Add bars\n",
        "plt.barh(range(X.shape[1]), importances[indices])\n",
        "# Add feature names as x-axis labels\n",
        "plt.yticks(range(X.shape[1]), names, fontsize = 10)\n",
        "# Create plot title\n",
        "#plt.title(\"Feature Importance\")\n",
        "plt.savefig('Figure7.png', dpi=300, bbox_inches = 'tight')\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqkAAAGdCAYAAAAxJerhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNK0lEQVR4nO3deXhN1+L/8c+RyElkFEMSGmIIQhOSGi5uxRBNFKVVtFRFDR0MpXRIXVOL0GqLKlWUpNXSqotrJhUq1YqqmFJDKqaGtEUiXEFyfn/4OV/nSgiVZifer+c5z3X2WnuttdfJPfl07SEmi8ViEQAAAGAgpYp6AAAAAMD/IqQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAzHvqgHANyt3Nxc/fbbb3J1dZXJZCrq4QAAgAKwWCw6f/68KlWqpFKl8l8vJaSi2Prtt9/k6+tb1MMAAAB34fjx43rggQfyLSekothydXWVdO2H3M3NrYhHAwAACiIzM1O+vr7W3+P5IaSi2Lp+it/NzY2QCgBAMXO7S/W4cQoAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSUSAmk0nLli3Lt9zPz09Tp07928YDAABKNkJqMfP777/rxRdfVJUqVWQ2m+Xt7a3w8HAlJCQU6bgSExM1YMCAIh0DAAAoOeyLegC4M126dNHly5cVExOj6tWr6/Tp04qLi9Off/5ZpOOqUKFCkfYPAABKFlZSi5Fz587pu+++0+TJk9WqVStVrVpVjRs3VlRUlB577DFJ107Lz549Wx06dFCZMmUUEBCgbdu26fDhw2rZsqWcnZ3VrFkzpaSk2LQ9a9Ys1ahRQw4ODqpdu7Y+++yzW45lzJgx8vHx0e7duyXdfLrfZDJp7ty5evzxx1WmTBn5+/trxYoVNm2sWLFC/v7+cnR0VKtWrRQTEyOTyaRz58799ckCAADFGiG1GHFxcZGLi4uWLVum7OzsfOu9/fbbevbZZ7Vr1y7VqVNHPXr00PPPP6+oqCjt2LFDFotFgwYNstb/97//rZdfflnDhw/X3r179fzzz6tPnz7atGnTTW1bLBYNHjxYsbGx+u677xQUFJTvOMaNG6du3bpp9+7devTRR9WzZ0+dOXNGknTkyBE9+eST6ty5s5KSkvT8889r5MiRtzz+7OxsZWZm2rwAAEAJZUGxsmTJEkvZsmUtjo6OlmbNmlmioqIsSUlJ1nJJln/961/W99u2bbNIssybN8+67csvv7Q4Ojpa3zdr1szSv39/m366du1qefTRR23a/frrry09evSwBAQEWE6cOGFTv2rVqpYPPvgg33FkZWVZJFnWrFljsVgsltdff93y4IMP2rQxcuRIiyTL2bNn8zz2MWPGWCTd9MrIyMhvugAAgMFkZGQU6Pc3K6nFTJcuXfTbb79pxYoVioiIUHx8vEJCQrRgwQJrnRtXN728vCRJgYGBNtsuXbpkXYlMTk5W8+bNbfpp3ry5kpOTbbYNGzZMP/74o7Zs2aLKlSvfdqw3jsPZ2Vlubm5KT0+XJB04cECNGjWyqd+4ceNbthcVFaWMjAzr6/jx47cdAwAAKJ4IqcWQo6Oj2rZtq1GjRun7779XZGSkxowZYy0vXbq09d8mkynfbbm5uXfUb9u2bXXy5EmtW7euQPVv7PN6v3fa543MZrPc3NxsXgAAoGQipJYAdevW1YULF+56/4CAgJseYZWQkKC6devabHvsscf0xRdfqF+/flq0aNFd9ydJtWvX1o4dO2y2JSYm/qU2AQBAycEjqIqRP//8U127dtVzzz2noKAgubq6aseOHXrnnXfUqVOnu2731VdfVbdu3RQcHKywsDD95z//0dKlS7Vx48ab6j7++OP67LPP1KtXL9nb2+vJJ5+8qz6ff/55vf/++3r99dfVt29f7dq1y3rJwvWVXgAAcP8ipBYjLi4uatKkiT744AOlpKToypUr8vX1Vf/+/fXmm2/edbudO3fWtGnTNGXKFL388suqVq2a5s+fr5YtW+ZZ/8knn1Rubq569eqlUqVK6YknnrjjPqtVq6YlS5Zo+PDhmjZtmpo2baqRI0fqxRdflNlsvutjAQAAJYPJYrFYinoQgCRNmDBBH3/8cYFviMrMzJS7u7syMjK4PhUAgGKioL+/WUlFkZk5c6YaNWqkcuXKKSEhQe+++67N81sBAMD9i5CKInPo0CGNHz9eZ86cUZUqVTR8+HBFRUUV9bAAAIABcLofxRan+wEAKH4K+vubR1ABAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDsS/qAQB/1YNj1qmUuUxRD+NvlzqpfVEPAQCAQsNKKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipuCdMJpOWLVsmSUpNTZXJZNKuXbuKdEwAAKD4KrYhNTIyUiaTSSaTSaVLl5aXl5fatm2rTz/9VLm5uUU9PMPz8/PT1KlTC6VtX19fpaWl6cEHH5QkxcfHy2Qy6dy5c4XSHwAAKHmKbUiVpIiICKWlpSk1NVVr1qxRq1at9PLLL6tDhw66evVqofZ9+fLlQm3fCHJycu4q8NvZ2cnb21v29vaFMCoAAHA/KNYh1Ww2y9vbW5UrV1ZISIjefPNNLV++XGvWrNGCBQus9c6dO6d+/fqpQoUKcnNzU+vWrZWUlGTT1vjx41WxYkW5urqqX79+euONN9SgQQNreWRkpDp37qwJEyaoUqVKql27tiTp+PHj6tatmzw8POTp6alOnTopNTXVpu25c+cqICBAjo6OqlOnjmbOnHnL48rNzdU777yjmjVrymw2q0qVKpowYYK1/HZ9Xh/rlClT5OPjo3LlymngwIG6cuWKJKlly5Y6evSohg0bZl2NlqQFCxbIw8NDK1asUN26dWU2m3Xs2DElJiaqbdu2Kl++vNzd3RUaGqqdO3fmO/4bT/enpqaqVatWkqSyZcvKZDIpMjJSsbGxKleunLKzs2327dy5s3r16nXL+QEAACVfsQ6peWndurXq16+vpUuXWrd17dpV6enpWrNmjX766SeFhISoTZs2OnPmjCRp4cKFmjBhgiZPnqyffvpJVapU0axZs25qOy4uTgcOHNCGDRu0cuVKXblyReHh4XJ1ddV3332nhIQEubi4KCIiwrrSunDhQo0ePVoTJkxQcnKyJk6cqFGjRikmJibfY4iKitKkSZM0atQo7d+/X1988YW8vLwkqUB9StKmTZuUkpKiTZs2KSYmRgsWLLAG96VLl+qBBx7QW2+9pbS0NKWlpVn3u3jxoiZPnqy5c+dq3759qlixos6fP6/evXtr69at+uGHH+Tv769HH31U58+fv+3n4evrq2+++UaSdODAAaWlpWnatGnq2rWrcnJytGLFCmvd9PR0rVq1Ss8991yebWVnZyszM9PmBQAASqYSeT62Tp062r17tyRp69at2r59u9LT02U2myVJU6ZM0bJly7RkyRINGDBAH374ofr27as+ffpIkkaPHq3169crKyvLpl1nZ2fNnTtXDg4OkqTPP/9cubm5mjt3rnU1cv78+fLw8FB8fLweeeQRjRkzRu+9956eeOIJSVK1atW0f/9+zZ49W717975p7OfPn9e0adM0Y8YMa3mNGjX0z3/+U5K0ePHi2/YpXVu1nDFjhuzs7FSnTh21b99ecXFx6t+/vzw9PWVnZydXV1d5e3vb9H/lyhXNnDlT9evXt25r3bq1TZ1PPvlEHh4e2rx5szp06HDLz8LOzk6enp6SpIoVK8rDw8Na1qNHD82fP19du3a1zmeVKlXUsmXLPNuKjo7WuHHjbtkfAAAoGUrcSqokWSwWa4BLSkpSVlaWypUrJxcXF+vryJEjSklJkXRtha9x48Y2bfzve0kKDAy0BtTrbR8+fFiurq7Wdj09PXXp0iWlpKTowoULSklJUd++fW36Hj9+vLXv/5WcnKzs7Gy1adMmz/Lb9XldvXr1ZGdnZ33v4+Oj9PT0286dg4ODgoKCbLadPn1a/fv3l7+/v9zd3eXm5qasrCwdO3bstu3dSv/+/bV+/XqdPHlS0rXLDa7fEJeXqKgoZWRkWF/Hjx//S/0DAADjKpErqcnJyapWrZokKSsrSz4+PoqPj7+p3o2regXh7Oxs8z4rK0sPPfSQFi5ceFPdChUqWFdi58yZoyZNmtiU3xggb+Tk5HTLMdyuz+tKly5tU2YymQp0E5STk9NNIbF37976888/NW3aNFWtWlVms1lNmzb9yzePBQcHq379+oqNjdUjjzyiffv2adWqVfnWN5vN1tVwAABQspW4kPrtt99qz549GjZsmCQpJCREp06dkr29vfz8/PLcp3bt2kpMTNSzzz5r3ZaYmHjbvkJCQrR48WJVrFhRbm5uN5W7u7urUqVK+vXXX9WzZ88Cjd/f319OTk6Ki4tTv3797rjPgnJwcFBOTk6B6iYkJGjmzJl69NFHJV27ceuPP/64o74k5dlfv379NHXqVJ08eVJhYWHy9fUtcLsAAKDkKtan+7Ozs3Xq1CmdPHlSO3fu1MSJE9WpUyd16NDBGjjDwsLUtGlTde7cWevXr1dqaqq+//57jRw5Ujt27JAkDR48WPPmzVNMTIwOHTqk8ePHa/fu3fmedr6uZ8+eKl++vDp16qTvvvtOR44cUXx8vIYMGaITJ05IksaNG6fo6GhNnz5dBw8e1J49ezR//ny9//77ebbp6Oio119/Xa+99ppiY2OVkpKiH374QfPmzStwnwXh5+enLVu26OTJk7cNnP7+/vrss8+UnJysH3/8UT179rztiu+NqlatKpPJpJUrV+r333+3uda3R48eOnHihObMmZPvDVMAAOD+U6xD6tq1a+Xj4yM/Pz9FRERo06ZNmj59upYvX249nW4ymbR69Wq1aNFCffr0Ua1atfTUU0/p6NGj1jvme/bsqaioKI0YMUIhISE6cuSIIiMj5ejoeMv+y5Qpoy1btqhKlSp64oknFBAQoL59++rSpUvWVc5+/fpp7ty5mj9/vgIDAxUaGqoFCxZYL0fIy6hRozR8+HCNHj1aAQEB6t69u/V60oL0WRBvvfWWUlNTVaNGDZvLBPIyb948nT17ViEhIerVq5eGDBmiihUrFrivypUra9y4cXrjjTfk5eWlQYMGWcvc3d3VpUsXubi4qHPnzgVuEwAAlGwmi8ViKepBGFHbtm3l7e2tzz77rKiHUuK1adNG9erV0/Tp0+9ov8zMTLm7u8t36FcqZS5TSKMzrtRJ7Yt6CAAA3LHrv78zMjJuucBW4q5JvRsXL17Uxx9/rPDwcNnZ2enLL7/Uxo0btWHDhqIeWol29uxZxcfHKz4+/rZ/4AAAANxfCKn6v0sCJkyYoEuXLql27dr65ptvFBYWVtRDK9GCg4N19uxZTZ482foXvAAAACRCqqRrj13auHFjUQ/jvvO/fz4WAADgumJ94xQAAABKJkIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeQCgAAAMPhOako9vaOC7/ln1UDAADFDyupAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcHiYP4q9B8esUylzmaIeRrGTOql9UQ8BAIB8sZIKAAAAwyGkAgAAwHAIqQAAADAcQioAAAAMh5AKAAAAwyGkAgAAwHAIqQAAADAcQioAAAAMh5AKAAAAwyGkAgAAwHAIqQAAADAcQiruuZYtW2ro0KFFPQwAAFCMEVJLqFOnTunll19WzZo15ejoKC8vLzVv3lyzZs3SxYsXi3p4AAAAt2Rf1APAvffrr7+qefPm8vDw0MSJExUYGCiz2aw9e/bok08+UeXKlfXYY48V9TDzlZOTI5PJpFKl+G8oAADuV6SAEuill16Svb29duzYoW7duikgIEDVq1dXp06dtGrVKnXs2FGSdO7cOfXr108VKlSQm5ubWrduraSkJGs7Y8eOVYMGDfTZZ5/Jz89P7u7ueuqpp3T+/HlrnQsXLujZZ5+Vi4uLfHx89N577900nuzsbI0YMUKVK1eWs7OzmjRpovj4eGv5ggUL5OHhoRUrVqhu3boym806duxY4U0QAAAwPEJqCfPnn39q/fr1GjhwoJydnfOsYzKZJEldu3ZVenq61qxZo59++kkhISFq06aNzpw5Y62bkpKiZcuWaeXKlVq5cqU2b96sSZMmWctfffVVbd68WcuXL9f69esVHx+vnTt32vQ3aNAgbdu2TYsWLdLu3bvVtWtXRURE6NChQ9Y6Fy9e1OTJkzV37lzt27dPFStWvGnc2dnZyszMtHkBAICSidP9Jczhw4dlsVhUu3Ztm+3ly5fXpUuXJEkDBw5Ux44dtX37dqWnp8tsNkuSpkyZomXLlmnJkiUaMGCAJCk3N1cLFiyQq6urJKlXr16Ki4vThAkTlJWVpXnz5unzzz9XmzZtJEkxMTF64IEHrP0eO3ZM8+fP17Fjx1SpUiVJ0ogRI7R27VrNnz9fEydOlCRduXJFM2fOVP369fM9tujoaI0bN+5eTBMAADA4Qup9Yvv27crNzVXPnj2VnZ2tpKQkZWVlqVy5cjb1/vvf/yolJcX63s/PzxpQJcnHx0fp6emSrq2yXr58WU2aNLGWe3p62gTkPXv2KCcnR7Vq1bLpJzs726ZvBwcHBQUF3fIYoqKi9Morr1jfZ2ZmytfXtyCHDwAAihlCaglTs2ZNmUwmHThwwGZ79erVJUlOTk6SpKysLPn4+NhcG3qdh4eH9d+lS5e2KTOZTMrNzS3weLKysmRnZ6effvpJdnZ2NmUuLi7Wfzs5OVkvQ8iP2Wy2rvoCAICSjZBawpQrV05t27bVjBkzNHjw4HyvSw0JCdGpU6dkb28vPz+/u+qrRo0aKl26tH788UdVqVJFknT27FkdPHhQoaGhkqTg4GDl5OQoPT1dDz/88F31AwAA7j/cOFUCzZw5U1evXlXDhg21ePFiJScn68CBA/r888/1yy+/yM7OTmFhYWratKk6d+6s9evXKzU1Vd9//71GjhypHTt2FKgfFxcX9e3bV6+++qq+/fZb7d27V5GRkTaPjqpVq5Z69uypZ599VkuXLtWRI0e0fft2RUdHa9WqVYU1BQAAoJhjJbUEqlGjhn7++WdNnDhRUVFROnHihMxms+rWrasRI0bopZdekslk0urVqzVy5Ej16dNHv//+u7y9vdWiRQt5eXkVuK93331XWVlZ6tixo1xdXTV8+HBlZGTY1Jk/f77Gjx+v4cOH6+TJkypfvrz+8Y9/qEOHDvf60AEAQAlhslgslqIeBHA3MjMz5e7uLt+hX6mUuUxRD6fYSZ3UvqiHAAC4D13//Z2RkSE3N7d863G6HwAAAIZDSAUAAIDhEFIBAABgOIRUAAAAGA4hFQAAAIZDSAUAAIDhEFIBAABgOIRUAAAAGA4hFQAAAIZDSAUAAIDh2Bf1AIC/au+48Fv+WTUAAFD8sJIKAAAAwyGkAgAAwHAIqQAAADAcQioAAAAMh5AKAAAAwyGkAgAAwHAIqQAAADAcQioAAAAMh4f5o9h7cMw6lTKXKephwMBSJ7Uv6iEAAO4QK6kAAAAwHEIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeQCgAAAMMptiHVZDJp2bJlRT0M/H9+fn6aOnWq9T2fDwAA+CsMGVJPnTqlwYMHq3r16jKbzfL19VXHjh0VFxdX1EMrMVq2bKmhQ4cWWvtpaWlq166dJCk1NVUmk0m7du0qtP4AAEDJYl/UA/hfqampat68uTw8PPTuu+8qMDBQV65c0bp16zRw4ED98ssvRT3E+4bFYlFOTo7s7e/8x8Tb27sQRgQAAO4XhltJfemll2QymbR9+3Z16dJFtWrVUr169fTKK6/ohx9+yHe/PXv2qHXr1nJyclK5cuU0YMAAZWVlWcvj4+PVuHFjOTs7y8PDQ82bN9fRo0clSWPHjlWDBg302Wefyc/PT+7u7nrqqad0/vx56/65ubmKjo5WtWrV5OTkpPr162vJkiXW8rNnz6pnz56qUKGCnJyc5O/vr/nz50uSLl++rEGDBsnHx0eOjo6qWrWqoqOjbzkPn376qerVqyez2SwfHx8NGjTIWnbu3Dn169dPFSpUkJubm1q3bq2kpCRr+e2OJzIyUps3b9a0adNkMplkMpmUmpqq+Ph4mUwmrVmzRg899JDMZrO2bt2qlJQUderUSV5eXnJxcVGjRo20cePGW47/xtP91apVkyQFBwfLZDKpZcuW2rJli0qXLq1Tp07Z7Dd06FA9/PDDt2wbAACUfIYKqWfOnNHatWs1cOBAOTs731Tu4eGR534XLlxQeHi4ypYtq8TERH399dfauHGjNdhdvXpVnTt3VmhoqHbv3q1t27ZpwIABMplM1jZSUlK0bNkyrVy5UitXrtTmzZs1adIka3l0dLRiY2P18ccfa9++fRo2bJieeeYZbd68WZI0atQo7d+/X2vWrFFycrJmzZql8uXLS5KmT5+uFStW6KuvvtKBAwe0cOFC+fn55TsPs2bN0sCBAzVgwADt2bNHK1asUM2aNa3lXbt2VXp6utasWaOffvpJISEhatOmjc6cOVOg45k2bZqaNm2q/v37Ky0tTWlpafL19bXu+8Ybb2jSpElKTk5WUFCQsrKy9OijjyouLk4///yzIiIi1LFjRx07dizfY7jR9u3bJUkbN25UWlqali5dqhYtWqh69er67LPPrPWuXLmihQsX6rnnnsuznezsbGVmZtq8AABAyWSo0/2HDx+WxWJRnTp17mi/L774QpcuXVJsbKw13M6YMUMdO3bU5MmTVbp0aWVkZKhDhw6qUaOGJCkgIMCmjdzcXC1YsECurq6SpF69eikuLk4TJkxQdna2Jk6cqI0bN6pp06aSpOrVq2vr1q2aPXu2QkNDdezYMQUHB6thw4aSZBNCjx07Jn9/f/3zn/+UyWRS1apVb3k848eP1/Dhw/Xyyy9btzVq1EiStHXrVm3fvl3p6ekym82SpClTpmjZsmVasmSJBgwYcNvjcXd3l4ODg8qUKZPnafm33npLbdu2tb739PRU/fr1re/ffvtt/fvf/9aKFStsVnjzU6FCBUlSuXLlbPrr27ev5s+fr1dffVWS9J///EeXLl1St27d8mwnOjpa48aNu21/AACg+DPUSqrFYrmr/ZKTk1W/fn2b1dfmzZsrNzdXBw4ckKenpyIjIxUeHq6OHTtq2rRpSktLs2nDz8/PGugkycfHR+np6ZKuheeLFy+qbdu2cnFxsb5iY2OVkpIiSXrxxRe1aNEiNWjQQK+99pq+//57a1uRkZHatWuXateurSFDhmj9+vX5Hkt6erp+++03tWnTJs/ypKQkZWVlqVy5cjZjOXLkiHUstzue27ketK/LysrSiBEjFBAQIA8PD7m4uCg5ObnAK6n5iYyM1OHDh62XcSxYsEDdunXLcxVdkqKiopSRkWF9HT9+/C/1DwAAjMtQK6n+/v4ymUyFcnPU/PnzNWTIEK1du1aLFy/Wv/71L23YsEH/+Mc/JEmlS5e2qW8ymZSbmytJ1mtbV61apcqVK9vUu76a2a5dOx09elSrV6/Whg0b1KZNGw0cOFBTpkxRSEiIjhw5ojVr1mjjxo3q1q2bwsLCbK5pvc7JyemWx5GVlSUfHx/Fx8ffVHbj5RC3Op7b+d+QOGLECG3YsEFTpkxRzZo15eTkpCeffFKXL18uUHv5qVixojp27Kj58+erWrVqWrNmTZ7HdZ3ZbLbONwAAKNkMtZLq6emp8PBwffTRR7pw4cJN5efOnctzv4CAACUlJdnsk5CQoFKlSql27drWbcHBwYqKitL333+vBx98UF988UWBxlW3bl2ZzWYdO3ZMNWvWtHndeC1nhQoV1Lt3b33++eeaOnWqPvnkE2uZm5ubunfvrjlz5mjx4sX65ptvbK4hvc7V1VV+fn75Pm4rJCREp06dkr29/U1juX4NbEE4ODgoJyenQHUTEhIUGRmpxx9/XIGBgfL29lZqauod9SUpz/769eunxYsX65NPPlGNGjXUvHnzArcLAABKLkOFVEn66KOPlJOTo8aNG+ubb77RoUOHlJycrOnTp1uvB/1fPXv2lKOjo3r37q29e/dq06ZNGjx4sHr16iUvLy8dOXJEUVFR2rZtm44ePar169fr0KFDN12Xmh9XV1eNGDFCw4YNU0xMjFJSUrRz5059+OGHiomJkSSNHj1ay5cv1+HDh7Vv3z6tXLnS2v7777+vL7/8Ur/88osOHjyor7/+Wt7e3vneCDZ27Fi99957mj59ug4dOmTtS5LCwsLUtGlTde7cWevXr1dqaqq+//57jRw5Ujt27CjwPPv5+enHH39Uamqq/vjjj1uusvr7+2vp0qXatWuXkpKS1KNHjwKvykrXVkydnJy0du1anT59WhkZGday8PBwubm5afz48erTp0+B2wQAACWb4UJq9erVtXPnTrVq1UrDhw/Xgw8+qLZt2youLk6zZs3Kc58yZcpo3bp1OnPmjBo1aqQnn3xSbdq00YwZM6zlv/zyi/WRVgMGDNDAgQP1/PPPF3hcb7/9tkaNGqXo6GgFBAQoIiJCq1atsj5eycHBQVFRUQoKClKLFi1kZ2enRYsWSboWct955x01bNhQjRo1UmpqqlavXq1SpfKe/t69e2vq1KmaOXOm6tWrpw4dOujQoUOSrp22X716tVq0aKE+ffqoVq1aeuqpp3T06FF5eXkV+HhGjBghOzs71a1bVxUqVLjl9aXvv/++ypYtq2bNmqljx44KDw9XSEhIgfuyt7fX9OnTNXv2bFWqVEmdOnWylpUqVUqRkZHKycnRs88+W+A2AQBAyWay3O3dSsA90rdvX/3+++9asWLFHe2XmZkpd3d3+Q79SqXMZQppdCgJUie1L+ohAAD+v+u/vzMyMuTm5pZvPUPdOIX7S0ZGhvbs2aMvvvjijgMqAAAo2QipKDKdOnXS9u3b9cILL9g8lxUAAICQiiJzq8dNAQCA+5vhbpwCAAAACKkAAAAwHEIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIfnpKLY2zsu/JZ/Vg0AABQ/rKQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADD4WH+KPYeHLNOpcxlinoYwN8udVL7oh4CABQaVlIBAABgOIRUAAAAGA4hFQAAAIZDSAUAAIDhEFIBAABgOIRUAAAAGA4hFQAAAIZDSAUAAIDhEFIBAABgOIRUAAAAGA4hFQAAAIZDSAUAAIDhEFJxV7Zt2yY7Ozu1b9++qIcCAABKIEIq7sq8efM0ePBgbdmyRb/99ltRDwcAAJQwhFTcsaysLC1evFgvvvii2rdvrwULFtiUr1ixQv7+/nJ0dFSrVq0UExMjk8mkc+fOWets3bpVDz/8sJycnOTr66shQ4bowoULf++BAAAAwyKk4o599dVXqlOnjmrXrq1nnnlGn376qSwWiyTpyJEjevLJJ9W5c2clJSXp+eef18iRI232T0lJUUREhLp06aLdu3dr8eLF2rp1qwYNGnTLfrOzs5WZmWnzAgAAJRMhFXds3rx5euaZZyRJERERysjI0ObNmyVJs2fPVu3atfXuu++qdu3aeuqppxQZGWmzf3R0tHr27KmhQ4fK399fzZo10/Tp0xUbG6tLly7l2290dLTc3d2tL19f30I7RgAAULQIqbgjBw4c0Pbt2/X0009Lkuzt7dW9e3fNmzfPWt6oUSObfRo3bmzzPikpSQsWLJCLi4v1FR4ertzcXB05ciTfvqOiopSRkWF9HT9+/B4fHQAAMAr7oh4Aipd58+bp6tWrqlSpknWbxWKR2WzWjBkzCtRGVlaWnn/+eQ0ZMuSmsipVquS7n9lsltlsvvNBAwCAYoeQigK7evWqYmNj9d577+mRRx6xKevcubO+/PJL1a5dW6tXr7YpS0xMtHkfEhKi/fv3q2bNmoU+ZgAAUDwRUlFgK1eu1NmzZ9W3b1+5u7vblHXp0kXz5s3TV199pffff1+vv/66+vbtq127dlnv/jeZTJKk119/Xf/4xz80aNAg9evXT87Oztq/f782bNhQ4NVYAABQsnFNKgps3rx5CgsLuymgStdC6o4dO3T+/HktWbJES5cuVVBQkGbNmmW9u//6qfqgoCBt3rxZBw8e1MMPP6zg4GCNHj3a5hICAABwfzNZrj87CCgkEyZM0Mcff3zPb3TKzMy8dpf/0K9UylzmnrYNFAepk/iLbwCKn+u/vzMyMuTm5pZvPU73456bOXOmGjVqpHLlyikhIUHvvvvubZ+BCgAAcCNCKu65Q4cOafz48Tpz5oyqVKmi4cOHKyoqqqiHBQAAihFCKu65Dz74QB988EFRDwMAABRj3DgFAAAAwyGkAgAAwHAIqQAAADAcQioAAAAMh5AKAAAAwyGkAgAAwHAIqQAAADAcnpOKYm/vuPBb/lk1AABQ/LCSCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeH+aPYe3DMOpUylynqYQAAikDqpPZFPQQUElZSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gtAiaTScuWLSvqYQAAABgWIbUQREZGymQy3fSKiIiQJKWlpaldu3ZFPMp7LzIyUp07dy7qYQAAgBLAvqgHUFJFRERo/vz5NtvMZrMkydvbuyiGVGxcvnxZDg4ORT0MAABQhFhJLSRms1ne3t42r7Jly0q6+XT/999/rwYNGsjR0VENGzbUsmXLZDKZtGvXLmudvXv3ql27dnJxcZGXl5d69eqlP/74w1resmVLDRkyRK+99po8PT3l7e2tsWPHWst79Oih7t2724zxypUrKl++vGJjYyVJubm5io6OVrVq1eTk5KT69etryZIlNvvs27dPHTp0kJubm1xdXfXwww8rJSVFY8eOVUxMjJYvX25dOY6Pj5ck7dmzR61bt5aTk5PKlSunAQMGKCsry9rm9RXYCRMmqFKlSqpdu/ZfmXoAAFACEFKLWGZmpjp27KjAwEDt3LlTb7/9tl5//XWbOufOnVPr1q0VHBysHTt2aO3atTp9+rS6detmUy8mJkbOzs768ccf9c477+itt97Shg0bJEk9e/bUf/7zH5twuG7dOl28eFGPP/64JCk6OlqxsbH6+OOPtW/fPg0bNkzPPPOMNm/eLEk6efKkWrRoIbPZrG+//VY//fSTnnvuOV29elUjRoxQt27dFBERobS0NKWlpalZs2a6cOGCwsPDVbZsWSUmJurrr7/Wxo0bNWjQIJuxx8XF6cCBA9qwYYNWrlyZ51xlZ2crMzPT5gUAAEomTvcXkpUrV8rFxcVm25tvvqk333zTZtsXX3whk8mkOXPmyNHRUXXr1tXJkyfVv39/a50ZM2YoODhYEydOtG779NNP5evrq4MHD6pWrVqSpKCgII0ZM0aS5O/vrxkzZiguLk5t27ZVeHi4nJ2d9e9//1u9evWy9v3YY4/J1dVV2dnZmjhxojZu3KimTZtKkqpXr66tW7dq9uzZCg0N1UcffSR3d3ctWrRIpUuXliRr35Lk5OSk7Oxsm8sZYmJidOnSJcXGxsrZ2dl6PB07dtTkyZPl5eUlSXJ2dtbcuXNveZo/Ojpa48aNK8j0AwCAYo6QWkhatWqlWbNm2Wzz9PS8qd6BAwcUFBQkR0dH67bGjRvb1ElKStKmTZtuCr2SlJKSYhNSb+Tj46P09HRJkr29vbp166aFCxeqV69eunDhgpYvX65FixZJkg4fPqyLFy+qbdu2Nm1cvnxZwcHBkqRdu3bp4YcftgbUgkhOTlb9+vWtAVWSmjdvrtzcXB04cMAaUgMDA297HWpUVJReeeUV6/vMzEz5+voWeCwAAKD4IKQWEmdnZ9WsWfOetJWVlWVdefxfPj4+1n//b3g0mUzKzc21vu/Zs6dCQ0OVnp6uDRs2yMnJyfrEgeuXAaxatUqVK1e2aef6DV9OTk735HjycmOIzY/ZbLaOBQAAlGyE1CJWu3Ztff7558rOzrYGsMTERJs6ISEh+uabb+Tn5yd7+7v/yJo1ayZfX18tXrxYa9asUdeuXa3Btm7dujKbzTp27JhCQ0Pz3D8oKEgxMTG6cuVKnqupDg4OysnJsdkWEBCgBQsW6MKFC9YgmpCQoFKlSnGDFAAAyBc3ThWS7OxsnTp1yuZ149341/Xo0UO5ubkaMGCAkpOTtW7dOk2ZMkXStZVQSRo4cKDOnDmjp59+WomJiUpJSdG6devUp0+fm0Lh7fTo0UMff/yxNmzYoJ49e1q3u7q6asSIERo2bJhiYmKUkpKinTt36sMPP1RMTIwkadCgQcrMzNRTTz2lHTt26NChQ/rss8904MABSZKfn592796tAwcO6I8//tCVK1fUs2dPOTo6qnfv3tq7d682bdqkwYMHq1evXtZT/QAAAP+LkFpI1q5dKx8fH5vXP//5z5vqubm56T//+Y927dqlBg0aaOTIkRo9erQkWa9TrVSpkhISEpSTk6NHHnlEgYGBGjp0qDw8PFSq1J19hD179tT+/ftVuXJlNW/e3Kbs7bff1qhRoxQdHa2AgABFRERo1apVqlatmiSpXLly+vbbb5WVlaXQ0FA99NBDmjNnjnVVtX///qpdu7YaNmyoChUqKCEhQWXKlNG6det05swZNWrUSE8++aTatGmjGTNm3PGcAgCA+4fJYrFYinoQsLVw4UL16dNHGRkZhXodaHGXmZkpd3d3+Q79SqXMZYp6OACAIpA6qX1RDwF36Prv74yMDLm5ueVbj2tSDSA2NlbVq1dX5cqVlZSUpNdff13dunUjoAIAgPsWIdUATp06pdGjR+vUqVPy8fFR165dNWHChKIeFgAAQJEhpBrAa6+9ptdee62ohwEAAGAY3DgFAAAAwyGkAgAAwHAIqQAAADAcQioAAAAMh5AKAAAAwyGkAgAAwHAIqQAAADAcnpOKYm/vuPBb/lk1AABQ/LCSCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeH+aPYe3DMOpUylynqYQAAUCCpk9oX9RCKBVZSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGUyQh1WQyadmyZUXR9W2NHTtWDRo0KOph3BOnTp1S27Zt5ezsLA8Pj3y33cnnUZLmBwAAGNc9D6mnTp3S4MGDVb16dZnNZvn6+qpjx46Ki4u7111JkuLj42UymXTu3Ll70t6IESMKbax/tw8++EBpaWnatWuXDh48mO+2tLQ0tWvXrkBtFsb8LFiwwBqYAQAAJMn+XjaWmpqq5s2by8PDQ++++64CAwN15coVrVu3TgMHDtQvv/xyL7u7pywWi3JycuTi4iIXF5eiHs49kZKSooceekj+/v633Obt7V3gNkvS/AAAAOO6pyupL730kkwmk7Zv364uXbqoVq1aqlevnl555RX98MMPee6T10rorl27ZDKZlJqaKkk6evSoOnbsqLJly8rZ2Vn16tXT6tWrlZqaqlatWkmSypYtK5PJpMjISElSbm6uoqOjVa1aNTk5Oal+/fpasmTJTf2uWbNGDz30kMxms7Zu3XrT6ezIyEh17txZU6ZMkY+Pj8qVK6eBAwfqypUr1jppaWlq3769nJycVK1aNX3xxRfy8/PT1KlTbzlfn376qerVqyez2SwfHx8NGjTIWnbs2DF16tRJLi4ucnNzU7du3XT69Gmb/ZcvX66QkBA5OjqqevXqGjdunK5evSpJ8vPz0zfffKPY2FjrvOS1Tbr5dP+JEyf09NNPy9PTU87OzmrYsKF+/PFHSXmf7p87d64CAgLk6OioOnXqaObMmday1NRUmUwmLV26VK1atVKZMmVUv359bdu2zfo59OnTRxkZGTKZTDKZTBo7duwt5w0AAJR892wl9cyZM1q7dq0mTJggZ2fnm8r/yuncgQMH6vLly9qyZYucnZ21f/9+ubi4yNfXV9988426dOmiAwcOyM3NTU5OTpKk6Ohoff755/r444/l7++vLVu26JlnnlGFChUUGhpqbfuNN97QlClTVL16dZUtW1bx8fE39b9p0yb5+Pho06ZNOnz4sLp3764GDRqof//+kqRnn31Wf/zxh+Lj41W6dGm98sorSk9Pv+UxzZo1S6+88oomTZqkdu3aKSMjQwkJCZKuBezrAXXz5s26evWqBg4cqO7du1vH99133+nZZ5/V9OnT9fDDDyslJUUDBgyQJI0ZM0aJiYl69tln5ebmpmnTpsnJyUmXL1++adv/ysrKUmhoqCpXrqwVK1bI29tbO3fuVG5ubp7HsXDhQo0ePVozZsxQcHCwfv75Z/Xv31/Ozs7q3bu3td7IkSM1ZcoU+fv7a+TIkXr66ad1+PBhNWvWTFOnTtXo0aN14MABScp3pTY7O1vZ2dnW95mZmbecYwAAUHzds5B6+PBhWSwW1alT5141aXXs2DF16dJFgYGBkqTq1atbyzw9PSVJFStWtAbh7OxsTZw4URs3blTTpk2t+2zdulWzZ8+2CalvvfWW2rZte8v+y5YtqxkzZsjOzk516tRR+/btFRcXp/79++uXX37Rxo0blZiYqIYNG0q6trJ44+n0vIwfP17Dhw/Xyy+/bN3WqFEjSVJcXJz27NmjI0eOyNfXV5IUGxurevXqKTExUY0aNdK4ceP0xhtvWINg9erV9fbbb+u1117TmDFjVKFCBZnNZjk5Odmczs9r242++OIL/f7770pMTLTObc2aNfM9jjFjxui9997TE088IUmqVq2a9u/fr9mzZ9uE1BEjRqh9+/aSpHHjxqlevXo6fPiw6tSpI3d3d5lMpttedhAdHa1x48bdsg4AACgZ7llItVgs96qpmwwZMkQvvvii1q9fr7CwMHXp0kVBQUH51j98+LAuXrx4U/i8fPmygoODbbZdD5a3Uq9ePdnZ2Vnf+/j4aM+ePZKkAwcOyN7eXiEhIdbymjVrqmzZsvm2l56ert9++01t2rTJszw5OVm+vr7WgCpJdevWlYeHh5KTk9WoUSMlJSUpISFBEyZMsNbJycnRpUuXdPHiRZUpU+a2x5WXXbt2KTg42BpQb+XChQtKSUlR3759ravKknT16lW5u7vb1L3x8/Lx8ZF0bR7u5D9qoqKi9Morr1jfZ2Zm2swRAAAoOe5ZSPX395fJZLrjm6NKlbp2WeyNIffG6z0lqV+/fgoPD9eqVau0fv16RUdH67333tPgwYPzbDMrK0uStGrVKlWuXNmmzGw227zP69KE/1W6dGmb9yaTKd/T3wWR12n2O5WVlaVx48ZZVzBv5OjoeNft3snYrs/znDlz1KRJE5uyG0O9ZDuHJpNJku54Ds1m802fHwAAKJnu2Y1Tnp6eCg8P10cffaQLFy7cVJ7fI6IqVKgg6drNR9ft2rXrpnq+vr564YUXtHTpUg0fPlxz5syRJDk4OEi6top4Xd26dWU2m3Xs2DHVrFnT5nWvV95q166tq1ev6ueff7ZuO3z4sM6ePZvvPq6urvLz88v3UU4BAQE6fvy4jh8/bt22f/9+nTt3TnXr1pUkhYSE6MCBAzcdX82aNa3B/24EBQVp165dOnPmzG3renl5qVKlSvr1119vGkO1atUK3KeDg4PN5wcAAHBPH0H10UcfqXnz5mrcuLHeeustBQUF6erVq9qwYYNmzZql5OTkm/a5HhzHjh2rCRMm6ODBg3rvvfds6gwdOlTt2rVTrVq1dPbsWW3atEkBAQGSpKpVq8pkMmnlypV69NFH5eTkJFdXV40YMULDhg1Tbm6u/vnPf1pvTHJzc7O5VvKvqlOnjsLCwjRgwADNmjVLpUuX1vDhw+Xk5GRdMczL2LFj9cILL6hixYpq166dzp8/r4SEBA0ePFhhYWEKDAxUz549NXXqVF29elUvvfSSQkNDrZcnjB49Wh06dFCVKlX05JNPqlSpUkpKStLevXs1fvz4uz6ep59+WhMnTlTnzp0VHR0tHx8f/fzzz6pUqZL1+t4bjRs3TkOGDJG7u7siIiKUnZ2tHTt26OzZszan5m/Fz89PWVlZiouLU/369VWmTJm7vlwBAACUDPf0EVTVq1fXzp071apVKw0fPlwPPvig2rZtq7i4OM2aNSvPfUqXLq0vv/xSv/zyi4KCgjR58uSbQlZOTo4GDhyogIAARUREqFatWtbHHFWuXNl6E5GXl5f1MU5vv/22Ro0apejoaOt+q1atuqMVvoKKjY2Vl5eXWrRooccff1z9+/eXq6vrLU+79+7dW1OnTtXMmTNVr149dejQQYcOHZJ07XT48uXLVbZsWbVo0UJhYWGqXr26Fi9ebN0/PDxcK1eu1Pr169WoUSP94x//0AcffKCqVav+pWNxcHDQ+vXrVbFiRT366KMKDAzUpEmTbjp9f12/fv00d+5czZ8/X4GBgQoNDdWCBQvuaJ6bNWumF154Qd27d1eFChX0zjvv/KVjAAAAxZ/JUph3PN2nTpw4IV9fX23cuDHfm6Pw12VmZsrd3V2+Q79SKTMrrwCA4iF1UvuiHkKRuv77OyMjQ25ubvnWu6en++9X3377rbKyshQYGKi0tDS99tpr8vPzU4sWLYp6aAAAAMUSIfUeuHLlit588039+uuvcnV1VbNmzbRw4cKbngoAAACAgiGk3gPh4eEKDw8v6mEAAACUGPf0xikAAADgXiCkAgAAwHAIqQAAADAcQioAAAAMh5AKAAAAwyGkAgAAwHAIqQAAADAcnpOKYm/vuPBb/lk1AABQ/LCSCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHEIqAAAADIeH+aPYe3DMOpUylynqYQAAUGKkTmpf1ENgJRUAAADGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVNwTJpNJy5YtkySlpqbKZDJp165dRTomAABQfN1RSI2MjJTJZLK+ypUrp4iICO3evfuOOo2MjFTnzp3vaJ/7xeHDh9WnTx898MADMpvNqlatmp5++mnt2LHjnvbj5+enqVOn3tM2r/P19VVaWpoefPBBSVJ8fLxMJpPOnTtXKP0BAICS545XUiMiIpSWlqa0tDTFxcXJ3t5eHTp0KIyx3Xd27Nihhx56SAcPHtTs2bO1f/9+/fvf/1adOnU0fPjwv308OTk5ys3NveP97Ozs5O3tLXt7+0IYFQAAuB/ccUg1m83y9vaWt7e3GjRooDfeeEPHjx/X77//bq1z/PhxdevWTR4eHvL09FSnTp2UmpoqSRo7dqxiYmK0fPly64psfHy8nnzySQ0aNMjaxtChQ2UymfTLL79Iki5fvixnZ2dt3LhRkpSbm6vo6GhVq1ZNTk5Oql+/vpYsWWIz1r1796pdu3ZycXGRl5eXevXqpT/++MNa3rJlSw0ZMkSvvfaaPD095e3trbFjx9q0ce7cOfXr108VKlSQm5ubWrduraSkJGt5UlKSWrVqJVdXV7m5uemhhx6yrnoePXpUHTt2VNmyZeXs7Kx69epp9erVec6rxWJRZGSk/P399d1336l9+/aqUaOGGjRooDFjxmj58uUFml/p/1aqp0yZIh8fH5UrV04DBw7UlStXrMd99OhRDRs2zPoZSNKCBQvk4eGhFStWqG7dujKbzTp27JgSExPVtm1blS9fXu7u7goNDdXOnTvz/Rm58XR/amqqWrVqJUkqW7asTCaTIiMjFRsbq3Llyik7O9tm386dO6tXr175tg0AAO4Pf+ma1KysLH3++eeqWbOmypUrJ0m6cuWKwsPD5erqqu+++04JCQlycXFRRESELl++rBEjRqhbt242K7LNmjVTaGio4uPjrW1v3rxZ5cuXt25LTEzUlStX1KxZM0lSdHS0YmNj9fHHH2vfvn0aNmyYnnnmGW3evFnStXDZunVrBQcHa8eOHVq7dq1Onz6tbt262RxDTEyMnJ2d9eOPP+qdd97RW2+9pQ0bNljLu3btqvT0dK1Zs0Y//fSTQkJC1KZNG505c0aS1LNnTz3wwANKTEzUTz/9pDfeeEOlS5eWJA0cOFDZ2dnasmWL9uzZo8mTJ8vFxSXPudy1a5f27dun4cOHq1Spmz8WDw+PAs3vdZs2bVJKSoo2bdqkmJgYLViwQAsWLJAkLV26VA888IDeeust62dw3cWLFzV58mTNnTtX+/btU8WKFXX+/Hn17t1bW7du1Q8//CB/f389+uijOn/+/C1/PqRrp/6/+eYbSdKBAweUlpamadOmqWvXrsrJydGKFSusddPT07Vq1So999xzebaVnZ2tzMxMmxcAACiZ7vh87MqVK61B68KFC/Lx8dHKlSutwWrx4sXKzc3V3LlzrSt08+fPl4eHh+Lj4/XII4/IyclJ2dnZ8vb2trbbsmVLvfzyy/r9999lb2+v/fv3a9SoUYqPj9cLL7yg+Ph4NWrUSGXKlFF2drYmTpyojRs3qmnTppKk6tWra+vWrZo9e7ZCQ0M1Y8YMBQcHa+LEidY+Pv30U/n6+urgwYOqVauWJCkoKEhjxoyRJPn7+2vGjBmKi4tT27ZttXXrVm3fvl3p6ekym82SpClTpmjZsmVasmSJBgwYoGPHjunVV19VnTp1rG1cd+zYMXXp0kWBgYHWMebn0KFDkmRtJz8FmV/p2qrljBkzZGdnpzp16qh9+/aKi4tT//795enpKTs7O7m6utp8BtK1EDxz5kzVr1/fuq1169Y2dT755BN5eHho8+bNt73Uw87OTp6enpKkihUrWsO2JPXo0UPz589X165dJUmff/65qlSpopYtW+bZVnR0tMaNG3fL/gAAQMlwxyuprVq10q5du7Rr1y5t375d4eHhateunY4ePSrp2unvw4cPy9XVVS4uLnJxcZGnp6cuXbqklJSUfNt98MEH5enpqc2bN+u7775TcHCwOnToYF0Z3bx5szW8HD58WBcvXlTbtm2tfbi4uCg2NtbaR1JSkjZt2mRTfj0A3jiOoKAgm3H4+PgoPT3d2kZWVpbKlStn086RI0esbbzyyivq16+fwsLCNGnSJJu2hwwZovHjx6t58+YaM2bMLW8ws1gsBZr/gs5vvXr1ZGdnl+dx3YqDg8NNc3L69Gn1799f/v7+cnd3l5ubm7KysnTs2LECjTk//fv31/r163Xy5ElJ1y43uH5zXl6ioqKUkZFhfR0/fvwv9Q8AAIzrjldSnZ2dVbNmTev7uXPnyt3dXXPmzNH48eOVlZWlhx56SAsXLrxp3woVKuTbrslkUosWLRQfHy+z2ayWLVsqKChI2dnZ2rt3r77//nuNGDFC0rXLDCRp1apVqly5sk0711c8s7Ky1LFjR02ePPmmvnx8fKz/vn5q/sZxXL9ZKCsrSz4+PjaXIVx3fUVw7Nix6tGjh1atWqU1a9ZozJgxWrRokR5//HH169dP4eHhWrVqldavX6/o6Gi99957Gjx48E3tXV/Z/eWXXxQcHJzvPBV0fm91XLfi5OR0U0js3bu3/vzzT02bNk1Vq1aV2WxW06ZNbS4vuBvBwcGqX7++YmNj9cgjj2jfvn1atWpVvvXNZrP18wUAACXbX7792mQyqVSpUvrvf/8rSQoJCdHixYtVsWJFubm55bmPg4ODcnJybtoeGhqqOXPmyGw2a8KECSpVqpRatGihd999V9nZ2WrevLkk2dzUExoammcfISEh+uabb+Tn53fXd5mHhITo1KlTsre3l5+fX771atWqpVq1amnYsGF6+umnNX/+fD3++OOSrl2T+cILL+iFF15QVFSU5syZk2dIbdCggerWrav33ntP3bt3v+m61HPnzsnDw6NA81sQ+X0GeUlISNDMmTP16KOPSrp249aNN6AVpC9JefbXr18/TZ06VSdPnlRYWJh8fX0L3C4AACi57vh0f3Z2tk6dOqVTp04pOTlZgwcPtq5aStduJCpfvrw6deqk7777TkeOHFF8fLyGDBmiEydOSLr2jM7du3frwIED+uOPP2zuOt+/f7/27dunf/7zn9ZtCxcuVMOGDeXs7CxJcnV11YgRIzRs2DDFxMQoJSVFO3fu1IcffqiYmBhJ125aOnPmjJ5++mklJiYqJSVF69atU58+fQoczsLCwtS0aVN17txZ69evV2pqqr7//nuNHDlSO3bs0H//+18NGjRI8fHxOnr0qBISEpSYmKiAgABJ155QsG7dOh05ckQ7d+7Upk2brGX/y2Qyaf78+Tp48KAefvhhrV69Wr/++qt2796tCRMmqFOnTgWe34Lw8/PTli1bdPLkydsGTn9/f3322WdKTk7Wjz/+qJ49e8rJyanAfVWtWlUmk0krV67U77//bl0Jl65dl3rixAnNmTMn3xumAADA/eeOQ+ratWvl4+MjHx8fNWnSRImJifr666+t14uWKVNGW7ZsUZUqVfTEE08oICBAffv21aVLl6wrf/3791ft2rXVsGFDVahQQQkJCZKkwMBAeXh4qEGDBtabs1q2bKmcnJybbqZ5++23NWrUKEVHRysgIEARERFatWqVqlWrJkmqVKmSEhISlJOTo0ceeUSBgYEaOnSoPDw88rx7Pi8mk0mrV69WixYt1KdPH9WqVUtPPfWUjh49Ki8vL9nZ2enPP//Us88+q1q1aqlbt25q166d9eaenJwcDRw40Dq+WrVqaebMmfn217hxY+3YsUM1a9ZU//79FRAQoMcee0z79u2zPni/IPNbEG+99ZZSU1NVo0aNW16GIUnz5s3T2bNnFRISol69emnIkCGqWLFigfuqXLmyxo0bpzfeeENeXl42jxpzd3dXly5d5OLiwh94AAAAViZLQe/YAQpJmzZtVK9ePU2fPv2O9svMzJS7u7t8h36lUuYyhTQ6AADuP6mT2hda29d/f2dkZNxygY0/CYQic/bsWcXHxys+Pv6WK8wAAOD+Q0hFkQkODtbZs2c1efJk1a5du6iHAwAADISQiiJz459yBQAAuNFf+rOoAAAAQGEgpAIAAMBwCKkAAAAwHEIqAAAADIeQCgAAAMMhpAIAAMBwCKkAAAAwHJ6TimJv77jwW/5ZNQAAUPywkgoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBxCKgAAAAyHkAoAAADDIaQCAADAcAipAAAAMBz7oh4AcLcsFoskKTMzs4hHAgAACur67+3rv8fzQ0hFsfXnn39Kknx9fYt4JAAA4E6dP39e7u7u+ZYTUlFseXp6SpKOHTt2yx9y3J3MzEz5+vrq+PHjcnNzK+rhlDjMb+Fifgsfc1y4SvL8WiwWnT9/XpUqVbplPUIqiq1Spa5dUu3u7l7i/g9sJG5ubsxvIWJ+CxfzW/iY48JVUue3IItL3DgFAAAAwyGkAgAAwHAIqSi2zGazxowZI7PZXNRDKZGY38LF/BYu5rfwMceFi/mVTJbb3f8PAAAA/M1YSQUAAIDhEFIBAABgOIRUAAAAGA4hFQAAAIZDSIWhfPTRR/Lz85Ojo6OaNGmi7du337L+119/rTp16sjR0VGBgYFavXq1TbnFYtHo0aPl4+MjJycnhYWF6dChQ4V5CIZ2r+c3MjJSJpPJ5hUREVGYh2BodzK/+/btU5cuXeTn5yeTyaSpU6f+5TZLuns9v2PHjr3p57dOnTqFeATGdifzO2fOHD388MMqW7asypYtq7CwsJvq8/1r617P733x/WsBDGLRokUWBwcHy6effmrZt2+fpX///hYPDw/L6dOn86yfkJBgsbOzs7zzzjuW/fv3W/71r39ZSpcubdmzZ4+1zqRJkyzu7u6WZcuWWZKSkiyPPfaYpVq1apb//ve/f9dhGUZhzG/v3r0tERERlrS0NOvrzJkzf9chGcqdzu/27dstI0aMsHz55ZcWb29vywcffPCX2yzJCmN+x4wZY6lXr57Nz+/vv/9eyEdiTHc6vz169LB89NFHlp9//tmSnJxsiYyMtLi7u1tOnDhhrcP37/8pjPm9H75/CakwjMaNG1sGDhxofZ+Tk2OpVKmSJTo6Os/63bp1s7Rv395mW5MmTSzPP/+8xWKxWHJzcy3e3t6Wd99911p+7tw5i9lstnz55ZeFcATGdq/n12K59iXZqVOnQhlvcXOn83ujqlWr5hmi/kqbJU1hzO+YMWMs9evXv4ejLL7+6s/a1atXLa6urpaYmBiLxcL37/+61/Nrsdwf37+c7ochXL58WT/99JPCwsKs20qVKqWwsDBt27Ytz322bdtmU1+SwsPDrfWPHDmiU6dO2dRxd3dXkyZN8m2zpCqM+b0uPj5eFStWVO3atfXiiy/qzz//vPcHYHB3M79F0WZxVZhzcejQIVWqVEnVq1dXz549dezYsb863GLnXszvxYsXdeXKFXl6ekri+/dGhTG/15X0719CKgzhjz/+UE5Ojry8vGy2e3l56dSpU3nuc+rUqVvWv/6/d9JmSVUY8ytJERERio2NVVxcnCZPnqzNmzerXbt2ysnJufcHYWB3M79F0WZxVVhz0aRJEy1YsEBr167VrFmzdOTIET388MM6f/78Xx1ysXIv5vf1119XpUqVrEGM79//UxjzK90f37/2RT0AAMXXU089Zf13YGCggoKCVKNGDcXHx6tNmzZFODLg9tq1a2f9d1BQkJo0aaKqVavqq6++Ut++fYtwZMXLpEmTtGjRIsXHx8vR0bGoh1Pi5De/98P3LyupMITy5cvLzs5Op0+fttl++vRpeXt757mPt7f3Letf/987abOkKoz5zUv16tVVvnx5HT58+K8Puhi5m/ktijaLq79rLjw8PFSrVi1+fv+/gszvlClTNGnSJK1fv15BQUHW7Xz//p/CmN+8lMTvX0IqDMHBwUEPPfSQ4uLirNtyc3MVFxenpk2b5rlP06ZNbepL0oYNG6z1q1WrJm9vb5s6mZmZ+vHHH/Nts6QqjPnNy4kTJ/Tnn3/Kx8fn3gy8mLib+S2KNourv2susrKylJKSws+vCja/77zzjt5++22tXbtWDRs2tCnj+/f/FMb85qVEfv8W9Z1bwHWLFi2ymM1my4IFCyz79++3DBgwwOLh4WE5deqUxWKxWHr16mV54403rPUTEhIs9vb2lilTpliSk5MtY8aMyfMRVB4eHpbly5dbdu/ebenUqdN9/QiUezm/58+ft4wYMcKybds2y5EjRywbN260hISEWPz9/S2XLl0qkmMsSnc6v9nZ2Zaff/7Z8vPPP1t8fHwsI0aMsPz888+WQ4cOFbjN+0lhzO/w4cMt8fHxliNHjlgSEhIsYWFhlvLly1vS09P/9uMranc6v5MmTbI4ODhYlixZYvMIpPPnz9vU4fv3mns9v/fL9y8hFYby4YcfWqpUqWJxcHCwNG7c2PLDDz9Yy0JDQy29e/e2qf/VV19ZatWqZXFwcLDUq1fPsmrVKpvy3Nxcy6hRoyxeXl4Ws9lsadOmjeXAgQN/x6EY0r2c34sXL1oeeeQRS4UKFSylS5e2VK1a1dK/f//7MkBddyfze+TIEYukm16hoaEFbvN+c6/nt3v37hYfHx+Lg4ODpXLlypbu3btbDh8+/DcekbHcyfxWrVo1z/kdM2aMtQ7fv7bu5fzeL9+/JovFYvl7124BAACAW+OaVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDiEVAAAABgOIRUAAACGQ0gFAACA4RBSAQAAYDj/D4YfWUmbIkKrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0016vXVL8vY"
      },
      "source": [
        "####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va2eri2EMzt1"
      },
      "source": [
        "import torch\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScjgYJQuozxs"
      },
      "source": [
        "X = df.iloc[:, 1:-1]\n",
        "y = df.iloc[:, -1:]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXt_bilCo4LV"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f5GPf3iL8z6",
        "outputId": "e3218335-ec55-47b7-f523-5529763cc457"
      },
      "source": [
        "X_train = torch.from_numpy(X_train.to_numpy()).float()\n",
        "y_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n",
        "\n",
        "X_test = torch.from_numpy(X_test.to_numpy()).float()\n",
        "y_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1644, 8]) torch.Size([1644])\n",
            "torch.Size([412, 8]) torch.Size([412])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgRv1d0jMTKs"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, n_features):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(n_features, 64)\n",
        "    self.fc2 = nn.Linear(64, 128)\n",
        "    self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return torch.sigmoid(self.fc3(x))\n",
        "\n",
        "net = Net(X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFNu-aflM831"
      },
      "source": [
        "def calculate_accuracy(y_true, y_pred):\n",
        "  predicted = y_pred.ge(.5).view(-1)\n",
        "  return (y_true == predicted).sum().float() / len(y_true)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkJEWcPXNJ9A",
        "outputId": "e1632465-5962-4d49-c93b-6b8a012a7f50"
      },
      "source": [
        "def round_tensor(t, decimal_places=4):\n",
        "  return round(t.item(), decimal_places)\n",
        "\n",
        "for epoch in range(4000):\n",
        "  y_pred = net(X_train)\n",
        "  y_pred = torch.squeeze(y_pred)\n",
        "\n",
        "  train_loss = criterion(y_pred, y_train)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    train_acc = calculate_accuracy(y_train, y_pred)\n",
        "    y_test_pred = net(X_test)\n",
        "    y_test_pred = torch.squeeze(y_test_pred)\n",
        "    test_loss = criterion(y_test_pred, y_test)\n",
        "    test_acc = calculate_accuracy(y_test, y_test_pred)\n",
        "\n",
        "    print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(epoch,train_loss,train_acc))\n",
        "\n",
        "\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  train_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(\"Test accuracy : {}\".format(test_acc))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\tloss : 0.6950936317443848\t accuracy : 0.4957420825958252\n",
            "epoch 100\tloss : 0.5275307297706604\t accuracy : 0.7524330615997314\n",
            "epoch 200\tloss : 0.4488246440887451\t accuracy : 0.790145993232727\n",
            "epoch 300\tloss : 0.42053666710853577\t accuracy : 0.8010948896408081\n",
            "epoch 400\tloss : 0.40528035163879395\t accuracy : 0.812652051448822\n",
            "epoch 500\tloss : 0.39498528838157654\t accuracy : 0.819343090057373\n",
            "epoch 600\tloss : 0.3867360055446625\t accuracy : 0.8181264996528625\n",
            "epoch 700\tloss : 0.38088297843933105\t accuracy : 0.8217761516571045\n",
            "epoch 800\tloss : 0.37559452652931213\t accuracy : 0.8205596208572388\n",
            "epoch 900\tloss : 0.3713049292564392\t accuracy : 0.8278588652610779\n",
            "epoch 1000\tloss : 0.3678814172744751\t accuracy : 0.8296837210655212\n",
            "epoch 1100\tloss : 0.3649345934391022\t accuracy : 0.8284671306610107\n",
            "epoch 1200\tloss : 0.3623911440372467\t accuracy : 0.8296837210655212\n",
            "epoch 1300\tloss : 0.36025741696357727\t accuracy : 0.8290753960609436\n",
            "epoch 1400\tloss : 0.3577999174594879\t accuracy : 0.830900251865387\n",
            "epoch 1500\tloss : 0.3554999828338623\t accuracy : 0.8345499038696289\n",
            "epoch 1600\tloss : 0.35313162207603455\t accuracy : 0.8357664346694946\n",
            "epoch 1700\tloss : 0.351510226726532\t accuracy : 0.8363747000694275\n",
            "epoch 1800\tloss : 0.34953412413597107\t accuracy : 0.8363747000694275\n",
            "epoch 1900\tloss : 0.3468608558177948\t accuracy : 0.8357664346694946\n",
            "epoch 2000\tloss : 0.3449370265007019\t accuracy : 0.8375912308692932\n",
            "epoch 2100\tloss : 0.34293338656425476\t accuracy : 0.841849148273468\n",
            "epoch 2200\tloss : 0.34062659740448\t accuracy : 0.8436739444732666\n",
            "epoch 2300\tloss : 0.3387531340122223\t accuracy : 0.8442822098731995\n",
            "epoch 2400\tloss : 0.33685925602912903\t accuracy : 0.8461070656776428\n",
            "epoch 2500\tloss : 0.3353842496871948\t accuracy : 0.8479318618774414\n",
            "epoch 2600\tloss : 0.3333400785923004\t accuracy : 0.8479318618774414\n",
            "epoch 2700\tloss : 0.3315334618091583\t accuracy : 0.8497567176818848\n",
            "epoch 2800\tloss : 0.3297021985054016\t accuracy : 0.8485401272773743\n",
            "epoch 2900\tloss : 0.3277653157711029\t accuracy : 0.8497567176818848\n",
            "epoch 3000\tloss : 0.3262558579444885\t accuracy : 0.8546229004859924\n",
            "epoch 3100\tloss : 0.32477325201034546\t accuracy : 0.8540145754814148\n",
            "epoch 3200\tloss : 0.3225911855697632\t accuracy : 0.8546229004859924\n",
            "epoch 3300\tloss : 0.32107818126678467\t accuracy : 0.8588807582855225\n",
            "epoch 3400\tloss : 0.31911200284957886\t accuracy : 0.8576642274856567\n",
            "epoch 3500\tloss : 0.3173941373825073\t accuracy : 0.8588807582855225\n",
            "epoch 3600\tloss : 0.31609445810317993\t accuracy : 0.8582724928855896\n",
            "epoch 3700\tloss : 0.3140951693058014\t accuracy : 0.8607056140899658\n",
            "epoch 3800\tloss : 0.3130580186843872\t accuracy : 0.8570559620857239\n",
            "epoch 3900\tloss : 0.3107012212276459\t accuracy : 0.8625304102897644\n",
            "Test accuracy : 0.8106796145439148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7LZV_rpOAEz",
        "outputId": "6bed8461-eb51-44a9-e422-f22927a2a983"
      },
      "source": [
        "classes = ['Non T2DM', 'T2M']\n",
        "\n",
        "y_pred = net(X_test)\n",
        "\n",
        "y_pred = y_pred.ge(.5).view(-1)\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=classes, digits=4))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Non T2DM     0.7965    0.8638    0.8288       213\n",
            "         T2M     0.8398    0.7638    0.8000       199\n",
            "\n",
            "    accuracy                         0.8155       412\n",
            "   macro avg     0.8182    0.8138    0.8144       412\n",
            "weighted avg     0.8174    0.8155    0.8149       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOX4X6v1bijb"
      },
      "source": [
        "y_pred = y_pred.detach().numpy()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9iVPcd9Rloo"
      },
      "source": [
        "#Keras DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHBlj5yRRq_j"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR5fU1jMSLil"
      },
      "source": [
        "X = df.iloc[:, 1:-1]\n",
        "y = df.iloc[:, -1:]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp7DMNG2TWtM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE0R0pWvTur3"
      },
      "source": [
        "X_train = X_train.values\n",
        "y_train = y_train.values\n",
        "X_test = X_test.values\n",
        "y_test = y_test.values"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7afjUAERrM8"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxJLQ_kIUY_q"
      },
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZdZx2bSTAo7"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqGHAg86TEJP",
        "outputId": "d2cc7864-3b8f-4cab-e7cf-82d0ff85f0b5"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=2000)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "52/52 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.5797\n",
            "Epoch 2/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6324 - accuracy: 0.6758\n",
            "Epoch 3/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.7135\n",
            "Epoch 4/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.5699 - accuracy: 0.7397\n",
            "Epoch 5/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.5450 - accuracy: 0.7433\n",
            "Epoch 6/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.5284 - accuracy: 0.7488\n",
            "Epoch 7/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.5178 - accuracy: 0.7439\n",
            "Epoch 8/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.7536\n",
            "Epoch 9/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4976 - accuracy: 0.7622\n",
            "Epoch 10/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4934 - accuracy: 0.7634\n",
            "Epoch 11/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.7646\n",
            "Epoch 12/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4870 - accuracy: 0.7579\n",
            "Epoch 13/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7573\n",
            "Epoch 14/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4750 - accuracy: 0.7664\n",
            "Epoch 15/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4750 - accuracy: 0.7603\n",
            "Epoch 16/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7640\n",
            "Epoch 17/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4773 - accuracy: 0.7652\n",
            "Epoch 18/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.7682\n",
            "Epoch 19/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4634 - accuracy: 0.7695\n",
            "Epoch 20/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4656 - accuracy: 0.7597\n",
            "Epoch 21/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4670 - accuracy: 0.7640\n",
            "Epoch 22/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.7731\n",
            "Epoch 23/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4694 - accuracy: 0.7664\n",
            "Epoch 24/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7707\n",
            "Epoch 25/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4564 - accuracy: 0.7792\n",
            "Epoch 26/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7713\n",
            "Epoch 27/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4546 - accuracy: 0.7762\n",
            "Epoch 28/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7786\n",
            "Epoch 29/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.7664\n",
            "Epoch 30/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4590 - accuracy: 0.7689\n",
            "Epoch 31/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7816\n",
            "Epoch 32/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.7871\n",
            "Epoch 33/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4448 - accuracy: 0.7774\n",
            "Epoch 34/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4450 - accuracy: 0.7835\n",
            "Epoch 35/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4474 - accuracy: 0.7835\n",
            "Epoch 36/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4513 - accuracy: 0.7689\n",
            "Epoch 37/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7841\n",
            "Epoch 38/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.7835\n",
            "Epoch 39/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 0.7810\n",
            "Epoch 40/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4390 - accuracy: 0.7871\n",
            "Epoch 41/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7810\n",
            "Epoch 42/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4448 - accuracy: 0.7768\n",
            "Epoch 43/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4564 - accuracy: 0.7640\n",
            "Epoch 44/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4428 - accuracy: 0.7859\n",
            "Epoch 45/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.7871\n",
            "Epoch 46/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4390 - accuracy: 0.7816\n",
            "Epoch 47/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4359 - accuracy: 0.7859\n",
            "Epoch 48/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.7841\n",
            "Epoch 49/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.7908\n",
            "Epoch 50/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.7871\n",
            "Epoch 51/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7865\n",
            "Epoch 52/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4338 - accuracy: 0.7883\n",
            "Epoch 53/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.7859\n",
            "Epoch 54/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.7859\n",
            "Epoch 55/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4293 - accuracy: 0.7974\n",
            "Epoch 56/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.7920\n",
            "Epoch 57/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.7865\n",
            "Epoch 58/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4292 - accuracy: 0.7950\n",
            "Epoch 59/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.7889\n",
            "Epoch 60/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.7901\n",
            "Epoch 61/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.7938\n",
            "Epoch 62/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4406 - accuracy: 0.7786\n",
            "Epoch 63/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4355 - accuracy: 0.7828\n",
            "Epoch 64/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.7908\n",
            "Epoch 65/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4243 - accuracy: 0.7908\n",
            "Epoch 66/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4223 - accuracy: 0.7895\n",
            "Epoch 67/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4243 - accuracy: 0.7938\n",
            "Epoch 68/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4256 - accuracy: 0.7950\n",
            "Epoch 69/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4200 - accuracy: 0.7993\n",
            "Epoch 70/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4251 - accuracy: 0.7847\n",
            "Epoch 71/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4200 - accuracy: 0.8005\n",
            "Epoch 72/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.7932\n",
            "Epoch 73/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4255 - accuracy: 0.7962\n",
            "Epoch 74/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4221 - accuracy: 0.7999\n",
            "Epoch 75/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.7865\n",
            "Epoch 76/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4234 - accuracy: 0.7932\n",
            "Epoch 77/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.4230 - accuracy: 0.7974\n",
            "Epoch 78/2000\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.4323 - accuracy: 0.7883\n",
            "Epoch 79/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.4224 - accuracy: 0.7993\n",
            "Epoch 80/2000\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.4173 - accuracy: 0.7926\n",
            "Epoch 81/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4181 - accuracy: 0.7938\n",
            "Epoch 82/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.4181 - accuracy: 0.7981\n",
            "Epoch 83/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4194 - accuracy: 0.8005\n",
            "Epoch 84/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.4158 - accuracy: 0.8023\n",
            "Epoch 85/2000\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.4145 - accuracy: 0.8023\n",
            "Epoch 86/2000\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.4174 - accuracy: 0.7944\n",
            "Epoch 87/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.4181 - accuracy: 0.7920\n",
            "Epoch 88/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.7932\n",
            "Epoch 89/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4162 - accuracy: 0.7968\n",
            "Epoch 90/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.7914\n",
            "Epoch 91/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4191 - accuracy: 0.7956\n",
            "Epoch 92/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4153 - accuracy: 0.7908\n",
            "Epoch 93/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4109 - accuracy: 0.8017\n",
            "Epoch 94/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4245 - accuracy: 0.7920\n",
            "Epoch 95/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4147 - accuracy: 0.7944\n",
            "Epoch 96/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4128 - accuracy: 0.7968\n",
            "Epoch 97/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4160 - accuracy: 0.8041\n",
            "Epoch 98/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4148 - accuracy: 0.7999\n",
            "Epoch 99/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4165 - accuracy: 0.7956\n",
            "Epoch 100/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8029\n",
            "Epoch 101/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4122 - accuracy: 0.8011\n",
            "Epoch 102/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4106 - accuracy: 0.8035\n",
            "Epoch 103/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4174 - accuracy: 0.7920\n",
            "Epoch 104/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4111 - accuracy: 0.8017\n",
            "Epoch 105/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4157 - accuracy: 0.7926\n",
            "Epoch 106/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4132 - accuracy: 0.7956\n",
            "Epoch 107/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4078 - accuracy: 0.8017\n",
            "Epoch 108/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4124 - accuracy: 0.7938\n",
            "Epoch 109/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4162 - accuracy: 0.7950\n",
            "Epoch 110/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4075 - accuracy: 0.7956\n",
            "Epoch 111/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.7968\n",
            "Epoch 112/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4054 - accuracy: 0.8005\n",
            "Epoch 113/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4079 - accuracy: 0.7993\n",
            "Epoch 114/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.7962\n",
            "Epoch 115/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4048 - accuracy: 0.8066\n",
            "Epoch 116/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4045 - accuracy: 0.8072\n",
            "Epoch 117/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4064 - accuracy: 0.8017\n",
            "Epoch 118/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4074 - accuracy: 0.8005\n",
            "Epoch 119/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4121 - accuracy: 0.7944\n",
            "Epoch 120/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8108\n",
            "Epoch 121/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4014 - accuracy: 0.8023\n",
            "Epoch 122/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4089 - accuracy: 0.7999\n",
            "Epoch 123/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4045 - accuracy: 0.8005\n",
            "Epoch 124/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4024 - accuracy: 0.8017\n",
            "Epoch 125/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4084 - accuracy: 0.7962\n",
            "Epoch 126/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4036 - accuracy: 0.8084\n",
            "Epoch 127/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4110 - accuracy: 0.7932\n",
            "Epoch 128/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.8066\n",
            "Epoch 129/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4045 - accuracy: 0.8054\n",
            "Epoch 130/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4029 - accuracy: 0.7993\n",
            "Epoch 131/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4009 - accuracy: 0.8017\n",
            "Epoch 132/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.7920\n",
            "Epoch 133/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.7944\n",
            "Epoch 134/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8084\n",
            "Epoch 135/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3998 - accuracy: 0.8054\n",
            "Epoch 136/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4011 - accuracy: 0.8054\n",
            "Epoch 137/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8029\n",
            "Epoch 138/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4022 - accuracy: 0.8078\n",
            "Epoch 139/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4087 - accuracy: 0.8041\n",
            "Epoch 140/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8084\n",
            "Epoch 141/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.7932\n",
            "Epoch 142/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3976 - accuracy: 0.8023\n",
            "Epoch 143/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4082 - accuracy: 0.7950\n",
            "Epoch 144/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4025 - accuracy: 0.7974\n",
            "Epoch 145/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3985 - accuracy: 0.8096\n",
            "Epoch 146/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3999 - accuracy: 0.8023\n",
            "Epoch 147/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.4000 - accuracy: 0.8066\n",
            "Epoch 148/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3999 - accuracy: 0.8151\n",
            "Epoch 149/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3966 - accuracy: 0.8066\n",
            "Epoch 150/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.3910 - accuracy: 0.8090\n",
            "Epoch 151/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4019 - accuracy: 0.8041\n",
            "Epoch 152/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.3992 - accuracy: 0.8078\n",
            "Epoch 153/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4048 - accuracy: 0.8041\n",
            "Epoch 154/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.3954 - accuracy: 0.8017\n",
            "Epoch 155/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3987 - accuracy: 0.8066\n",
            "Epoch 156/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3921 - accuracy: 0.8090\n",
            "Epoch 157/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8120\n",
            "Epoch 158/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.8060\n",
            "Epoch 159/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3904 - accuracy: 0.8108\n",
            "Epoch 160/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3996 - accuracy: 0.8011\n",
            "Epoch 161/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.4021 - accuracy: 0.8066\n",
            "Epoch 162/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8090\n",
            "Epoch 163/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3943 - accuracy: 0.8035\n",
            "Epoch 164/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3907 - accuracy: 0.8133\n",
            "Epoch 165/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3968 - accuracy: 0.8047\n",
            "Epoch 166/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8011\n",
            "Epoch 167/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8078\n",
            "Epoch 168/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8066\n",
            "Epoch 169/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8072\n",
            "Epoch 170/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8078\n",
            "Epoch 171/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8169\n",
            "Epoch 172/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8145\n",
            "Epoch 173/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8127\n",
            "Epoch 174/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8035\n",
            "Epoch 175/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3917 - accuracy: 0.8084\n",
            "Epoch 176/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8102\n",
            "Epoch 177/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.7993\n",
            "Epoch 178/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.7999\n",
            "Epoch 179/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8035\n",
            "Epoch 180/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8072\n",
            "Epoch 181/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8151\n",
            "Epoch 182/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8133\n",
            "Epoch 183/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8139\n",
            "Epoch 184/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8224\n",
            "Epoch 185/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8072\n",
            "Epoch 186/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8060\n",
            "Epoch 187/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8096\n",
            "Epoch 188/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8096\n",
            "Epoch 189/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8029\n",
            "Epoch 190/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8005\n",
            "Epoch 191/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8017\n",
            "Epoch 192/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8066\n",
            "Epoch 193/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8133\n",
            "Epoch 194/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8017\n",
            "Epoch 195/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8114\n",
            "Epoch 196/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8060\n",
            "Epoch 197/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8090\n",
            "Epoch 198/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8096\n",
            "Epoch 199/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8096\n",
            "Epoch 200/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3851 - accuracy: 0.8108\n",
            "Epoch 201/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8127\n",
            "Epoch 202/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8078\n",
            "Epoch 203/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8139\n",
            "Epoch 204/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8139\n",
            "Epoch 205/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8078\n",
            "Epoch 206/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.8145\n",
            "Epoch 207/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.8133\n",
            "Epoch 208/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8157\n",
            "Epoch 209/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8060\n",
            "Epoch 210/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8145\n",
            "Epoch 211/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8145\n",
            "Epoch 212/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8078\n",
            "Epoch 213/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8096\n",
            "Epoch 214/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3820 - accuracy: 0.8139\n",
            "Epoch 215/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8133\n",
            "Epoch 216/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8127\n",
            "Epoch 217/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8133\n",
            "Epoch 218/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.8090\n",
            "Epoch 219/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8254\n",
            "Epoch 220/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.8230\n",
            "Epoch 221/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.8060\n",
            "Epoch 222/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8127\n",
            "Epoch 223/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8157\n",
            "Epoch 224/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.8151\n",
            "Epoch 225/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8066\n",
            "Epoch 226/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8151\n",
            "Epoch 227/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8120\n",
            "Epoch 228/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8090\n",
            "Epoch 229/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8163\n",
            "Epoch 230/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8181\n",
            "Epoch 231/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8206\n",
            "Epoch 232/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8139\n",
            "Epoch 233/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8157\n",
            "Epoch 234/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8127\n",
            "Epoch 235/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8102\n",
            "Epoch 236/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8163\n",
            "Epoch 237/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8139\n",
            "Epoch 238/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.8230\n",
            "Epoch 239/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8181\n",
            "Epoch 240/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8187\n",
            "Epoch 241/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3782 - accuracy: 0.8157\n",
            "Epoch 242/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8151\n",
            "Epoch 243/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8181\n",
            "Epoch 244/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8206\n",
            "Epoch 245/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3785 - accuracy: 0.8120\n",
            "Epoch 246/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8175\n",
            "Epoch 247/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8181\n",
            "Epoch 248/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8084\n",
            "Epoch 249/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.8139\n",
            "Epoch 250/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8212\n",
            "Epoch 251/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8181\n",
            "Epoch 252/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8157\n",
            "Epoch 253/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3788 - accuracy: 0.8120\n",
            "Epoch 254/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3792 - accuracy: 0.8169\n",
            "Epoch 255/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8212\n",
            "Epoch 256/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3818 - accuracy: 0.8206\n",
            "Epoch 257/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8127\n",
            "Epoch 258/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3757 - accuracy: 0.8254\n",
            "Epoch 259/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8181\n",
            "Epoch 260/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3785 - accuracy: 0.8139\n",
            "Epoch 261/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3767 - accuracy: 0.8139\n",
            "Epoch 262/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8236\n",
            "Epoch 263/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8175\n",
            "Epoch 264/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8193\n",
            "Epoch 265/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.8181\n",
            "Epoch 266/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8169\n",
            "Epoch 267/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8163\n",
            "Epoch 268/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3749 - accuracy: 0.8224\n",
            "Epoch 269/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8175\n",
            "Epoch 270/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.8175\n",
            "Epoch 271/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8145\n",
            "Epoch 272/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8157\n",
            "Epoch 273/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8339\n",
            "Epoch 274/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3746 - accuracy: 0.8200\n",
            "Epoch 275/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8181\n",
            "Epoch 276/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8133\n",
            "Epoch 277/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.8248\n",
            "Epoch 278/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8206\n",
            "Epoch 279/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8151\n",
            "Epoch 280/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8224\n",
            "Epoch 281/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3782 - accuracy: 0.8102\n",
            "Epoch 282/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.8206\n",
            "Epoch 283/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8157\n",
            "Epoch 284/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8120\n",
            "Epoch 285/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8096\n",
            "Epoch 286/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8163\n",
            "Epoch 287/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8254\n",
            "Epoch 288/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8181\n",
            "Epoch 289/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.8212\n",
            "Epoch 290/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8200\n",
            "Epoch 291/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8242\n",
            "Epoch 292/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.8193\n",
            "Epoch 293/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3742 - accuracy: 0.8206\n",
            "Epoch 294/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8187\n",
            "Epoch 295/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8169\n",
            "Epoch 296/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8248\n",
            "Epoch 297/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8230\n",
            "Epoch 298/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8175\n",
            "Epoch 299/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.8175\n",
            "Epoch 300/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8193\n",
            "Epoch 301/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.8169\n",
            "Epoch 302/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8236\n",
            "Epoch 303/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.8200\n",
            "Epoch 304/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8206\n",
            "Epoch 305/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8218\n",
            "Epoch 306/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.8200\n",
            "Epoch 307/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8175\n",
            "Epoch 308/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8157\n",
            "Epoch 309/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8169\n",
            "Epoch 310/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.8254\n",
            "Epoch 311/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8139\n",
            "Epoch 312/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8163\n",
            "Epoch 313/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8181\n",
            "Epoch 314/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8206\n",
            "Epoch 315/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8248\n",
            "Epoch 316/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8242\n",
            "Epoch 317/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8145\n",
            "Epoch 318/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8242\n",
            "Epoch 319/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8218\n",
            "Epoch 320/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.8181\n",
            "Epoch 321/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8291\n",
            "Epoch 322/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8236\n",
            "Epoch 323/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8224\n",
            "Epoch 324/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8230\n",
            "Epoch 325/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3723 - accuracy: 0.8212\n",
            "Epoch 326/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8230\n",
            "Epoch 327/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8248\n",
            "Epoch 328/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.8224\n",
            "Epoch 329/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3830 - accuracy: 0.8133\n",
            "Epoch 330/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8224\n",
            "Epoch 331/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8212\n",
            "Epoch 332/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8151\n",
            "Epoch 333/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.8266\n",
            "Epoch 334/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8309\n",
            "Epoch 335/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8169\n",
            "Epoch 336/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8175\n",
            "Epoch 337/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8248\n",
            "Epoch 338/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8242\n",
            "Epoch 339/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8224\n",
            "Epoch 340/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3705 - accuracy: 0.8218\n",
            "Epoch 341/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8193\n",
            "Epoch 342/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8200\n",
            "Epoch 343/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3661 - accuracy: 0.8254\n",
            "Epoch 344/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8297\n",
            "Epoch 345/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8236\n",
            "Epoch 346/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8175\n",
            "Epoch 347/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8175\n",
            "Epoch 348/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3723 - accuracy: 0.8206\n",
            "Epoch 349/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3668 - accuracy: 0.8175\n",
            "Epoch 350/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3767 - accuracy: 0.8206\n",
            "Epoch 351/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8200\n",
            "Epoch 352/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8248\n",
            "Epoch 353/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.8218\n",
            "Epoch 354/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8175\n",
            "Epoch 355/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8133\n",
            "Epoch 356/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3597 - accuracy: 0.8266\n",
            "Epoch 357/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8242\n",
            "Epoch 358/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 0.8200\n",
            "Epoch 359/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8224\n",
            "Epoch 360/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8242\n",
            "Epoch 361/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8145\n",
            "Epoch 362/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8187\n",
            "Epoch 363/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8163\n",
            "Epoch 364/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8224\n",
            "Epoch 365/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.8187\n",
            "Epoch 366/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3646 - accuracy: 0.8157\n",
            "Epoch 367/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8175\n",
            "Epoch 368/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8187\n",
            "Epoch 369/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8236\n",
            "Epoch 370/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8266\n",
            "Epoch 371/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8248\n",
            "Epoch 372/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3613 - accuracy: 0.8279\n",
            "Epoch 373/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8212\n",
            "Epoch 374/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.8224\n",
            "Epoch 375/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3626 - accuracy: 0.8266\n",
            "Epoch 376/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3642 - accuracy: 0.8266\n",
            "Epoch 377/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8218\n",
            "Epoch 378/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8291\n",
            "Epoch 379/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8285\n",
            "Epoch 380/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8206\n",
            "Epoch 381/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8200\n",
            "Epoch 382/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8248\n",
            "Epoch 383/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3634 - accuracy: 0.8181\n",
            "Epoch 384/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8248\n",
            "Epoch 385/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8218\n",
            "Epoch 386/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8315\n",
            "Epoch 387/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8254\n",
            "Epoch 388/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.8230\n",
            "Epoch 389/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8236\n",
            "Epoch 390/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8206\n",
            "Epoch 391/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8285\n",
            "Epoch 392/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.8321\n",
            "Epoch 393/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8212\n",
            "Epoch 394/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8175\n",
            "Epoch 395/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3599 - accuracy: 0.8333\n",
            "Epoch 396/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8315\n",
            "Epoch 397/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8193\n",
            "Epoch 398/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8303\n",
            "Epoch 399/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3596 - accuracy: 0.8297\n",
            "Epoch 400/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8248\n",
            "Epoch 401/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3581 - accuracy: 0.8345\n",
            "Epoch 402/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3607 - accuracy: 0.8279\n",
            "Epoch 403/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3622 - accuracy: 0.8303\n",
            "Epoch 404/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8339\n",
            "Epoch 405/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8242\n",
            "Epoch 406/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8279\n",
            "Epoch 407/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.8127\n",
            "Epoch 408/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8151\n",
            "Epoch 409/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3599 - accuracy: 0.8206\n",
            "Epoch 410/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8248\n",
            "Epoch 411/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.8315\n",
            "Epoch 412/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3659 - accuracy: 0.8212\n",
            "Epoch 413/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8230\n",
            "Epoch 414/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3664 - accuracy: 0.8181\n",
            "Epoch 415/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8279\n",
            "Epoch 416/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8285\n",
            "Epoch 417/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8303\n",
            "Epoch 418/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8181\n",
            "Epoch 419/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3571 - accuracy: 0.8291\n",
            "Epoch 420/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8297\n",
            "Epoch 421/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8260\n",
            "Epoch 422/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8352\n",
            "Epoch 423/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8187\n",
            "Epoch 424/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3542 - accuracy: 0.8291\n",
            "Epoch 425/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8175\n",
            "Epoch 426/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8309\n",
            "Epoch 427/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8309\n",
            "Epoch 428/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8321\n",
            "Epoch 429/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8260\n",
            "Epoch 430/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8242\n",
            "Epoch 431/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8303\n",
            "Epoch 432/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3607 - accuracy: 0.8254\n",
            "Epoch 433/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8224\n",
            "Epoch 434/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.8315\n",
            "Epoch 435/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8297\n",
            "Epoch 436/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3564 - accuracy: 0.8212\n",
            "Epoch 437/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8254\n",
            "Epoch 438/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8315\n",
            "Epoch 439/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8339\n",
            "Epoch 440/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8254\n",
            "Epoch 441/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3567 - accuracy: 0.8370\n",
            "Epoch 442/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8339\n",
            "Epoch 443/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.8327\n",
            "Epoch 444/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8333\n",
            "Epoch 445/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.8236\n",
            "Epoch 446/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.8236\n",
            "Epoch 447/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8297\n",
            "Epoch 448/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8382\n",
            "Epoch 449/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8285\n",
            "Epoch 450/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8248\n",
            "Epoch 451/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8315\n",
            "Epoch 452/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8358\n",
            "Epoch 453/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3577 - accuracy: 0.8236\n",
            "Epoch 454/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8279\n",
            "Epoch 455/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3566 - accuracy: 0.8291\n",
            "Epoch 456/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8254\n",
            "Epoch 457/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8358\n",
            "Epoch 458/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8315\n",
            "Epoch 459/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8333\n",
            "Epoch 460/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3574 - accuracy: 0.8273\n",
            "Epoch 461/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8352\n",
            "Epoch 462/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.8230\n",
            "Epoch 463/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8370\n",
            "Epoch 464/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.8297\n",
            "Epoch 465/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8297\n",
            "Epoch 466/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3512 - accuracy: 0.8321\n",
            "Epoch 467/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8181\n",
            "Epoch 468/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3533 - accuracy: 0.8291\n",
            "Epoch 469/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.3558 - accuracy: 0.8345\n",
            "Epoch 470/2000\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.3534 - accuracy: 0.8358\n",
            "Epoch 471/2000\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.3598 - accuracy: 0.8303\n",
            "Epoch 472/2000\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.3545 - accuracy: 0.8315\n",
            "Epoch 473/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.3488 - accuracy: 0.8339\n",
            "Epoch 474/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3602 - accuracy: 0.8266\n",
            "Epoch 475/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3575 - accuracy: 0.8273\n",
            "Epoch 476/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3525 - accuracy: 0.8345\n",
            "Epoch 477/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8352\n",
            "Epoch 478/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8236\n",
            "Epoch 479/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.8309\n",
            "Epoch 480/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8412\n",
            "Epoch 481/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3474 - accuracy: 0.8364\n",
            "Epoch 482/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8412\n",
            "Epoch 483/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8309\n",
            "Epoch 484/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8339\n",
            "Epoch 485/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8303\n",
            "Epoch 486/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3553 - accuracy: 0.8254\n",
            "Epoch 487/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8297\n",
            "Epoch 488/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8285\n",
            "Epoch 489/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3569 - accuracy: 0.8327\n",
            "Epoch 490/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8309\n",
            "Epoch 491/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8321\n",
            "Epoch 492/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8303\n",
            "Epoch 493/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8309\n",
            "Epoch 494/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8364\n",
            "Epoch 495/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8376\n",
            "Epoch 496/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8412\n",
            "Epoch 497/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3486 - accuracy: 0.8352\n",
            "Epoch 498/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8339\n",
            "Epoch 499/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8315\n",
            "Epoch 500/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8352\n",
            "Epoch 501/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8291\n",
            "Epoch 502/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8376\n",
            "Epoch 503/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.8303\n",
            "Epoch 504/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8333\n",
            "Epoch 505/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8333\n",
            "Epoch 506/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3473 - accuracy: 0.8309\n",
            "Epoch 507/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.8388\n",
            "Epoch 508/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8339\n",
            "Epoch 509/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8394\n",
            "Epoch 510/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.8309\n",
            "Epoch 511/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8327\n",
            "Epoch 512/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8352\n",
            "Epoch 513/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8248\n",
            "Epoch 514/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8315\n",
            "Epoch 515/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8352\n",
            "Epoch 516/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3490 - accuracy: 0.8358\n",
            "Epoch 517/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8297\n",
            "Epoch 518/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8394\n",
            "Epoch 519/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.8388\n",
            "Epoch 520/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8400\n",
            "Epoch 521/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8431\n",
            "Epoch 522/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8339\n",
            "Epoch 523/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8376\n",
            "Epoch 524/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8370\n",
            "Epoch 525/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8376\n",
            "Epoch 526/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8339\n",
            "Epoch 527/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8285\n",
            "Epoch 528/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8327\n",
            "Epoch 529/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8412\n",
            "Epoch 530/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8382\n",
            "Epoch 531/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8339\n",
            "Epoch 532/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8321\n",
            "Epoch 533/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8345\n",
            "Epoch 534/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3440 - accuracy: 0.8388\n",
            "Epoch 535/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8364\n",
            "Epoch 536/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8376\n",
            "Epoch 537/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8345\n",
            "Epoch 538/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8382\n",
            "Epoch 539/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8376\n",
            "Epoch 540/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8382\n",
            "Epoch 541/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8352\n",
            "Epoch 542/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8412\n",
            "Epoch 543/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8352\n",
            "Epoch 544/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8376\n",
            "Epoch 545/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8382\n",
            "Epoch 546/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8370\n",
            "Epoch 547/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8370\n",
            "Epoch 548/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8321\n",
            "Epoch 549/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8364\n",
            "Epoch 550/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8345\n",
            "Epoch 551/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8376\n",
            "Epoch 552/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.8266\n",
            "Epoch 553/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.8345\n",
            "Epoch 554/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8339\n",
            "Epoch 555/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8358\n",
            "Epoch 556/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3410 - accuracy: 0.8418\n",
            "Epoch 557/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8382\n",
            "Epoch 558/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8382\n",
            "Epoch 559/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8327\n",
            "Epoch 560/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8431\n",
            "Epoch 561/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3435 - accuracy: 0.8418\n",
            "Epoch 562/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8358\n",
            "Epoch 563/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3478 - accuracy: 0.8327\n",
            "Epoch 564/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8400\n",
            "Epoch 565/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8388\n",
            "Epoch 566/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8449\n",
            "Epoch 567/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8425\n",
            "Epoch 568/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8358\n",
            "Epoch 569/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3401 - accuracy: 0.8376\n",
            "Epoch 570/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3381 - accuracy: 0.8400\n",
            "Epoch 571/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8455\n",
            "Epoch 572/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3399 - accuracy: 0.8358\n",
            "Epoch 573/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8370\n",
            "Epoch 574/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8394\n",
            "Epoch 575/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3428 - accuracy: 0.8321\n",
            "Epoch 576/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8309\n",
            "Epoch 577/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8327\n",
            "Epoch 578/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8279\n",
            "Epoch 579/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8437\n",
            "Epoch 580/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8321\n",
            "Epoch 581/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8400\n",
            "Epoch 582/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3413 - accuracy: 0.8358\n",
            "Epoch 583/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8388\n",
            "Epoch 584/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.8364\n",
            "Epoch 585/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8309\n",
            "Epoch 586/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8345\n",
            "Epoch 587/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3404 - accuracy: 0.8388\n",
            "Epoch 588/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8333\n",
            "Epoch 589/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3443 - accuracy: 0.8315\n",
            "Epoch 590/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8443\n",
            "Epoch 591/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8394\n",
            "Epoch 592/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8376\n",
            "Epoch 593/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8412\n",
            "Epoch 594/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8394\n",
            "Epoch 595/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.8418\n",
            "Epoch 596/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.8412\n",
            "Epoch 597/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8473\n",
            "Epoch 598/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8443\n",
            "Epoch 599/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8400\n",
            "Epoch 600/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8339\n",
            "Epoch 601/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3332 - accuracy: 0.8412\n",
            "Epoch 602/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8333\n",
            "Epoch 603/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8449\n",
            "Epoch 604/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8467\n",
            "Epoch 605/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8479\n",
            "Epoch 606/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3449 - accuracy: 0.8376\n",
            "Epoch 607/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8345\n",
            "Epoch 608/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3332 - accuracy: 0.8443\n",
            "Epoch 609/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8516\n",
            "Epoch 610/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8297\n",
            "Epoch 611/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3373 - accuracy: 0.8425\n",
            "Epoch 612/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8376\n",
            "Epoch 613/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8412\n",
            "Epoch 614/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8394\n",
            "Epoch 615/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8443\n",
            "Epoch 616/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8400\n",
            "Epoch 617/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.8461\n",
            "Epoch 618/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8485\n",
            "Epoch 619/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8455\n",
            "Epoch 620/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8364\n",
            "Epoch 621/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3444 - accuracy: 0.8358\n",
            "Epoch 622/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8382\n",
            "Epoch 623/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8473\n",
            "Epoch 624/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.8303\n",
            "Epoch 625/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8467\n",
            "Epoch 626/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8358\n",
            "Epoch 627/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8418\n",
            "Epoch 628/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8388\n",
            "Epoch 629/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8273\n",
            "Epoch 630/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8370\n",
            "Epoch 631/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8382\n",
            "Epoch 632/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8491\n",
            "Epoch 633/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8418\n",
            "Epoch 634/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.8455\n",
            "Epoch 635/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8479\n",
            "Epoch 636/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8388\n",
            "Epoch 637/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8449\n",
            "Epoch 638/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8364\n",
            "Epoch 639/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8333\n",
            "Epoch 640/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8394\n",
            "Epoch 641/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8516\n",
            "Epoch 642/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8412\n",
            "Epoch 643/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8443\n",
            "Epoch 644/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8467\n",
            "Epoch 645/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8455\n",
            "Epoch 646/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8485\n",
            "Epoch 647/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3306 - accuracy: 0.8455\n",
            "Epoch 648/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8498\n",
            "Epoch 649/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8303\n",
            "Epoch 650/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3382 - accuracy: 0.8406\n",
            "Epoch 651/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8431\n",
            "Epoch 652/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3286 - accuracy: 0.8504\n",
            "Epoch 653/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8412\n",
            "Epoch 654/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8455\n",
            "Epoch 655/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8333\n",
            "Epoch 656/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8455\n",
            "Epoch 657/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8437\n",
            "Epoch 658/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8485\n",
            "Epoch 659/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8431\n",
            "Epoch 660/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8455\n",
            "Epoch 661/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8291\n",
            "Epoch 662/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8491\n",
            "Epoch 663/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8473\n",
            "Epoch 664/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8455\n",
            "Epoch 665/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.8510\n",
            "Epoch 666/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8437\n",
            "Epoch 667/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8558\n",
            "Epoch 668/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8504\n",
            "Epoch 669/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8455\n",
            "Epoch 670/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8443\n",
            "Epoch 671/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8443\n",
            "Epoch 672/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8455\n",
            "Epoch 673/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8352\n",
            "Epoch 674/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8418\n",
            "Epoch 675/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8400\n",
            "Epoch 676/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3327 - accuracy: 0.8455\n",
            "Epoch 677/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8412\n",
            "Epoch 678/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.8437\n",
            "Epoch 679/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8479\n",
            "Epoch 680/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8473\n",
            "Epoch 681/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8412\n",
            "Epoch 682/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3311 - accuracy: 0.8425\n",
            "Epoch 683/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8498\n",
            "Epoch 684/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8449\n",
            "Epoch 685/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3298 - accuracy: 0.8376\n",
            "Epoch 686/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8504\n",
            "Epoch 687/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8431\n",
            "Epoch 688/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8479\n",
            "Epoch 689/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8461\n",
            "Epoch 690/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8425\n",
            "Epoch 691/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8461\n",
            "Epoch 692/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8376\n",
            "Epoch 693/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8528\n",
            "Epoch 694/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8437\n",
            "Epoch 695/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3306 - accuracy: 0.8431\n",
            "Epoch 696/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8467\n",
            "Epoch 697/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8516\n",
            "Epoch 698/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3300 - accuracy: 0.8394\n",
            "Epoch 699/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8400\n",
            "Epoch 700/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8412\n",
            "Epoch 701/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8534\n",
            "Epoch 702/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8443\n",
            "Epoch 703/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8443\n",
            "Epoch 704/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8516\n",
            "Epoch 705/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.8491\n",
            "Epoch 706/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8339\n",
            "Epoch 707/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8437\n",
            "Epoch 708/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8467\n",
            "Epoch 709/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8491\n",
            "Epoch 710/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3257 - accuracy: 0.8498\n",
            "Epoch 711/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3247 - accuracy: 0.8473\n",
            "Epoch 712/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8522\n",
            "Epoch 713/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8491\n",
            "Epoch 714/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8467\n",
            "Epoch 715/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8455\n",
            "Epoch 716/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8455\n",
            "Epoch 717/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8412\n",
            "Epoch 718/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8431\n",
            "Epoch 719/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8418\n",
            "Epoch 720/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8491\n",
            "Epoch 721/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8467\n",
            "Epoch 722/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8473\n",
            "Epoch 723/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8406\n",
            "Epoch 724/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8467\n",
            "Epoch 725/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8455\n",
            "Epoch 726/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3293 - accuracy: 0.8437\n",
            "Epoch 727/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8406\n",
            "Epoch 728/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8485\n",
            "Epoch 729/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3250 - accuracy: 0.8449\n",
            "Epoch 730/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3207 - accuracy: 0.8540\n",
            "Epoch 731/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8455\n",
            "Epoch 732/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8491\n",
            "Epoch 733/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8467\n",
            "Epoch 734/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8498\n",
            "Epoch 735/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8498\n",
            "Epoch 736/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8528\n",
            "Epoch 737/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8552\n",
            "Epoch 738/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8467\n",
            "Epoch 739/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8528\n",
            "Epoch 740/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8376\n",
            "Epoch 741/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3224 - accuracy: 0.8522\n",
            "Epoch 742/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8437\n",
            "Epoch 743/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8534\n",
            "Epoch 744/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3170 - accuracy: 0.8564\n",
            "Epoch 745/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8485\n",
            "Epoch 746/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3328 - accuracy: 0.8455\n",
            "Epoch 747/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3212 - accuracy: 0.8498\n",
            "Epoch 748/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8534\n",
            "Epoch 749/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3182 - accuracy: 0.8467\n",
            "Epoch 750/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8473\n",
            "Epoch 751/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8522\n",
            "Epoch 752/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3191 - accuracy: 0.8455\n",
            "Epoch 753/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3224 - accuracy: 0.8522\n",
            "Epoch 754/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8504\n",
            "Epoch 755/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8504\n",
            "Epoch 756/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8534\n",
            "Epoch 757/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3224 - accuracy: 0.8485\n",
            "Epoch 758/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8571\n",
            "Epoch 759/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3186 - accuracy: 0.8546\n",
            "Epoch 760/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3237 - accuracy: 0.8425\n",
            "Epoch 761/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3143 - accuracy: 0.8534\n",
            "Epoch 762/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8479\n",
            "Epoch 763/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8485\n",
            "Epoch 764/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3232 - accuracy: 0.8455\n",
            "Epoch 765/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8479\n",
            "Epoch 766/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8412\n",
            "Epoch 767/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8504\n",
            "Epoch 768/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8467\n",
            "Epoch 769/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8522\n",
            "Epoch 770/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3224 - accuracy: 0.8522\n",
            "Epoch 771/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8510\n",
            "Epoch 772/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8516\n",
            "Epoch 773/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8491\n",
            "Epoch 774/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8516\n",
            "Epoch 775/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8498\n",
            "Epoch 776/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8491\n",
            "Epoch 777/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8431\n",
            "Epoch 778/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.8510\n",
            "Epoch 779/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8461\n",
            "Epoch 780/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8540\n",
            "Epoch 781/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8485\n",
            "Epoch 782/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8473\n",
            "Epoch 783/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8431\n",
            "Epoch 784/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8467\n",
            "Epoch 785/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8479\n",
            "Epoch 786/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8479\n",
            "Epoch 787/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3191 - accuracy: 0.8546\n",
            "Epoch 788/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8540\n",
            "Epoch 789/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8516\n",
            "Epoch 790/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8443\n",
            "Epoch 791/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.8540\n",
            "Epoch 792/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3153 - accuracy: 0.8443\n",
            "Epoch 793/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3164 - accuracy: 0.8577\n",
            "Epoch 794/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3140 - accuracy: 0.8473\n",
            "Epoch 795/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8491\n",
            "Epoch 796/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8473\n",
            "Epoch 797/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8510\n",
            "Epoch 798/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8522\n",
            "Epoch 799/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8510\n",
            "Epoch 800/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8491\n",
            "Epoch 801/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3167 - accuracy: 0.8516\n",
            "Epoch 802/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3142 - accuracy: 0.8546\n",
            "Epoch 803/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8534\n",
            "Epoch 804/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8540\n",
            "Epoch 805/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3164 - accuracy: 0.8577\n",
            "Epoch 806/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.8437\n",
            "Epoch 807/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8418\n",
            "Epoch 808/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3145 - accuracy: 0.8522\n",
            "Epoch 809/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3153 - accuracy: 0.8601\n",
            "Epoch 810/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3155 - accuracy: 0.8522\n",
            "Epoch 811/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8534\n",
            "Epoch 812/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8552\n",
            "Epoch 813/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3164 - accuracy: 0.8491\n",
            "Epoch 814/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8504\n",
            "Epoch 815/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3148 - accuracy: 0.8534\n",
            "Epoch 816/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8418\n",
            "Epoch 817/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8510\n",
            "Epoch 818/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8498\n",
            "Epoch 819/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8571\n",
            "Epoch 820/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8516\n",
            "Epoch 821/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8437\n",
            "Epoch 822/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8522\n",
            "Epoch 823/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.8540\n",
            "Epoch 824/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8534\n",
            "Epoch 825/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8558\n",
            "Epoch 826/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8516\n",
            "Epoch 827/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8528\n",
            "Epoch 828/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.8437\n",
            "Epoch 829/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.8485\n",
            "Epoch 830/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3154 - accuracy: 0.8540\n",
            "Epoch 831/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8522\n",
            "Epoch 832/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8425\n",
            "Epoch 833/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8504\n",
            "Epoch 834/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8479\n",
            "Epoch 835/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8510\n",
            "Epoch 836/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3143 - accuracy: 0.8504\n",
            "Epoch 837/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8583\n",
            "Epoch 838/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8558\n",
            "Epoch 839/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8461\n",
            "Epoch 840/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3128 - accuracy: 0.8510\n",
            "Epoch 841/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8583\n",
            "Epoch 842/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8485\n",
            "Epoch 843/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.8491\n",
            "Epoch 844/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.8449\n",
            "Epoch 845/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3116 - accuracy: 0.8564\n",
            "Epoch 846/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.8540\n",
            "Epoch 847/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8613\n",
            "Epoch 848/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8485\n",
            "Epoch 849/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3159 - accuracy: 0.8504\n",
            "Epoch 850/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8498\n",
            "Epoch 851/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8516\n",
            "Epoch 852/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8504\n",
            "Epoch 853/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8498\n",
            "Epoch 854/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8504\n",
            "Epoch 855/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8552\n",
            "Epoch 856/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3102 - accuracy: 0.8595\n",
            "Epoch 857/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8498\n",
            "Epoch 858/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8510\n",
            "Epoch 859/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8522\n",
            "Epoch 860/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8528\n",
            "Epoch 861/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8607\n",
            "Epoch 862/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3132 - accuracy: 0.8516\n",
            "Epoch 863/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.8504\n",
            "Epoch 864/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8546\n",
            "Epoch 865/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8491\n",
            "Epoch 866/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8522\n",
            "Epoch 867/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3149 - accuracy: 0.8540\n",
            "Epoch 868/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8601\n",
            "Epoch 869/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3095 - accuracy: 0.8558\n",
            "Epoch 870/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3137 - accuracy: 0.8449\n",
            "Epoch 871/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8516\n",
            "Epoch 872/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.8473\n",
            "Epoch 873/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8461\n",
            "Epoch 874/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8589\n",
            "Epoch 875/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8534\n",
            "Epoch 876/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3149 - accuracy: 0.8534\n",
            "Epoch 877/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8510\n",
            "Epoch 878/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.8522\n",
            "Epoch 879/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8461\n",
            "Epoch 880/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.8528\n",
            "Epoch 881/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.8619\n",
            "Epoch 882/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.8522\n",
            "Epoch 883/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8552\n",
            "Epoch 884/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.8522\n",
            "Epoch 885/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3099 - accuracy: 0.8571\n",
            "Epoch 886/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3153 - accuracy: 0.8479\n",
            "Epoch 887/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8534\n",
            "Epoch 888/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8510\n",
            "Epoch 889/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8552\n",
            "Epoch 890/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8467\n",
            "Epoch 891/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3096 - accuracy: 0.8534\n",
            "Epoch 892/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8607\n",
            "Epoch 893/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8613\n",
            "Epoch 894/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8613\n",
            "Epoch 895/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3172 - accuracy: 0.8400\n",
            "Epoch 896/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8558\n",
            "Epoch 897/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3107 - accuracy: 0.8601\n",
            "Epoch 898/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8564\n",
            "Epoch 899/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8455\n",
            "Epoch 900/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.8601\n",
            "Epoch 901/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3121 - accuracy: 0.8595\n",
            "Epoch 902/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8595\n",
            "Epoch 903/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3076 - accuracy: 0.8552\n",
            "Epoch 904/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3065 - accuracy: 0.8619\n",
            "Epoch 905/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.8516\n",
            "Epoch 906/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8583\n",
            "Epoch 907/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3106 - accuracy: 0.8552\n",
            "Epoch 908/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8498\n",
            "Epoch 909/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3054 - accuracy: 0.8637\n",
            "Epoch 910/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3079 - accuracy: 0.8625\n",
            "Epoch 911/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3172 - accuracy: 0.8455\n",
            "Epoch 912/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3094 - accuracy: 0.8534\n",
            "Epoch 913/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3153 - accuracy: 0.8510\n",
            "Epoch 914/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8534\n",
            "Epoch 915/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8528\n",
            "Epoch 916/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3122 - accuracy: 0.8498\n",
            "Epoch 917/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3030 - accuracy: 0.8571\n",
            "Epoch 918/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8491\n",
            "Epoch 919/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8510\n",
            "Epoch 920/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8504\n",
            "Epoch 921/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3096 - accuracy: 0.8564\n",
            "Epoch 922/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.8577\n",
            "Epoch 923/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3047 - accuracy: 0.8619\n",
            "Epoch 924/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3121 - accuracy: 0.8528\n",
            "Epoch 925/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8692\n",
            "Epoch 926/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.8516\n",
            "Epoch 927/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3107 - accuracy: 0.8534\n",
            "Epoch 928/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3054 - accuracy: 0.8552\n",
            "Epoch 929/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8540\n",
            "Epoch 930/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3118 - accuracy: 0.8534\n",
            "Epoch 931/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3090 - accuracy: 0.8595\n",
            "Epoch 932/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3098 - accuracy: 0.8540\n",
            "Epoch 933/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8564\n",
            "Epoch 934/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3106 - accuracy: 0.8540\n",
            "Epoch 935/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8619\n",
            "Epoch 936/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8619\n",
            "Epoch 937/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3056 - accuracy: 0.8558\n",
            "Epoch 938/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3101 - accuracy: 0.8510\n",
            "Epoch 939/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3031 - accuracy: 0.8571\n",
            "Epoch 940/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3145 - accuracy: 0.8479\n",
            "Epoch 941/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3102 - accuracy: 0.8449\n",
            "Epoch 942/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8583\n",
            "Epoch 943/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8522\n",
            "Epoch 944/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3130 - accuracy: 0.8504\n",
            "Epoch 945/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8546\n",
            "Epoch 946/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.8625\n",
            "Epoch 947/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.8668\n",
            "Epoch 948/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.8510\n",
            "Epoch 949/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.8613\n",
            "Epoch 950/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3027 - accuracy: 0.8583\n",
            "Epoch 951/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8534\n",
            "Epoch 952/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8583\n",
            "Epoch 953/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8571\n",
            "Epoch 954/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.8601\n",
            "Epoch 955/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3080 - accuracy: 0.8571\n",
            "Epoch 956/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8534\n",
            "Epoch 957/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8571\n",
            "Epoch 958/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8504\n",
            "Epoch 959/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8546\n",
            "Epoch 960/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3072 - accuracy: 0.8498\n",
            "Epoch 961/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3035 - accuracy: 0.8644\n",
            "Epoch 962/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8552\n",
            "Epoch 963/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8571\n",
            "Epoch 964/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3053 - accuracy: 0.8595\n",
            "Epoch 965/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3069 - accuracy: 0.8546\n",
            "Epoch 966/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3070 - accuracy: 0.8607\n",
            "Epoch 967/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.8595\n",
            "Epoch 968/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8510\n",
            "Epoch 969/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8552\n",
            "Epoch 970/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3070 - accuracy: 0.8510\n",
            "Epoch 971/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3102 - accuracy: 0.8558\n",
            "Epoch 972/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3035 - accuracy: 0.8613\n",
            "Epoch 973/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8662\n",
            "Epoch 974/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8528\n",
            "Epoch 975/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8583\n",
            "Epoch 976/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3080 - accuracy: 0.8571\n",
            "Epoch 977/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8571\n",
            "Epoch 978/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3060 - accuracy: 0.8534\n",
            "Epoch 979/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8589\n",
            "Epoch 980/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.8625\n",
            "Epoch 981/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3080 - accuracy: 0.8552\n",
            "Epoch 982/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3107 - accuracy: 0.8498\n",
            "Epoch 983/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3019 - accuracy: 0.8601\n",
            "Epoch 984/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3021 - accuracy: 0.8540\n",
            "Epoch 985/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.8589\n",
            "Epoch 986/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3026 - accuracy: 0.8595\n",
            "Epoch 987/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8552\n",
            "Epoch 988/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3017 - accuracy: 0.8607\n",
            "Epoch 989/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8601\n",
            "Epoch 990/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3067 - accuracy: 0.8601\n",
            "Epoch 991/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8564\n",
            "Epoch 992/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.8571\n",
            "Epoch 993/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.8564\n",
            "Epoch 994/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8571\n",
            "Epoch 995/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8595\n",
            "Epoch 996/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3057 - accuracy: 0.8577\n",
            "Epoch 997/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8516\n",
            "Epoch 998/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.8534\n",
            "Epoch 999/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3017 - accuracy: 0.8607\n",
            "Epoch 1000/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.8583\n",
            "Epoch 1001/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8613\n",
            "Epoch 1002/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.8552\n",
            "Epoch 1003/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.8619\n",
            "Epoch 1004/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.8595\n",
            "Epoch 1005/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2994 - accuracy: 0.8564\n",
            "Epoch 1006/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3012 - accuracy: 0.8540\n",
            "Epoch 1007/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.8583\n",
            "Epoch 1008/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.8485\n",
            "Epoch 1009/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3040 - accuracy: 0.8552\n",
            "Epoch 1010/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.8619\n",
            "Epoch 1011/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8564\n",
            "Epoch 1012/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8583\n",
            "Epoch 1013/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.8601\n",
            "Epoch 1014/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.8601\n",
            "Epoch 1015/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.8595\n",
            "Epoch 1016/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.8534\n",
            "Epoch 1017/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3019 - accuracy: 0.8571\n",
            "Epoch 1018/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2995 - accuracy: 0.8571\n",
            "Epoch 1019/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2992 - accuracy: 0.8656\n",
            "Epoch 1020/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2981 - accuracy: 0.8577\n",
            "Epoch 1021/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8583\n",
            "Epoch 1022/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8668\n",
            "Epoch 1023/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3090 - accuracy: 0.8498\n",
            "Epoch 1024/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8613\n",
            "Epoch 1025/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3049 - accuracy: 0.8552\n",
            "Epoch 1026/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8601\n",
            "Epoch 1027/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3009 - accuracy: 0.8644\n",
            "Epoch 1028/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8637\n",
            "Epoch 1029/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8650\n",
            "Epoch 1030/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.8504\n",
            "Epoch 1031/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.8619\n",
            "Epoch 1032/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.8589\n",
            "Epoch 1033/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8589\n",
            "Epoch 1034/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.8583\n",
            "Epoch 1035/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2992 - accuracy: 0.8650\n",
            "Epoch 1036/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2980 - accuracy: 0.8625\n",
            "Epoch 1037/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2972 - accuracy: 0.8552\n",
            "Epoch 1038/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3058 - accuracy: 0.8528\n",
            "Epoch 1039/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3047 - accuracy: 0.8552\n",
            "Epoch 1040/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2965 - accuracy: 0.8631\n",
            "Epoch 1041/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2991 - accuracy: 0.8583\n",
            "Epoch 1042/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2982 - accuracy: 0.8650\n",
            "Epoch 1043/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3046 - accuracy: 0.8552\n",
            "Epoch 1044/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2989 - accuracy: 0.8637\n",
            "Epoch 1045/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8571\n",
            "Epoch 1046/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3000 - accuracy: 0.8571\n",
            "Epoch 1047/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.8656\n",
            "Epoch 1048/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8601\n",
            "Epoch 1049/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.8595\n",
            "Epoch 1050/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.8704\n",
            "Epoch 1051/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8601\n",
            "Epoch 1052/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.8577\n",
            "Epoch 1053/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.8619\n",
            "Epoch 1054/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8540\n",
            "Epoch 1055/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8504\n",
            "Epoch 1056/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8625\n",
            "Epoch 1057/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2954 - accuracy: 0.8674\n",
            "Epoch 1058/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8564\n",
            "Epoch 1059/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.8674\n",
            "Epoch 1060/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.8613\n",
            "Epoch 1061/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8601\n",
            "Epoch 1062/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8546\n",
            "Epoch 1063/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8644\n",
            "Epoch 1064/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8571\n",
            "Epoch 1065/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2939 - accuracy: 0.8656\n",
            "Epoch 1066/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8589\n",
            "Epoch 1067/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3032 - accuracy: 0.8510\n",
            "Epoch 1068/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8625\n",
            "Epoch 1069/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2974 - accuracy: 0.8662\n",
            "Epoch 1070/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8631\n",
            "Epoch 1071/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.8601\n",
            "Epoch 1072/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.8650\n",
            "Epoch 1073/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2925 - accuracy: 0.8625\n",
            "Epoch 1074/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8534\n",
            "Epoch 1075/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8680\n",
            "Epoch 1076/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8601\n",
            "Epoch 1077/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3003 - accuracy: 0.8528\n",
            "Epoch 1078/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8571\n",
            "Epoch 1079/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8625\n",
            "Epoch 1080/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2961 - accuracy: 0.8571\n",
            "Epoch 1081/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8571\n",
            "Epoch 1082/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8656\n",
            "Epoch 1083/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8601\n",
            "Epoch 1084/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.8601\n",
            "Epoch 1085/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8607\n",
            "Epoch 1086/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.8571\n",
            "Epoch 1087/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8680\n",
            "Epoch 1088/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2941 - accuracy: 0.8607\n",
            "Epoch 1089/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8698\n",
            "Epoch 1090/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.8656\n",
            "Epoch 1091/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.8625\n",
            "Epoch 1092/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2925 - accuracy: 0.8656\n",
            "Epoch 1093/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.8613\n",
            "Epoch 1094/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3029 - accuracy: 0.8583\n",
            "Epoch 1095/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.8625\n",
            "Epoch 1096/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2918 - accuracy: 0.8680\n",
            "Epoch 1097/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8613\n",
            "Epoch 1098/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8717\n",
            "Epoch 1099/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8710\n",
            "Epoch 1100/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3007 - accuracy: 0.8656\n",
            "Epoch 1101/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2997 - accuracy: 0.8479\n",
            "Epoch 1102/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8637\n",
            "Epoch 1103/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8710\n",
            "Epoch 1104/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.8619\n",
            "Epoch 1105/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8595\n",
            "Epoch 1106/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2975 - accuracy: 0.8571\n",
            "Epoch 1107/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8528\n",
            "Epoch 1108/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8601\n",
            "Epoch 1109/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.8552\n",
            "Epoch 1110/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.8662\n",
            "Epoch 1111/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2899 - accuracy: 0.8613\n",
            "Epoch 1112/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2911 - accuracy: 0.8637\n",
            "Epoch 1113/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8625\n",
            "Epoch 1114/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.8637\n",
            "Epoch 1115/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8619\n",
            "Epoch 1116/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2980 - accuracy: 0.8698\n",
            "Epoch 1117/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8692\n",
            "Epoch 1118/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8625\n",
            "Epoch 1119/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2878 - accuracy: 0.8619\n",
            "Epoch 1120/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2964 - accuracy: 0.8631\n",
            "Epoch 1121/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.8704\n",
            "Epoch 1122/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8692\n",
            "Epoch 1123/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2980 - accuracy: 0.8625\n",
            "Epoch 1124/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8686\n",
            "Epoch 1125/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8607\n",
            "Epoch 1126/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2913 - accuracy: 0.8607\n",
            "Epoch 1127/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.8644\n",
            "Epoch 1128/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8577\n",
            "Epoch 1129/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.8698\n",
            "Epoch 1130/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8656\n",
            "Epoch 1131/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.8741\n",
            "Epoch 1132/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2989 - accuracy: 0.8601\n",
            "Epoch 1133/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2908 - accuracy: 0.8619\n",
            "Epoch 1134/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2892 - accuracy: 0.8656\n",
            "Epoch 1135/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8710\n",
            "Epoch 1136/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3036 - accuracy: 0.8546\n",
            "Epoch 1137/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8729\n",
            "Epoch 1138/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2904 - accuracy: 0.8662\n",
            "Epoch 1139/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8692\n",
            "Epoch 1140/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2905 - accuracy: 0.8686\n",
            "Epoch 1141/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.8680\n",
            "Epoch 1142/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8686\n",
            "Epoch 1143/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2874 - accuracy: 0.8723\n",
            "Epoch 1144/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8698\n",
            "Epoch 1145/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2904 - accuracy: 0.8644\n",
            "Epoch 1146/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2874 - accuracy: 0.8644\n",
            "Epoch 1147/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8662\n",
            "Epoch 1148/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8680\n",
            "Epoch 1149/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2922 - accuracy: 0.8631\n",
            "Epoch 1150/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2920 - accuracy: 0.8680\n",
            "Epoch 1151/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2964 - accuracy: 0.8589\n",
            "Epoch 1152/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8650\n",
            "Epoch 1153/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8668\n",
            "Epoch 1154/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2976 - accuracy: 0.8564\n",
            "Epoch 1155/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8735\n",
            "Epoch 1156/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8674\n",
            "Epoch 1157/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8607\n",
            "Epoch 1158/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8650\n",
            "Epoch 1159/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8729\n",
            "Epoch 1160/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2928 - accuracy: 0.8650\n",
            "Epoch 1161/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8650\n",
            "Epoch 1162/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8637\n",
            "Epoch 1163/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.8637\n",
            "Epoch 1164/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2899 - accuracy: 0.8668\n",
            "Epoch 1165/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8613\n",
            "Epoch 1166/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8674\n",
            "Epoch 1167/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.8656\n",
            "Epoch 1168/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2878 - accuracy: 0.8680\n",
            "Epoch 1169/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8571\n",
            "Epoch 1170/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8680\n",
            "Epoch 1171/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2904 - accuracy: 0.8710\n",
            "Epoch 1172/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2883 - accuracy: 0.8607\n",
            "Epoch 1173/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8528\n",
            "Epoch 1174/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2898 - accuracy: 0.8729\n",
            "Epoch 1175/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8552\n",
            "Epoch 1176/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.8644\n",
            "Epoch 1177/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8668\n",
            "Epoch 1178/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2900 - accuracy: 0.8668\n",
            "Epoch 1179/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.8680\n",
            "Epoch 1180/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.8631\n",
            "Epoch 1181/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8674\n",
            "Epoch 1182/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8637\n",
            "Epoch 1183/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8589\n",
            "Epoch 1184/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2897 - accuracy: 0.8674\n",
            "Epoch 1185/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8644\n",
            "Epoch 1186/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8619\n",
            "Epoch 1187/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2915 - accuracy: 0.8668\n",
            "Epoch 1188/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8686\n",
            "Epoch 1189/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2890 - accuracy: 0.8644\n",
            "Epoch 1190/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8644\n",
            "Epoch 1191/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.8674\n",
            "Epoch 1192/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.8619\n",
            "Epoch 1193/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.8631\n",
            "Epoch 1194/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2926 - accuracy: 0.8631\n",
            "Epoch 1195/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8680\n",
            "Epoch 1196/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8564\n",
            "Epoch 1197/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2831 - accuracy: 0.8644\n",
            "Epoch 1198/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8577\n",
            "Epoch 1199/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8674\n",
            "Epoch 1200/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2920 - accuracy: 0.8619\n",
            "Epoch 1201/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2888 - accuracy: 0.8710\n",
            "Epoch 1202/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8625\n",
            "Epoch 1203/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8601\n",
            "Epoch 1204/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8601\n",
            "Epoch 1205/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8637\n",
            "Epoch 1206/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8723\n",
            "Epoch 1207/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2867 - accuracy: 0.8656\n",
            "Epoch 1208/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2893 - accuracy: 0.8741\n",
            "Epoch 1209/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.8656\n",
            "Epoch 1210/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8644\n",
            "Epoch 1211/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.8723\n",
            "Epoch 1212/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2816 - accuracy: 0.8747\n",
            "Epoch 1213/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2910 - accuracy: 0.8656\n",
            "Epoch 1214/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2874 - accuracy: 0.8637\n",
            "Epoch 1215/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8668\n",
            "Epoch 1216/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8650\n",
            "Epoch 1217/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2884 - accuracy: 0.8631\n",
            "Epoch 1218/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.8717\n",
            "Epoch 1219/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.8656\n",
            "Epoch 1220/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2921 - accuracy: 0.8650\n",
            "Epoch 1221/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.2896 - accuracy: 0.8729\n",
            "Epoch 1222/2000\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.2858 - accuracy: 0.8631\n",
            "Epoch 1223/2000\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.2852 - accuracy: 0.8698\n",
            "Epoch 1224/2000\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.2866 - accuracy: 0.8644\n",
            "Epoch 1225/2000\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.2822 - accuracy: 0.8704\n",
            "Epoch 1226/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.8644\n",
            "Epoch 1227/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8747\n",
            "Epoch 1228/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8790\n",
            "Epoch 1229/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2909 - accuracy: 0.8698\n",
            "Epoch 1230/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8729\n",
            "Epoch 1231/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8601\n",
            "Epoch 1232/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8650\n",
            "Epoch 1233/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2807 - accuracy: 0.8686\n",
            "Epoch 1234/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2870 - accuracy: 0.8644\n",
            "Epoch 1235/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.8747\n",
            "Epoch 1236/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8656\n",
            "Epoch 1237/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2850 - accuracy: 0.8631\n",
            "Epoch 1238/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.8710\n",
            "Epoch 1239/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2817 - accuracy: 0.8723\n",
            "Epoch 1240/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.8589\n",
            "Epoch 1241/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.8735\n",
            "Epoch 1242/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2844 - accuracy: 0.8662\n",
            "Epoch 1243/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2816 - accuracy: 0.8680\n",
            "Epoch 1244/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.8631\n",
            "Epoch 1245/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8704\n",
            "Epoch 1246/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8686\n",
            "Epoch 1247/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8717\n",
            "Epoch 1248/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8704\n",
            "Epoch 1249/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8662\n",
            "Epoch 1250/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8686\n",
            "Epoch 1251/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2844 - accuracy: 0.8607\n",
            "Epoch 1252/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8698\n",
            "Epoch 1253/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.8589\n",
            "Epoch 1254/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.8607\n",
            "Epoch 1255/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2822 - accuracy: 0.8698\n",
            "Epoch 1256/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8698\n",
            "Epoch 1257/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2819 - accuracy: 0.8698\n",
            "Epoch 1258/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8595\n",
            "Epoch 1259/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.8698\n",
            "Epoch 1260/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8759\n",
            "Epoch 1261/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8735\n",
            "Epoch 1262/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8783\n",
            "Epoch 1263/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8704\n",
            "Epoch 1264/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.8692\n",
            "Epoch 1265/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.8704\n",
            "Epoch 1266/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2892 - accuracy: 0.8674\n",
            "Epoch 1267/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.8735\n",
            "Epoch 1268/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.8680\n",
            "Epoch 1269/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2831 - accuracy: 0.8710\n",
            "Epoch 1270/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2850 - accuracy: 0.8704\n",
            "Epoch 1271/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8771\n",
            "Epoch 1272/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.8656\n",
            "Epoch 1273/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2791 - accuracy: 0.8704\n",
            "Epoch 1274/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8753\n",
            "Epoch 1275/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8729\n",
            "Epoch 1276/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8710\n",
            "Epoch 1277/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8717\n",
            "Epoch 1278/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8558\n",
            "Epoch 1279/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2789 - accuracy: 0.8698\n",
            "Epoch 1280/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8783\n",
            "Epoch 1281/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.8729\n",
            "Epoch 1282/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2835 - accuracy: 0.8704\n",
            "Epoch 1283/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8674\n",
            "Epoch 1284/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2862 - accuracy: 0.8680\n",
            "Epoch 1285/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8704\n",
            "Epoch 1286/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2888 - accuracy: 0.8686\n",
            "Epoch 1287/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8698\n",
            "Epoch 1288/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.8650\n",
            "Epoch 1289/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8680\n",
            "Epoch 1290/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2777 - accuracy: 0.8790\n",
            "Epoch 1291/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2796 - accuracy: 0.8717\n",
            "Epoch 1292/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8692\n",
            "Epoch 1293/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.8741\n",
            "Epoch 1294/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2751 - accuracy: 0.8735\n",
            "Epoch 1295/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.8686\n",
            "Epoch 1296/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8686\n",
            "Epoch 1297/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.8668\n",
            "Epoch 1298/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8692\n",
            "Epoch 1299/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8710\n",
            "Epoch 1300/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8741\n",
            "Epoch 1301/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8729\n",
            "Epoch 1302/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2795 - accuracy: 0.8704\n",
            "Epoch 1303/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.8631\n",
            "Epoch 1304/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.8698\n",
            "Epoch 1305/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8686\n",
            "Epoch 1306/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2818 - accuracy: 0.8704\n",
            "Epoch 1307/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2783 - accuracy: 0.8710\n",
            "Epoch 1308/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2766 - accuracy: 0.8741\n",
            "Epoch 1309/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8802\n",
            "Epoch 1310/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8735\n",
            "Epoch 1311/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2779 - accuracy: 0.8704\n",
            "Epoch 1312/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8680\n",
            "Epoch 1313/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8644\n",
            "Epoch 1314/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2897 - accuracy: 0.8662\n",
            "Epoch 1315/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2796 - accuracy: 0.8753\n",
            "Epoch 1316/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8717\n",
            "Epoch 1317/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2892 - accuracy: 0.8777\n",
            "Epoch 1318/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.8710\n",
            "Epoch 1319/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2853 - accuracy: 0.8741\n",
            "Epoch 1320/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2761 - accuracy: 0.8802\n",
            "Epoch 1321/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2791 - accuracy: 0.8704\n",
            "Epoch 1322/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8625\n",
            "Epoch 1323/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8668\n",
            "Epoch 1324/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.8674\n",
            "Epoch 1325/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2789 - accuracy: 0.8680\n",
            "Epoch 1326/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8680\n",
            "Epoch 1327/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.8650\n",
            "Epoch 1328/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8814\n",
            "Epoch 1329/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2813 - accuracy: 0.8680\n",
            "Epoch 1330/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2907 - accuracy: 0.8686\n",
            "Epoch 1331/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8753\n",
            "Epoch 1332/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8631\n",
            "Epoch 1333/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8735\n",
            "Epoch 1334/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8698\n",
            "Epoch 1335/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8747\n",
            "Epoch 1336/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.8710\n",
            "Epoch 1337/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2764 - accuracy: 0.8717\n",
            "Epoch 1338/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2836 - accuracy: 0.8656\n",
            "Epoch 1339/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8844\n",
            "Epoch 1340/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8729\n",
            "Epoch 1341/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8717\n",
            "Epoch 1342/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2816 - accuracy: 0.8680\n",
            "Epoch 1343/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8747\n",
            "Epoch 1344/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8698\n",
            "Epoch 1345/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.8802\n",
            "Epoch 1346/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2839 - accuracy: 0.8644\n",
            "Epoch 1347/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8650\n",
            "Epoch 1348/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2771 - accuracy: 0.8723\n",
            "Epoch 1349/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2799 - accuracy: 0.8759\n",
            "Epoch 1350/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2790 - accuracy: 0.8735\n",
            "Epoch 1351/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2794 - accuracy: 0.8704\n",
            "Epoch 1352/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2824 - accuracy: 0.8692\n",
            "Epoch 1353/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2781 - accuracy: 0.8710\n",
            "Epoch 1354/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2807 - accuracy: 0.8729\n",
            "Epoch 1355/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2786 - accuracy: 0.8637\n",
            "Epoch 1356/2000\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2732 - accuracy: 0.8820\n",
            "Epoch 1357/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2784 - accuracy: 0.8771\n",
            "Epoch 1358/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8723\n",
            "Epoch 1359/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8717\n",
            "Epoch 1360/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8723\n",
            "Epoch 1361/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2784 - accuracy: 0.8710\n",
            "Epoch 1362/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8704\n",
            "Epoch 1363/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2929 - accuracy: 0.8589\n",
            "Epoch 1364/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8668\n",
            "Epoch 1365/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.8759\n",
            "Epoch 1366/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8741\n",
            "Epoch 1367/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8674\n",
            "Epoch 1368/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8674\n",
            "Epoch 1369/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2794 - accuracy: 0.8741\n",
            "Epoch 1370/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8735\n",
            "Epoch 1371/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.8765\n",
            "Epoch 1372/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8765\n",
            "Epoch 1373/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2813 - accuracy: 0.8692\n",
            "Epoch 1374/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8729\n",
            "Epoch 1375/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2822 - accuracy: 0.8735\n",
            "Epoch 1376/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8631\n",
            "Epoch 1377/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2810 - accuracy: 0.8704\n",
            "Epoch 1378/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.8790\n",
            "Epoch 1379/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.8717\n",
            "Epoch 1380/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.8710\n",
            "Epoch 1381/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.8650\n",
            "Epoch 1382/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8698\n",
            "Epoch 1383/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2746 - accuracy: 0.8759\n",
            "Epoch 1384/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2753 - accuracy: 0.8650\n",
            "Epoch 1385/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.8735\n",
            "Epoch 1386/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8668\n",
            "Epoch 1387/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.8814\n",
            "Epoch 1388/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2777 - accuracy: 0.8692\n",
            "Epoch 1389/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8765\n",
            "Epoch 1390/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2732 - accuracy: 0.8717\n",
            "Epoch 1391/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.8753\n",
            "Epoch 1392/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2752 - accuracy: 0.8802\n",
            "Epoch 1393/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2728 - accuracy: 0.8759\n",
            "Epoch 1394/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8729\n",
            "Epoch 1395/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8808\n",
            "Epoch 1396/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2873 - accuracy: 0.8656\n",
            "Epoch 1397/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2770 - accuracy: 0.8765\n",
            "Epoch 1398/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8790\n",
            "Epoch 1399/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8729\n",
            "Epoch 1400/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2764 - accuracy: 0.8686\n",
            "Epoch 1401/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2760 - accuracy: 0.8783\n",
            "Epoch 1402/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8753\n",
            "Epoch 1403/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2728 - accuracy: 0.8765\n",
            "Epoch 1404/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8765\n",
            "Epoch 1405/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8637\n",
            "Epoch 1406/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2892 - accuracy: 0.8571\n",
            "Epoch 1407/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8680\n",
            "Epoch 1408/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2716 - accuracy: 0.8826\n",
            "Epoch 1409/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8753\n",
            "Epoch 1410/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8710\n",
            "Epoch 1411/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8765\n",
            "Epoch 1412/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2750 - accuracy: 0.8735\n",
            "Epoch 1413/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2775 - accuracy: 0.8668\n",
            "Epoch 1414/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8650\n",
            "Epoch 1415/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8710\n",
            "Epoch 1416/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8674\n",
            "Epoch 1417/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8650\n",
            "Epoch 1418/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8747\n",
            "Epoch 1419/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8771\n",
            "Epoch 1420/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2794 - accuracy: 0.8674\n",
            "Epoch 1421/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8765\n",
            "Epoch 1422/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8644\n",
            "Epoch 1423/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8668\n",
            "Epoch 1424/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2721 - accuracy: 0.8747\n",
            "Epoch 1425/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2716 - accuracy: 0.8808\n",
            "Epoch 1426/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2697 - accuracy: 0.8826\n",
            "Epoch 1427/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8753\n",
            "Epoch 1428/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8692\n",
            "Epoch 1429/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8796\n",
            "Epoch 1430/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.8680\n",
            "Epoch 1431/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.8644\n",
            "Epoch 1432/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.8765\n",
            "Epoch 1433/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2715 - accuracy: 0.8783\n",
            "Epoch 1434/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8741\n",
            "Epoch 1435/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8729\n",
            "Epoch 1436/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2737 - accuracy: 0.8771\n",
            "Epoch 1437/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.8650\n",
            "Epoch 1438/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.8759\n",
            "Epoch 1439/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8832\n",
            "Epoch 1440/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.8710\n",
            "Epoch 1441/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.8771\n",
            "Epoch 1442/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8814\n",
            "Epoch 1443/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.8680\n",
            "Epoch 1444/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.8650\n",
            "Epoch 1445/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.8735\n",
            "Epoch 1446/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2720 - accuracy: 0.8820\n",
            "Epoch 1447/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.8656\n",
            "Epoch 1448/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8771\n",
            "Epoch 1449/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.8771\n",
            "Epoch 1450/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8814\n",
            "Epoch 1451/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2690 - accuracy: 0.8729\n",
            "Epoch 1452/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.8735\n",
            "Epoch 1453/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.8741\n",
            "Epoch 1454/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.8783\n",
            "Epoch 1455/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8710\n",
            "Epoch 1456/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2689 - accuracy: 0.8777\n",
            "Epoch 1457/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2764 - accuracy: 0.8796\n",
            "Epoch 1458/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2688 - accuracy: 0.8790\n",
            "Epoch 1459/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2757 - accuracy: 0.8741\n",
            "Epoch 1460/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8741\n",
            "Epoch 1461/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.8777\n",
            "Epoch 1462/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.8802\n",
            "Epoch 1463/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.8747\n",
            "Epoch 1464/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8765\n",
            "Epoch 1465/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2716 - accuracy: 0.8723\n",
            "Epoch 1466/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.8735\n",
            "Epoch 1467/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.8729\n",
            "Epoch 1468/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2690 - accuracy: 0.8783\n",
            "Epoch 1469/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.8631\n",
            "Epoch 1470/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.8765\n",
            "Epoch 1471/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2750 - accuracy: 0.8753\n",
            "Epoch 1472/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8771\n",
            "Epoch 1473/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.8771\n",
            "Epoch 1474/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.8783\n",
            "Epoch 1475/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.8796\n",
            "Epoch 1476/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.8759\n",
            "Epoch 1477/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.8723\n",
            "Epoch 1478/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.8753\n",
            "Epoch 1479/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.8765\n",
            "Epoch 1480/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.8765\n",
            "Epoch 1481/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8717\n",
            "Epoch 1482/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.8790\n",
            "Epoch 1483/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2757 - accuracy: 0.8735\n",
            "Epoch 1484/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.8698\n",
            "Epoch 1485/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8765\n",
            "Epoch 1486/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2687 - accuracy: 0.8808\n",
            "Epoch 1487/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8680\n",
            "Epoch 1488/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.8820\n",
            "Epoch 1489/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8802\n",
            "Epoch 1490/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.8838\n",
            "Epoch 1491/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8674\n",
            "Epoch 1492/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2864 - accuracy: 0.8674\n",
            "Epoch 1493/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.8796\n",
            "Epoch 1494/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.8802\n",
            "Epoch 1495/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8747\n",
            "Epoch 1496/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8863\n",
            "Epoch 1497/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.8808\n",
            "Epoch 1498/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2714 - accuracy: 0.8637\n",
            "Epoch 1499/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8741\n",
            "Epoch 1500/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.8783\n",
            "Epoch 1501/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.8808\n",
            "Epoch 1502/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8723\n",
            "Epoch 1503/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2619 - accuracy: 0.8777\n",
            "Epoch 1504/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.8814\n",
            "Epoch 1505/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8698\n",
            "Epoch 1506/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2694 - accuracy: 0.8814\n",
            "Epoch 1507/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2642 - accuracy: 0.8863\n",
            "Epoch 1508/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2681 - accuracy: 0.8783\n",
            "Epoch 1509/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8814\n",
            "Epoch 1510/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.8704\n",
            "Epoch 1511/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.8704\n",
            "Epoch 1512/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.8765\n",
            "Epoch 1513/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8747\n",
            "Epoch 1514/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.8771\n",
            "Epoch 1515/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8802\n",
            "Epoch 1516/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.8790\n",
            "Epoch 1517/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2697 - accuracy: 0.8692\n",
            "Epoch 1518/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8783\n",
            "Epoch 1519/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2687 - accuracy: 0.8759\n",
            "Epoch 1520/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.8777\n",
            "Epoch 1521/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.8741\n",
            "Epoch 1522/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.8753\n",
            "Epoch 1523/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2626 - accuracy: 0.8863\n",
            "Epoch 1524/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2743 - accuracy: 0.8753\n",
            "Epoch 1525/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.8820\n",
            "Epoch 1526/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2624 - accuracy: 0.8802\n",
            "Epoch 1527/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8741\n",
            "Epoch 1528/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2712 - accuracy: 0.8692\n",
            "Epoch 1529/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2693 - accuracy: 0.8783\n",
            "Epoch 1530/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2673 - accuracy: 0.8759\n",
            "Epoch 1531/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.8735\n",
            "Epoch 1532/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.8747\n",
            "Epoch 1533/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.8735\n",
            "Epoch 1534/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.8802\n",
            "Epoch 1535/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.8692\n",
            "Epoch 1536/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2624 - accuracy: 0.8790\n",
            "Epoch 1537/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.8765\n",
            "Epoch 1538/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2728 - accuracy: 0.8753\n",
            "Epoch 1539/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2629 - accuracy: 0.8796\n",
            "Epoch 1540/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.8735\n",
            "Epoch 1541/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2696 - accuracy: 0.8796\n",
            "Epoch 1542/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.8759\n",
            "Epoch 1543/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.8838\n",
            "Epoch 1544/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.8777\n",
            "Epoch 1545/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8747\n",
            "Epoch 1546/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8838\n",
            "Epoch 1547/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.8796\n",
            "Epoch 1548/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.8735\n",
            "Epoch 1549/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2667 - accuracy: 0.8765\n",
            "Epoch 1550/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8698\n",
            "Epoch 1551/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.8765\n",
            "Epoch 1552/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2671 - accuracy: 0.8765\n",
            "Epoch 1553/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8783\n",
            "Epoch 1554/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8759\n",
            "Epoch 1555/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.8802\n",
            "Epoch 1556/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8838\n",
            "Epoch 1557/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8741\n",
            "Epoch 1558/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8790\n",
            "Epoch 1559/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2694 - accuracy: 0.8723\n",
            "Epoch 1560/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2649 - accuracy: 0.8741\n",
            "Epoch 1561/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2640 - accuracy: 0.8741\n",
            "Epoch 1562/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8698\n",
            "Epoch 1563/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.8765\n",
            "Epoch 1564/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8802\n",
            "Epoch 1565/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.8832\n",
            "Epoch 1566/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.8710\n",
            "Epoch 1567/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.8710\n",
            "Epoch 1568/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8856\n",
            "Epoch 1569/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8832\n",
            "Epoch 1570/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2737 - accuracy: 0.8686\n",
            "Epoch 1571/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8631\n",
            "Epoch 1572/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2714 - accuracy: 0.8771\n",
            "Epoch 1573/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.8875\n",
            "Epoch 1574/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2719 - accuracy: 0.8717\n",
            "Epoch 1575/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2635 - accuracy: 0.8814\n",
            "Epoch 1576/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.8790\n",
            "Epoch 1577/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8826\n",
            "Epoch 1578/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8802\n",
            "Epoch 1579/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2666 - accuracy: 0.8771\n",
            "Epoch 1580/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.8783\n",
            "Epoch 1581/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.8808\n",
            "Epoch 1582/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8735\n",
            "Epoch 1583/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.8802\n",
            "Epoch 1584/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.8753\n",
            "Epoch 1585/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8783\n",
            "Epoch 1586/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.8777\n",
            "Epoch 1587/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802\n",
            "Epoch 1588/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8863\n",
            "Epoch 1589/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8850\n",
            "Epoch 1590/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8717\n",
            "Epoch 1591/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.8783\n",
            "Epoch 1592/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8790\n",
            "Epoch 1593/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2639 - accuracy: 0.8869\n",
            "Epoch 1594/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.8741\n",
            "Epoch 1595/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8765\n",
            "Epoch 1596/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8820\n",
            "Epoch 1597/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.8741\n",
            "Epoch 1598/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2719 - accuracy: 0.8735\n",
            "Epoch 1599/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8808\n",
            "Epoch 1600/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.8820\n",
            "Epoch 1601/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2613 - accuracy: 0.8826\n",
            "Epoch 1602/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2678 - accuracy: 0.8802\n",
            "Epoch 1603/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8814\n",
            "Epoch 1604/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8783\n",
            "Epoch 1605/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.8850\n",
            "Epoch 1606/2000\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.2632 - accuracy: 0.8814\n",
            "Epoch 1607/2000\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.2625 - accuracy: 0.8863\n",
            "Epoch 1608/2000\n",
            "52/52 [==============================] - 1s 12ms/step - loss: 0.2725 - accuracy: 0.8729\n",
            "Epoch 1609/2000\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.2722 - accuracy: 0.8753\n",
            "Epoch 1610/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2607 - accuracy: 0.8783\n",
            "Epoch 1611/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8735\n",
            "Epoch 1612/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8747\n",
            "Epoch 1613/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.8777\n",
            "Epoch 1614/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8838\n",
            "Epoch 1615/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.8838\n",
            "Epoch 1616/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8850\n",
            "Epoch 1617/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8856\n",
            "Epoch 1618/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8844\n",
            "Epoch 1619/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2593 - accuracy: 0.8771\n",
            "Epoch 1620/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.8735\n",
            "Epoch 1621/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.8796\n",
            "Epoch 1622/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8765\n",
            "Epoch 1623/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.8826\n",
            "Epoch 1624/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8747\n",
            "Epoch 1625/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.8698\n",
            "Epoch 1626/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.8875\n",
            "Epoch 1627/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.8777\n",
            "Epoch 1628/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2650 - accuracy: 0.8771\n",
            "Epoch 1629/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.8790\n",
            "Epoch 1630/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2649 - accuracy: 0.8771\n",
            "Epoch 1631/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.8844\n",
            "Epoch 1632/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.8771\n",
            "Epoch 1633/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8905\n",
            "Epoch 1634/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.8771\n",
            "Epoch 1635/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2629 - accuracy: 0.8802\n",
            "Epoch 1636/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.8759\n",
            "Epoch 1637/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.8783\n",
            "Epoch 1638/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.8820\n",
            "Epoch 1639/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.8783\n",
            "Epoch 1640/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.8881\n",
            "Epoch 1641/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.8832\n",
            "Epoch 1642/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.8808\n",
            "Epoch 1643/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.8826\n",
            "Epoch 1644/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2629 - accuracy: 0.8832\n",
            "Epoch 1645/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.8856\n",
            "Epoch 1646/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.8790\n",
            "Epoch 1647/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8808\n",
            "Epoch 1648/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8790\n",
            "Epoch 1649/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8808\n",
            "Epoch 1650/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8875\n",
            "Epoch 1651/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8838\n",
            "Epoch 1652/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.8875\n",
            "Epoch 1653/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.8856\n",
            "Epoch 1654/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2613 - accuracy: 0.8729\n",
            "Epoch 1655/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2675 - accuracy: 0.8765\n",
            "Epoch 1656/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8850\n",
            "Epoch 1657/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.8771\n",
            "Epoch 1658/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2689 - accuracy: 0.8735\n",
            "Epoch 1659/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.8710\n",
            "Epoch 1660/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.8717\n",
            "Epoch 1661/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.8844\n",
            "Epoch 1662/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2559 - accuracy: 0.8844\n",
            "Epoch 1663/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8783\n",
            "Epoch 1664/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8777\n",
            "Epoch 1665/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2629 - accuracy: 0.8741\n",
            "Epoch 1666/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2647 - accuracy: 0.8820\n",
            "Epoch 1667/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2629 - accuracy: 0.8826\n",
            "Epoch 1668/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2595 - accuracy: 0.8844\n",
            "Epoch 1669/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.8723\n",
            "Epoch 1670/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8747\n",
            "Epoch 1671/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2629 - accuracy: 0.8777\n",
            "Epoch 1672/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8783\n",
            "Epoch 1673/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.8832\n",
            "Epoch 1674/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8741\n",
            "Epoch 1675/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.8844\n",
            "Epoch 1676/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.8838\n",
            "Epoch 1677/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8717\n",
            "Epoch 1678/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.8820\n",
            "Epoch 1679/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2605 - accuracy: 0.8808\n",
            "Epoch 1680/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.8777\n",
            "Epoch 1681/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8881\n",
            "Epoch 1682/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.8808\n",
            "Epoch 1683/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.8747\n",
            "Epoch 1684/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8808\n",
            "Epoch 1685/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2592 - accuracy: 0.8802\n",
            "Epoch 1686/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.8771\n",
            "Epoch 1687/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2583 - accuracy: 0.8771\n",
            "Epoch 1688/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2595 - accuracy: 0.8814\n",
            "Epoch 1689/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8838\n",
            "Epoch 1690/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.8838\n",
            "Epoch 1691/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8808\n",
            "Epoch 1692/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2559 - accuracy: 0.8838\n",
            "Epoch 1693/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.8832\n",
            "Epoch 1694/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8753\n",
            "Epoch 1695/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.8783\n",
            "Epoch 1696/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8917\n",
            "Epoch 1697/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2775 - accuracy: 0.8710\n",
            "Epoch 1698/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.8875\n",
            "Epoch 1699/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8832\n",
            "Epoch 1700/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8826\n",
            "Epoch 1701/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8771\n",
            "Epoch 1702/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8850\n",
            "Epoch 1703/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.8850\n",
            "Epoch 1704/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8856\n",
            "Epoch 1705/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.8796\n",
            "Epoch 1706/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8765\n",
            "Epoch 1707/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8850\n",
            "Epoch 1708/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2561 - accuracy: 0.8820\n",
            "Epoch 1709/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2534 - accuracy: 0.8838\n",
            "Epoch 1710/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2584 - accuracy: 0.8844\n",
            "Epoch 1711/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8850\n",
            "Epoch 1712/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.8911\n",
            "Epoch 1713/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.8838\n",
            "Epoch 1714/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.8869\n",
            "Epoch 1715/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.8777\n",
            "Epoch 1716/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2497 - accuracy: 0.8929\n",
            "Epoch 1717/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.8863\n",
            "Epoch 1718/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.8850\n",
            "Epoch 1719/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8765\n",
            "Epoch 1720/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.8814\n",
            "Epoch 1721/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2493 - accuracy: 0.8893\n",
            "Epoch 1722/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.8869\n",
            "Epoch 1723/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.8863\n",
            "Epoch 1724/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.8796\n",
            "Epoch 1725/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8765\n",
            "Epoch 1726/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.8820\n",
            "Epoch 1727/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.8875\n",
            "Epoch 1728/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2600 - accuracy: 0.8796\n",
            "Epoch 1729/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8783\n",
            "Epoch 1730/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8765\n",
            "Epoch 1731/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.8863\n",
            "Epoch 1732/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8875\n",
            "Epoch 1733/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.8832\n",
            "Epoch 1734/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2511 - accuracy: 0.8850\n",
            "Epoch 1735/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.8869\n",
            "Epoch 1736/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8771\n",
            "Epoch 1737/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8796\n",
            "Epoch 1738/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8850\n",
            "Epoch 1739/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2527 - accuracy: 0.8820\n",
            "Epoch 1740/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8887\n",
            "Epoch 1741/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.8771\n",
            "Epoch 1742/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.8802\n",
            "Epoch 1743/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8662\n",
            "Epoch 1744/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2646 - accuracy: 0.8820\n",
            "Epoch 1745/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8820\n",
            "Epoch 1746/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.8832\n",
            "Epoch 1747/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.8856\n",
            "Epoch 1748/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8820\n",
            "Epoch 1749/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2595 - accuracy: 0.8850\n",
            "Epoch 1750/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2527 - accuracy: 0.8856\n",
            "Epoch 1751/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.8863\n",
            "Epoch 1752/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2522 - accuracy: 0.8790\n",
            "Epoch 1753/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.8856\n",
            "Epoch 1754/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8850\n",
            "Epoch 1755/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2533 - accuracy: 0.8814\n",
            "Epoch 1756/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.8826\n",
            "Epoch 1757/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2592 - accuracy: 0.8771\n",
            "Epoch 1758/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.8844\n",
            "Epoch 1759/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.8814\n",
            "Epoch 1760/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8832\n",
            "Epoch 1761/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.8796\n",
            "Epoch 1762/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.8844\n",
            "Epoch 1763/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.8820\n",
            "Epoch 1764/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2507 - accuracy: 0.8875\n",
            "Epoch 1765/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.8796\n",
            "Epoch 1766/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8881\n",
            "Epoch 1767/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8881\n",
            "Epoch 1768/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.8850\n",
            "Epoch 1769/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8856\n",
            "Epoch 1770/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8790\n",
            "Epoch 1771/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8875\n",
            "Epoch 1772/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8844\n",
            "Epoch 1773/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2534 - accuracy: 0.8875\n",
            "Epoch 1774/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2516 - accuracy: 0.8881\n",
            "Epoch 1775/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2525 - accuracy: 0.8802\n",
            "Epoch 1776/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.8814\n",
            "Epoch 1777/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8753\n",
            "Epoch 1778/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2588 - accuracy: 0.8832\n",
            "Epoch 1779/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.8838\n",
            "Epoch 1780/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.8759\n",
            "Epoch 1781/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2593 - accuracy: 0.8844\n",
            "Epoch 1782/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2551 - accuracy: 0.8710\n",
            "Epoch 1783/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8814\n",
            "Epoch 1784/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2499 - accuracy: 0.8856\n",
            "Epoch 1785/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.8875\n",
            "Epoch 1786/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.8814\n",
            "Epoch 1787/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.8893\n",
            "Epoch 1788/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.8850\n",
            "Epoch 1789/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.8838\n",
            "Epoch 1790/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.8881\n",
            "Epoch 1791/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2583 - accuracy: 0.8759\n",
            "Epoch 1792/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8832\n",
            "Epoch 1793/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.8863\n",
            "Epoch 1794/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.8869\n",
            "Epoch 1795/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2584 - accuracy: 0.8741\n",
            "Epoch 1796/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.8838\n",
            "Epoch 1797/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2590 - accuracy: 0.8753\n",
            "Epoch 1798/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.8814\n",
            "Epoch 1799/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.8881\n",
            "Epoch 1800/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.8826\n",
            "Epoch 1801/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2533 - accuracy: 0.8850\n",
            "Epoch 1802/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.8838\n",
            "Epoch 1803/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2493 - accuracy: 0.8850\n",
            "Epoch 1804/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.8814\n",
            "Epoch 1805/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.8923\n",
            "Epoch 1806/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.8936\n",
            "Epoch 1807/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.8820\n",
            "Epoch 1808/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.8856\n",
            "Epoch 1809/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8856\n",
            "Epoch 1810/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2495 - accuracy: 0.8844\n",
            "Epoch 1811/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.8814\n",
            "Epoch 1812/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.8887\n",
            "Epoch 1813/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8826\n",
            "Epoch 1814/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8887\n",
            "Epoch 1815/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2476 - accuracy: 0.8881\n",
            "Epoch 1816/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.8863\n",
            "Epoch 1817/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.8844\n",
            "Epoch 1818/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.8783\n",
            "Epoch 1819/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2471 - accuracy: 0.8850\n",
            "Epoch 1820/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.8838\n",
            "Epoch 1821/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.8838\n",
            "Epoch 1822/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.8960\n",
            "Epoch 1823/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.8783\n",
            "Epoch 1824/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2492 - accuracy: 0.8869\n",
            "Epoch 1825/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8844\n",
            "Epoch 1826/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.8832\n",
            "Epoch 1827/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.8783\n",
            "Epoch 1828/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.8923\n",
            "Epoch 1829/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8850\n",
            "Epoch 1830/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8838\n",
            "Epoch 1831/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2520 - accuracy: 0.8790\n",
            "Epoch 1832/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8814\n",
            "Epoch 1833/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.8826\n",
            "Epoch 1834/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2575 - accuracy: 0.8796\n",
            "Epoch 1835/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.8850\n",
            "Epoch 1836/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.8838\n",
            "Epoch 1837/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2494 - accuracy: 0.8832\n",
            "Epoch 1838/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.8881\n",
            "Epoch 1839/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2501 - accuracy: 0.8887\n",
            "Epoch 1840/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.8881\n",
            "Epoch 1841/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2561 - accuracy: 0.8887\n",
            "Epoch 1842/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8802\n",
            "Epoch 1843/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.8832\n",
            "Epoch 1844/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.8869\n",
            "Epoch 1845/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.8856\n",
            "Epoch 1846/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8887\n",
            "Epoch 1847/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.8929\n",
            "Epoch 1848/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.8850\n",
            "Epoch 1849/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2586 - accuracy: 0.8850\n",
            "Epoch 1850/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2581 - accuracy: 0.8802\n",
            "Epoch 1851/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8881\n",
            "Epoch 1852/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2411 - accuracy: 0.8942\n",
            "Epoch 1853/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2577 - accuracy: 0.8783\n",
            "Epoch 1854/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.8911\n",
            "Epoch 1855/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8844\n",
            "Epoch 1856/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2525 - accuracy: 0.8869\n",
            "Epoch 1857/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8850\n",
            "Epoch 1858/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.8893\n",
            "Epoch 1859/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.8869\n",
            "Epoch 1860/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.8905\n",
            "Epoch 1861/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2459 - accuracy: 0.8887\n",
            "Epoch 1862/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.8911\n",
            "Epoch 1863/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8856\n",
            "Epoch 1864/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8808\n",
            "Epoch 1865/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2494 - accuracy: 0.8875\n",
            "Epoch 1866/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.8808\n",
            "Epoch 1867/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2579 - accuracy: 0.8777\n",
            "Epoch 1868/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8826\n",
            "Epoch 1869/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.8856\n",
            "Epoch 1870/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.8875\n",
            "Epoch 1871/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8881\n",
            "Epoch 1872/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.8881\n",
            "Epoch 1873/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.8911\n",
            "Epoch 1874/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8820\n",
            "Epoch 1875/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8826\n",
            "Epoch 1876/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2497 - accuracy: 0.8838\n",
            "Epoch 1877/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2409 - accuracy: 0.8911\n",
            "Epoch 1878/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.8893\n",
            "Epoch 1879/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8771\n",
            "Epoch 1880/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.8881\n",
            "Epoch 1881/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.8704\n",
            "Epoch 1882/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2511 - accuracy: 0.8875\n",
            "Epoch 1883/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.8777\n",
            "Epoch 1884/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8893\n",
            "Epoch 1885/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.8856\n",
            "Epoch 1886/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.8899\n",
            "Epoch 1887/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2535 - accuracy: 0.8856\n",
            "Epoch 1888/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8704\n",
            "Epoch 1889/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.8802\n",
            "Epoch 1890/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2499 - accuracy: 0.8838\n",
            "Epoch 1891/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8911\n",
            "Epoch 1892/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.8826\n",
            "Epoch 1893/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.8899\n",
            "Epoch 1894/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8960\n",
            "Epoch 1895/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2462 - accuracy: 0.8832\n",
            "Epoch 1896/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2402 - accuracy: 0.8936\n",
            "Epoch 1897/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8875\n",
            "Epoch 1898/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.8948\n",
            "Epoch 1899/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8972\n",
            "Epoch 1900/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2527 - accuracy: 0.8832\n",
            "Epoch 1901/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8887\n",
            "Epoch 1902/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.8917\n",
            "Epoch 1903/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.8893\n",
            "Epoch 1904/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8814\n",
            "Epoch 1905/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.8820\n",
            "Epoch 1906/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8850\n",
            "Epoch 1907/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.8911\n",
            "Epoch 1908/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.8942\n",
            "Epoch 1909/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8820\n",
            "Epoch 1910/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.8844\n",
            "Epoch 1911/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.8881\n",
            "Epoch 1912/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2443 - accuracy: 0.8917\n",
            "Epoch 1913/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8820\n",
            "Epoch 1914/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.8899\n",
            "Epoch 1915/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.8875\n",
            "Epoch 1916/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2409 - accuracy: 0.8905\n",
            "Epoch 1917/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2401 - accuracy: 0.8936\n",
            "Epoch 1918/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8960\n",
            "Epoch 1919/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2552 - accuracy: 0.8814\n",
            "Epoch 1920/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2402 - accuracy: 0.8869\n",
            "Epoch 1921/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.8899\n",
            "Epoch 1922/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.8838\n",
            "Epoch 1923/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2553 - accuracy: 0.8723\n",
            "Epoch 1924/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2440 - accuracy: 0.8863\n",
            "Epoch 1925/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.8917\n",
            "Epoch 1926/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2393 - accuracy: 0.8929\n",
            "Epoch 1927/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.8978\n",
            "Epoch 1928/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.8905\n",
            "Epoch 1929/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2534 - accuracy: 0.8808\n",
            "Epoch 1930/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8869\n",
            "Epoch 1931/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8942\n",
            "Epoch 1932/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.8936\n",
            "Epoch 1933/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.8881\n",
            "Epoch 1934/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.8863\n",
            "Epoch 1935/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2494 - accuracy: 0.8887\n",
            "Epoch 1936/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8905\n",
            "Epoch 1937/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.8881\n",
            "Epoch 1938/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2426 - accuracy: 0.8850\n",
            "Epoch 1939/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.8893\n",
            "Epoch 1940/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.8911\n",
            "Epoch 1941/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.8869\n",
            "Epoch 1942/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.8820\n",
            "Epoch 1943/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.8856\n",
            "Epoch 1944/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.8936\n",
            "Epoch 1945/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8820\n",
            "Epoch 1946/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.8826\n",
            "Epoch 1947/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2462 - accuracy: 0.8856\n",
            "Epoch 1948/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.8863\n",
            "Epoch 1949/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.8826\n",
            "Epoch 1950/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8844\n",
            "Epoch 1951/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.8923\n",
            "Epoch 1952/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.8856\n",
            "Epoch 1953/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8899\n",
            "Epoch 1954/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2384 - accuracy: 0.8954\n",
            "Epoch 1955/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2407 - accuracy: 0.8887\n",
            "Epoch 1956/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2437 - accuracy: 0.8929\n",
            "Epoch 1957/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.8863\n",
            "Epoch 1958/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.8923\n",
            "Epoch 1959/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8893\n",
            "Epoch 1960/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2493 - accuracy: 0.8881\n",
            "Epoch 1961/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.8838\n",
            "Epoch 1962/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8936\n",
            "Epoch 1963/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8948\n",
            "Epoch 1964/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2349 - accuracy: 0.8936\n",
            "Epoch 1965/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2394 - accuracy: 0.8887\n",
            "Epoch 1966/2000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2422 - accuracy: 0.8838\n",
            "Epoch 1967/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8881\n",
            "Epoch 1968/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8875\n",
            "Epoch 1969/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.8905\n",
            "Epoch 1970/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8893\n",
            "Epoch 1971/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8893\n",
            "Epoch 1972/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.8832\n",
            "Epoch 1973/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.8881\n",
            "Epoch 1974/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.8917\n",
            "Epoch 1975/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.8875\n",
            "Epoch 1976/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.8856\n",
            "Epoch 1977/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.8905\n",
            "Epoch 1978/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8875\n",
            "Epoch 1979/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.8905\n",
            "Epoch 1980/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.8911\n",
            "Epoch 1981/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.8936\n",
            "Epoch 1982/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.8796\n",
            "Epoch 1983/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.8923\n",
            "Epoch 1984/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.8796\n",
            "Epoch 1985/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.8942\n",
            "Epoch 1986/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8905\n",
            "Epoch 1987/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.8905\n",
            "Epoch 1988/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2430 - accuracy: 0.8832\n",
            "Epoch 1989/2000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.8820\n",
            "Epoch 1990/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.8905\n",
            "Epoch 1991/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 1992/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2401 - accuracy: 0.8875\n",
            "Epoch 1993/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2403 - accuracy: 0.8954\n",
            "Epoch 1994/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2384 - accuracy: 0.8917\n",
            "Epoch 1995/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.8777\n",
            "Epoch 1996/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.8820\n",
            "Epoch 1997/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2403 - accuracy: 0.8838\n",
            "Epoch 1998/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.8887\n",
            "Epoch 1999/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.8820\n",
            "Epoch 2000/2000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.8875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb100200af0>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQCIx4mVRrYC",
        "outputId": "7142799a-3e6c-408d-b456-5e1566761fe5"
      },
      "source": [
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: %.4f' % (accuracy*100))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step - loss: 0.5725 - accuracy: 0.7621\n",
            "Accuracy: 76.2136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtxDYa_xRaC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa747df6-e9fd-4784-8f2f-ceab8db32536"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "y_pred = (predictions > 0.5)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWV32AgTQ-yZ",
        "outputId": "9a69d7cd-fa43-4658-ab34-425fe3dbc892"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[162  51]\n",
            " [ 47 152]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7751    0.7606    0.7678       213\n",
            "           1     0.7488    0.7638    0.7562       199\n",
            "\n",
            "    accuracy                         0.7621       412\n",
            "   macro avg     0.7619    0.7622    0.7620       412\n",
            "weighted avg     0.7624    0.7621    0.7622       412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWdKO76zSIk-"
      },
      "source": [
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd-ZD_eLXXRz",
        "outputId": "ff0969ec-d4c4-4fdd-ab06-1c9bfe17b935"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "def base_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "model = KerasClassifier(build_fn=base_model)\n",
        "model.fit(X_train, y_train, epochs=4000)\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-64ec857c713c>:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=base_model)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2875 - accuracy: 0.8692\n",
            "Epoch 1502/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8516\n",
            "Epoch 1503/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8552\n",
            "Epoch 1504/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.8625\n",
            "Epoch 1505/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2925 - accuracy: 0.8595\n",
            "Epoch 1506/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2909 - accuracy: 0.8662\n",
            "Epoch 1507/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8717\n",
            "Epoch 1508/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2928 - accuracy: 0.8619\n",
            "Epoch 1509/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.8650\n",
            "Epoch 1510/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8650\n",
            "Epoch 1511/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2962 - accuracy: 0.8619\n",
            "Epoch 1512/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2918 - accuracy: 0.8717\n",
            "Epoch 1513/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8686\n",
            "Epoch 1514/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8656\n",
            "Epoch 1515/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2905 - accuracy: 0.8619\n",
            "Epoch 1516/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.8692\n",
            "Epoch 1517/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8631\n",
            "Epoch 1518/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2987 - accuracy: 0.8650\n",
            "Epoch 1519/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3045 - accuracy: 0.8607\n",
            "Epoch 1520/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2931 - accuracy: 0.8662\n",
            "Epoch 1521/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2890 - accuracy: 0.8680\n",
            "Epoch 1522/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8637\n",
            "Epoch 1523/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8674\n",
            "Epoch 1524/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8644\n",
            "Epoch 1525/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2918 - accuracy: 0.8540\n",
            "Epoch 1526/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2964 - accuracy: 0.8637\n",
            "Epoch 1527/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.8662\n",
            "Epoch 1528/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8631\n",
            "Epoch 1529/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8631\n",
            "Epoch 1530/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2937 - accuracy: 0.8625\n",
            "Epoch 1531/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.8656\n",
            "Epoch 1532/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.8552\n",
            "Epoch 1533/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.8680\n",
            "Epoch 1534/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2884 - accuracy: 0.8704\n",
            "Epoch 1535/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8577\n",
            "Epoch 1536/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.8625\n",
            "Epoch 1537/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8662\n",
            "Epoch 1538/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2884 - accuracy: 0.8662\n",
            "Epoch 1539/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2864 - accuracy: 0.8704\n",
            "Epoch 1540/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8625\n",
            "Epoch 1541/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3012 - accuracy: 0.8644\n",
            "Epoch 1542/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.8577\n",
            "Epoch 1543/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2915 - accuracy: 0.8668\n",
            "Epoch 1544/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2925 - accuracy: 0.8619\n",
            "Epoch 1545/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8613\n",
            "Epoch 1546/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8644\n",
            "Epoch 1547/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.8656\n",
            "Epoch 1548/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8680\n",
            "Epoch 1549/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8637\n",
            "Epoch 1550/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8571\n",
            "Epoch 1551/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8704\n",
            "Epoch 1552/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2904 - accuracy: 0.8650\n",
            "Epoch 1553/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.8625\n",
            "Epoch 1554/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.8662\n",
            "Epoch 1555/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2902 - accuracy: 0.8619\n",
            "Epoch 1556/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8631\n",
            "Epoch 1557/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3120 - accuracy: 0.8504\n",
            "Epoch 1558/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8564\n",
            "Epoch 1559/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.8601\n",
            "Epoch 1560/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8595\n",
            "Epoch 1561/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8656\n",
            "Epoch 1562/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2925 - accuracy: 0.8704\n",
            "Epoch 1563/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.8564\n",
            "Epoch 1564/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.3063 - accuracy: 0.8546\n",
            "Epoch 1565/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8637\n",
            "Epoch 1566/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8741\n",
            "Epoch 1567/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2983 - accuracy: 0.8601\n",
            "Epoch 1568/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8564\n",
            "Epoch 1569/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.8516\n",
            "Epoch 1570/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8552\n",
            "Epoch 1571/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8625\n",
            "Epoch 1572/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8717\n",
            "Epoch 1573/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8680\n",
            "Epoch 1574/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2894 - accuracy: 0.8644\n",
            "Epoch 1575/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.8698\n",
            "Epoch 1576/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8637\n",
            "Epoch 1577/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8577\n",
            "Epoch 1578/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2975 - accuracy: 0.8613\n",
            "Epoch 1579/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.8564\n",
            "Epoch 1580/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.8680\n",
            "Epoch 1581/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8625\n",
            "Epoch 1582/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8680\n",
            "Epoch 1583/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2898 - accuracy: 0.8637\n",
            "Epoch 1584/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2940 - accuracy: 0.8662\n",
            "Epoch 1585/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.8729\n",
            "Epoch 1586/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8583\n",
            "Epoch 1587/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8668\n",
            "Epoch 1588/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8644\n",
            "Epoch 1589/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8686\n",
            "Epoch 1590/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8595\n",
            "Epoch 1591/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2959 - accuracy: 0.8631\n",
            "Epoch 1592/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.8571\n",
            "Epoch 1593/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2892 - accuracy: 0.8595\n",
            "Epoch 1594/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2913 - accuracy: 0.8656\n",
            "Epoch 1595/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2894 - accuracy: 0.8601\n",
            "Epoch 1596/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8680\n",
            "Epoch 1597/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2898 - accuracy: 0.8680\n",
            "Epoch 1598/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2870 - accuracy: 0.8717\n",
            "Epoch 1599/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8498\n",
            "Epoch 1600/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8644\n",
            "Epoch 1601/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.8571\n",
            "Epoch 1602/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8710\n",
            "Epoch 1603/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2867 - accuracy: 0.8698\n",
            "Epoch 1604/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8619\n",
            "Epoch 1605/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8717\n",
            "Epoch 1606/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.8710\n",
            "Epoch 1607/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.8662\n",
            "Epoch 1608/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8546\n",
            "Epoch 1609/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.8625\n",
            "Epoch 1610/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2926 - accuracy: 0.8607\n",
            "Epoch 1611/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2925 - accuracy: 0.8589\n",
            "Epoch 1612/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2881 - accuracy: 0.8717\n",
            "Epoch 1613/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2874 - accuracy: 0.8698\n",
            "Epoch 1614/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.8668\n",
            "Epoch 1615/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2952 - accuracy: 0.8613\n",
            "Epoch 1616/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2858 - accuracy: 0.8710\n",
            "Epoch 1617/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2921 - accuracy: 0.8601\n",
            "Epoch 1618/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.8680\n",
            "Epoch 1619/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2894 - accuracy: 0.8644\n",
            "Epoch 1620/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2902 - accuracy: 0.8631\n",
            "Epoch 1621/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8650\n",
            "Epoch 1622/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2903 - accuracy: 0.8650\n",
            "Epoch 1623/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8717\n",
            "Epoch 1624/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.8668\n",
            "Epoch 1625/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8510\n",
            "Epoch 1626/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.8613\n",
            "Epoch 1627/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.8613\n",
            "Epoch 1628/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8710\n",
            "Epoch 1629/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2819 - accuracy: 0.8765\n",
            "Epoch 1630/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2909 - accuracy: 0.8637\n",
            "Epoch 1631/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8668\n",
            "Epoch 1632/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2888 - accuracy: 0.8674\n",
            "Epoch 1633/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8637\n",
            "Epoch 1634/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2928 - accuracy: 0.8595\n",
            "Epoch 1635/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8662\n",
            "Epoch 1636/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8650\n",
            "Epoch 1637/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.8656\n",
            "Epoch 1638/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8674\n",
            "Epoch 1639/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.8723\n",
            "Epoch 1640/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2870 - accuracy: 0.8583\n",
            "Epoch 1641/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8607\n",
            "Epoch 1642/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.8698\n",
            "Epoch 1643/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8674\n",
            "Epoch 1644/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8686\n",
            "Epoch 1645/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2858 - accuracy: 0.8680\n",
            "Epoch 1646/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8698\n",
            "Epoch 1647/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.8686\n",
            "Epoch 1648/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8674\n",
            "Epoch 1649/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2835 - accuracy: 0.8656\n",
            "Epoch 1650/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8601\n",
            "Epoch 1651/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2922 - accuracy: 0.8723\n",
            "Epoch 1652/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.8601\n",
            "Epoch 1653/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.8680\n",
            "Epoch 1654/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2897 - accuracy: 0.8631\n",
            "Epoch 1655/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2894 - accuracy: 0.8583\n",
            "Epoch 1656/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8650\n",
            "Epoch 1657/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2944 - accuracy: 0.8625\n",
            "Epoch 1658/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.8644\n",
            "Epoch 1659/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2905 - accuracy: 0.8625\n",
            "Epoch 1660/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2918 - accuracy: 0.8692\n",
            "Epoch 1661/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8546\n",
            "Epoch 1662/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8680\n",
            "Epoch 1663/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2849 - accuracy: 0.8698\n",
            "Epoch 1664/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3009 - accuracy: 0.8534\n",
            "Epoch 1665/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8674\n",
            "Epoch 1666/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.8668\n",
            "Epoch 1667/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.8698\n",
            "Epoch 1668/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8686\n",
            "Epoch 1669/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8607\n",
            "Epoch 1670/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.8741\n",
            "Epoch 1671/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.8686\n",
            "Epoch 1672/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.8644\n",
            "Epoch 1673/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.8650\n",
            "Epoch 1674/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.8637\n",
            "Epoch 1675/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8637\n",
            "Epoch 1676/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2857 - accuracy: 0.8674\n",
            "Epoch 1677/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8704\n",
            "Epoch 1678/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.8631\n",
            "Epoch 1679/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2862 - accuracy: 0.8692\n",
            "Epoch 1680/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8710\n",
            "Epoch 1681/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.8686\n",
            "Epoch 1682/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8650\n",
            "Epoch 1683/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2844 - accuracy: 0.8650\n",
            "Epoch 1684/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2877 - accuracy: 0.8656\n",
            "Epoch 1685/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8680\n",
            "Epoch 1686/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2857 - accuracy: 0.8637\n",
            "Epoch 1687/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2853 - accuracy: 0.8686\n",
            "Epoch 1688/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.8625\n",
            "Epoch 1689/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2836 - accuracy: 0.8607\n",
            "Epoch 1690/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.8607\n",
            "Epoch 1691/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.8680\n",
            "Epoch 1692/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2764 - accuracy: 0.8686\n",
            "Epoch 1693/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2821 - accuracy: 0.8680\n",
            "Epoch 1694/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8710\n",
            "Epoch 1695/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2933 - accuracy: 0.8601\n",
            "Epoch 1696/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8710\n",
            "Epoch 1697/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2859 - accuracy: 0.8662\n",
            "Epoch 1698/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2829 - accuracy: 0.8729\n",
            "Epoch 1699/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8540\n",
            "Epoch 1700/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2898 - accuracy: 0.8631\n",
            "Epoch 1701/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2817 - accuracy: 0.8668\n",
            "Epoch 1702/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8698\n",
            "Epoch 1703/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8625\n",
            "Epoch 1704/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.8656\n",
            "Epoch 1705/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2795 - accuracy: 0.8698\n",
            "Epoch 1706/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8595\n",
            "Epoch 1707/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2834 - accuracy: 0.8668\n",
            "Epoch 1708/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.8650\n",
            "Epoch 1709/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8631\n",
            "Epoch 1710/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8698\n",
            "Epoch 1711/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2820 - accuracy: 0.8710\n",
            "Epoch 1712/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8607\n",
            "Epoch 1713/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8631\n",
            "Epoch 1714/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8710\n",
            "Epoch 1715/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2795 - accuracy: 0.8717\n",
            "Epoch 1716/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8674\n",
            "Epoch 1717/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8692\n",
            "Epoch 1718/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.8650\n",
            "Epoch 1719/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.8680\n",
            "Epoch 1720/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8637\n",
            "Epoch 1721/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2939 - accuracy: 0.8583\n",
            "Epoch 1722/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8710\n",
            "Epoch 1723/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.8692\n",
            "Epoch 1724/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8692\n",
            "Epoch 1725/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8631\n",
            "Epoch 1726/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2825 - accuracy: 0.8662\n",
            "Epoch 1727/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.8644\n",
            "Epoch 1728/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8656\n",
            "Epoch 1729/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8552\n",
            "Epoch 1730/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8735\n",
            "Epoch 1731/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8613\n",
            "Epoch 1732/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2849 - accuracy: 0.8619\n",
            "Epoch 1733/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2810 - accuracy: 0.8717\n",
            "Epoch 1734/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2774 - accuracy: 0.8698\n",
            "Epoch 1735/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2868 - accuracy: 0.8662\n",
            "Epoch 1736/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.8680\n",
            "Epoch 1737/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.8686\n",
            "Epoch 1738/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2926 - accuracy: 0.8607\n",
            "Epoch 1739/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8650\n",
            "Epoch 1740/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8656\n",
            "Epoch 1741/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.8710\n",
            "Epoch 1742/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8704\n",
            "Epoch 1743/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8619\n",
            "Epoch 1744/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2820 - accuracy: 0.8710\n",
            "Epoch 1745/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8692\n",
            "Epoch 1746/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2821 - accuracy: 0.8753\n",
            "Epoch 1747/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.8625\n",
            "Epoch 1748/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2864 - accuracy: 0.8710\n",
            "Epoch 1749/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2777 - accuracy: 0.8698\n",
            "Epoch 1750/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2895 - accuracy: 0.8650\n",
            "Epoch 1751/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.8698\n",
            "Epoch 1752/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8704\n",
            "Epoch 1753/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8644\n",
            "Epoch 1754/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8723\n",
            "Epoch 1755/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8717\n",
            "Epoch 1756/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.8680\n",
            "Epoch 1757/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.8692\n",
            "Epoch 1758/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8668\n",
            "Epoch 1759/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2835 - accuracy: 0.8619\n",
            "Epoch 1760/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2799 - accuracy: 0.8668\n",
            "Epoch 1761/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8796\n",
            "Epoch 1762/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2844 - accuracy: 0.8650\n",
            "Epoch 1763/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2890 - accuracy: 0.8698\n",
            "Epoch 1764/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2794 - accuracy: 0.8735\n",
            "Epoch 1765/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8637\n",
            "Epoch 1766/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8668\n",
            "Epoch 1767/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8650\n",
            "Epoch 1768/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8637\n",
            "Epoch 1769/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.8741\n",
            "Epoch 1770/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8698\n",
            "Epoch 1771/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8650\n",
            "Epoch 1772/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.8729\n",
            "Epoch 1773/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2819 - accuracy: 0.8704\n",
            "Epoch 1774/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8723\n",
            "Epoch 1775/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.8723\n",
            "Epoch 1776/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.8710\n",
            "Epoch 1777/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8717\n",
            "Epoch 1778/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8601\n",
            "Epoch 1779/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8692\n",
            "Epoch 1780/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3070 - accuracy: 0.8528\n",
            "Epoch 1781/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.3073 - accuracy: 0.8528\n",
            "Epoch 1782/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8698\n",
            "Epoch 1783/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8741\n",
            "Epoch 1784/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8631\n",
            "Epoch 1785/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2856 - accuracy: 0.8698\n",
            "Epoch 1786/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8577\n",
            "Epoch 1787/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8644\n",
            "Epoch 1788/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8704\n",
            "Epoch 1789/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2825 - accuracy: 0.8680\n",
            "Epoch 1790/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8656\n",
            "Epoch 1791/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2841 - accuracy: 0.8668\n",
            "Epoch 1792/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.8686\n",
            "Epoch 1793/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.8674\n",
            "Epoch 1794/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8735\n",
            "Epoch 1795/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2811 - accuracy: 0.8619\n",
            "Epoch 1796/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8717\n",
            "Epoch 1797/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8717\n",
            "Epoch 1798/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.8686\n",
            "Epoch 1799/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8717\n",
            "Epoch 1800/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.8741\n",
            "Epoch 1801/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2756 - accuracy: 0.8723\n",
            "Epoch 1802/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8698\n",
            "Epoch 1803/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2825 - accuracy: 0.8686\n",
            "Epoch 1804/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8753\n",
            "Epoch 1805/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8589\n",
            "Epoch 1806/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2783 - accuracy: 0.8814\n",
            "Epoch 1807/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2777 - accuracy: 0.8729\n",
            "Epoch 1808/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2751 - accuracy: 0.8717\n",
            "Epoch 1809/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.8619\n",
            "Epoch 1810/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8698\n",
            "Epoch 1811/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8759\n",
            "Epoch 1812/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8613\n",
            "Epoch 1813/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2956 - accuracy: 0.8516\n",
            "Epoch 1814/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.8680\n",
            "Epoch 1815/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2771 - accuracy: 0.8710\n",
            "Epoch 1816/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2831 - accuracy: 0.8686\n",
            "Epoch 1817/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2815 - accuracy: 0.8735\n",
            "Epoch 1818/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2834 - accuracy: 0.8601\n",
            "Epoch 1819/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2784 - accuracy: 0.8698\n",
            "Epoch 1820/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.8637\n",
            "Epoch 1821/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8656\n",
            "Epoch 1822/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.8674\n",
            "Epoch 1823/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2815 - accuracy: 0.8662\n",
            "Epoch 1824/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8668\n",
            "Epoch 1825/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8777\n",
            "Epoch 1826/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2796 - accuracy: 0.8704\n",
            "Epoch 1827/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2785 - accuracy: 0.8650\n",
            "Epoch 1828/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2794 - accuracy: 0.8717\n",
            "Epoch 1829/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8589\n",
            "Epoch 1830/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2849 - accuracy: 0.8668\n",
            "Epoch 1831/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2796 - accuracy: 0.8686\n",
            "Epoch 1832/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8644\n",
            "Epoch 1833/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8595\n",
            "Epoch 1834/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.8680\n",
            "Epoch 1835/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8723\n",
            "Epoch 1836/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8698\n",
            "Epoch 1837/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8808\n",
            "Epoch 1838/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8710\n",
            "Epoch 1839/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8644\n",
            "Epoch 1840/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8723\n",
            "Epoch 1841/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8710\n",
            "Epoch 1842/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2771 - accuracy: 0.8753\n",
            "Epoch 1843/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8777\n",
            "Epoch 1844/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2811 - accuracy: 0.8680\n",
            "Epoch 1845/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2783 - accuracy: 0.8656\n",
            "Epoch 1846/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8704\n",
            "Epoch 1847/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.8656\n",
            "Epoch 1848/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.8686\n",
            "Epoch 1849/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8589\n",
            "Epoch 1850/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2779 - accuracy: 0.8747\n",
            "Epoch 1851/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2742 - accuracy: 0.8710\n",
            "Epoch 1852/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2813 - accuracy: 0.8607\n",
            "Epoch 1853/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.8771\n",
            "Epoch 1854/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8686\n",
            "Epoch 1855/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8771\n",
            "Epoch 1856/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2763 - accuracy: 0.8759\n",
            "Epoch 1857/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2750 - accuracy: 0.8637\n",
            "Epoch 1858/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2796 - accuracy: 0.8698\n",
            "Epoch 1859/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2836 - accuracy: 0.8656\n",
            "Epoch 1860/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.8741\n",
            "Epoch 1861/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8619\n",
            "Epoch 1862/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2770 - accuracy: 0.8704\n",
            "Epoch 1863/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8686\n",
            "Epoch 1864/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2747 - accuracy: 0.8735\n",
            "Epoch 1865/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2802 - accuracy: 0.8668\n",
            "Epoch 1866/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8698\n",
            "Epoch 1867/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8710\n",
            "Epoch 1868/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2756 - accuracy: 0.8698\n",
            "Epoch 1869/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8759\n",
            "Epoch 1870/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2743 - accuracy: 0.8698\n",
            "Epoch 1871/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2838 - accuracy: 0.8637\n",
            "Epoch 1872/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2783 - accuracy: 0.8747\n",
            "Epoch 1873/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8668\n",
            "Epoch 1874/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8650\n",
            "Epoch 1875/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8704\n",
            "Epoch 1876/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8729\n",
            "Epoch 1877/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8656\n",
            "Epoch 1878/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8771\n",
            "Epoch 1879/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8771\n",
            "Epoch 1880/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8601\n",
            "Epoch 1881/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8790\n",
            "Epoch 1882/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8704\n",
            "Epoch 1883/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8759\n",
            "Epoch 1884/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.8686\n",
            "Epoch 1885/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8704\n",
            "Epoch 1886/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8656\n",
            "Epoch 1887/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8735\n",
            "Epoch 1888/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8698\n",
            "Epoch 1889/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2750 - accuracy: 0.8692\n",
            "Epoch 1890/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8717\n",
            "Epoch 1891/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8704\n",
            "Epoch 1892/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8674\n",
            "Epoch 1893/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2831 - accuracy: 0.8710\n",
            "Epoch 1894/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.8735\n",
            "Epoch 1895/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.8686\n",
            "Epoch 1896/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8723\n",
            "Epoch 1897/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8656\n",
            "Epoch 1898/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.8777\n",
            "Epoch 1899/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.8735\n",
            "Epoch 1900/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2761 - accuracy: 0.8710\n",
            "Epoch 1901/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2782 - accuracy: 0.8644\n",
            "Epoch 1902/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8759\n",
            "Epoch 1903/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8741\n",
            "Epoch 1904/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8729\n",
            "Epoch 1905/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8777\n",
            "Epoch 1906/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8668\n",
            "Epoch 1907/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.8704\n",
            "Epoch 1908/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2763 - accuracy: 0.8729\n",
            "Epoch 1909/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8680\n",
            "Epoch 1910/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2690 - accuracy: 0.8686\n",
            "Epoch 1911/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2888 - accuracy: 0.8589\n",
            "Epoch 1912/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2742 - accuracy: 0.8741\n",
            "Epoch 1913/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2882 - accuracy: 0.8668\n",
            "Epoch 1914/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2750 - accuracy: 0.8698\n",
            "Epoch 1915/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8631\n",
            "Epoch 1916/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8674\n",
            "Epoch 1917/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2764 - accuracy: 0.8704\n",
            "Epoch 1918/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8723\n",
            "Epoch 1919/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.8619\n",
            "Epoch 1920/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2737 - accuracy: 0.8735\n",
            "Epoch 1921/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8759\n",
            "Epoch 1922/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8710\n",
            "Epoch 1923/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2816 - accuracy: 0.8686\n",
            "Epoch 1924/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2836 - accuracy: 0.8656\n",
            "Epoch 1925/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8741\n",
            "Epoch 1926/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8729\n",
            "Epoch 1927/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8613\n",
            "Epoch 1928/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8771\n",
            "Epoch 1929/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8686\n",
            "Epoch 1930/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2754 - accuracy: 0.8710\n",
            "Epoch 1931/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8723\n",
            "Epoch 1932/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.8698\n",
            "Epoch 1933/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8668\n",
            "Epoch 1934/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.8704\n",
            "Epoch 1935/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8723\n",
            "Epoch 1936/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2726 - accuracy: 0.8759\n",
            "Epoch 1937/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2799 - accuracy: 0.8656\n",
            "Epoch 1938/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8698\n",
            "Epoch 1939/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8698\n",
            "Epoch 1940/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8698\n",
            "Epoch 1941/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2780 - accuracy: 0.8674\n",
            "Epoch 1942/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.8686\n",
            "Epoch 1943/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2748 - accuracy: 0.8686\n",
            "Epoch 1944/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.8717\n",
            "Epoch 1945/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2750 - accuracy: 0.8710\n",
            "Epoch 1946/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8710\n",
            "Epoch 1947/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2696 - accuracy: 0.8771\n",
            "Epoch 1948/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8723\n",
            "Epoch 1949/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2836 - accuracy: 0.8686\n",
            "Epoch 1950/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2794 - accuracy: 0.8753\n",
            "Epoch 1951/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8680\n",
            "Epoch 1952/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8662\n",
            "Epoch 1953/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.8717\n",
            "Epoch 1954/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2738 - accuracy: 0.8729\n",
            "Epoch 1955/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8686\n",
            "Epoch 1956/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.8753\n",
            "Epoch 1957/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.8771\n",
            "Epoch 1958/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2764 - accuracy: 0.8662\n",
            "Epoch 1959/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8710\n",
            "Epoch 1960/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2715 - accuracy: 0.8820\n",
            "Epoch 1961/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8698\n",
            "Epoch 1962/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2799 - accuracy: 0.8674\n",
            "Epoch 1963/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8668\n",
            "Epoch 1964/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2789 - accuracy: 0.8601\n",
            "Epoch 1965/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8735\n",
            "Epoch 1966/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.8686\n",
            "Epoch 1967/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8753\n",
            "Epoch 1968/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.8753\n",
            "Epoch 1969/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.8783\n",
            "Epoch 1970/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.8753\n",
            "Epoch 1971/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2746 - accuracy: 0.8686\n",
            "Epoch 1972/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8765\n",
            "Epoch 1973/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8704\n",
            "Epoch 1974/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8692\n",
            "Epoch 1975/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8741\n",
            "Epoch 1976/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8601\n",
            "Epoch 1977/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8698\n",
            "Epoch 1978/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8692\n",
            "Epoch 1979/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2844 - accuracy: 0.8637\n",
            "Epoch 1980/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2782 - accuracy: 0.8753\n",
            "Epoch 1981/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8765\n",
            "Epoch 1982/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.8692\n",
            "Epoch 1983/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.8753\n",
            "Epoch 1984/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8741\n",
            "Epoch 1985/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8698\n",
            "Epoch 1986/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.8759\n",
            "Epoch 1987/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8710\n",
            "Epoch 1988/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8759\n",
            "Epoch 1989/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8686\n",
            "Epoch 1990/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8686\n",
            "Epoch 1991/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8771\n",
            "Epoch 1992/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2824 - accuracy: 0.8717\n",
            "Epoch 1993/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2707 - accuracy: 0.8832\n",
            "Epoch 1994/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2693 - accuracy: 0.8747\n",
            "Epoch 1995/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8540\n",
            "Epoch 1996/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8631\n",
            "Epoch 1997/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8698\n",
            "Epoch 1998/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2779 - accuracy: 0.8656\n",
            "Epoch 1999/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2693 - accuracy: 0.8723\n",
            "Epoch 2000/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8662\n",
            "Epoch 2001/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.8832\n",
            "Epoch 2002/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2807 - accuracy: 0.8771\n",
            "Epoch 2003/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.8777\n",
            "Epoch 2004/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2717 - accuracy: 0.8796\n",
            "Epoch 2005/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.8735\n",
            "Epoch 2006/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2725 - accuracy: 0.8808\n",
            "Epoch 2007/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8723\n",
            "Epoch 2008/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8698\n",
            "Epoch 2009/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.8747\n",
            "Epoch 2010/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.8796\n",
            "Epoch 2011/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8723\n",
            "Epoch 2012/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.8644\n",
            "Epoch 2013/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8777\n",
            "Epoch 2014/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.8759\n",
            "Epoch 2015/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2727 - accuracy: 0.8704\n",
            "Epoch 2016/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8741\n",
            "Epoch 2017/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.8790\n",
            "Epoch 2018/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2811 - accuracy: 0.8710\n",
            "Epoch 2019/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2791 - accuracy: 0.8729\n",
            "Epoch 2020/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2689 - accuracy: 0.8802\n",
            "Epoch 2021/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2746 - accuracy: 0.8662\n",
            "Epoch 2022/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8802\n",
            "Epoch 2023/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.8644\n",
            "Epoch 2024/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2862 - accuracy: 0.8631\n",
            "Epoch 2025/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.8777\n",
            "Epoch 2026/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2732 - accuracy: 0.8777\n",
            "Epoch 2027/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8692\n",
            "Epoch 2028/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2816 - accuracy: 0.8668\n",
            "Epoch 2029/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8704\n",
            "Epoch 2030/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2714 - accuracy: 0.8771\n",
            "Epoch 2031/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8717\n",
            "Epoch 2032/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8698\n",
            "Epoch 2033/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.8753\n",
            "Epoch 2034/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8710\n",
            "Epoch 2035/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8644\n",
            "Epoch 2036/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2696 - accuracy: 0.8771\n",
            "Epoch 2037/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8704\n",
            "Epoch 2038/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.8662\n",
            "Epoch 2039/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2716 - accuracy: 0.8747\n",
            "Epoch 2040/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2715 - accuracy: 0.8735\n",
            "Epoch 2041/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8796\n",
            "Epoch 2042/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.8704\n",
            "Epoch 2043/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8710\n",
            "Epoch 2044/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.8698\n",
            "Epoch 2045/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2690 - accuracy: 0.8802\n",
            "Epoch 2046/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.8759\n",
            "Epoch 2047/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.8796\n",
            "Epoch 2048/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8680\n",
            "Epoch 2049/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2803 - accuracy: 0.8631\n",
            "Epoch 2050/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.8656\n",
            "Epoch 2051/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8747\n",
            "Epoch 2052/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8710\n",
            "Epoch 2053/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.8747\n",
            "Epoch 2054/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8710\n",
            "Epoch 2055/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.8692\n",
            "Epoch 2056/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2742 - accuracy: 0.8704\n",
            "Epoch 2057/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.8777\n",
            "Epoch 2058/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8783\n",
            "Epoch 2059/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8710\n",
            "Epoch 2060/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2689 - accuracy: 0.8808\n",
            "Epoch 2061/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.8765\n",
            "Epoch 2062/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.8729\n",
            "Epoch 2063/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2771 - accuracy: 0.8686\n",
            "Epoch 2064/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2705 - accuracy: 0.8717\n",
            "Epoch 2065/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8759\n",
            "Epoch 2066/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8777\n",
            "Epoch 2067/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2688 - accuracy: 0.8759\n",
            "Epoch 2068/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.8802\n",
            "Epoch 2069/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.8814\n",
            "Epoch 2070/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.8820\n",
            "Epoch 2071/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2618 - accuracy: 0.8814\n",
            "Epoch 2072/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2834 - accuracy: 0.8656\n",
            "Epoch 2073/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8796\n",
            "Epoch 2074/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.8753\n",
            "Epoch 2075/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.8802\n",
            "Epoch 2076/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2720 - accuracy: 0.8674\n",
            "Epoch 2077/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2666 - accuracy: 0.8808\n",
            "Epoch 2078/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.8759\n",
            "Epoch 2079/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8783\n",
            "Epoch 2080/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2681 - accuracy: 0.8777\n",
            "Epoch 2081/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2688 - accuracy: 0.8765\n",
            "Epoch 2082/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.8771\n",
            "Epoch 2083/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2786 - accuracy: 0.8729\n",
            "Epoch 2084/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8631\n",
            "Epoch 2085/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2729 - accuracy: 0.8771\n",
            "Epoch 2086/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8765\n",
            "Epoch 2087/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2697 - accuracy: 0.8777\n",
            "Epoch 2088/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.8686\n",
            "Epoch 2089/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.8753\n",
            "Epoch 2090/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2693 - accuracy: 0.8832\n",
            "Epoch 2091/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2632 - accuracy: 0.8844\n",
            "Epoch 2092/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8783\n",
            "Epoch 2093/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8729\n",
            "Epoch 2094/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2745 - accuracy: 0.8710\n",
            "Epoch 2095/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2675 - accuracy: 0.8777\n",
            "Epoch 2096/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2679 - accuracy: 0.8814\n",
            "Epoch 2097/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8698\n",
            "Epoch 2098/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.8759\n",
            "Epoch 2099/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2683 - accuracy: 0.8753\n",
            "Epoch 2100/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2663 - accuracy: 0.8814\n",
            "Epoch 2101/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2707 - accuracy: 0.8850\n",
            "Epoch 2102/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2688 - accuracy: 0.8692\n",
            "Epoch 2103/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.8765\n",
            "Epoch 2104/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.8680\n",
            "Epoch 2105/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2728 - accuracy: 0.8686\n",
            "Epoch 2106/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2688 - accuracy: 0.8783\n",
            "Epoch 2107/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.8741\n",
            "Epoch 2108/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.8771\n",
            "Epoch 2109/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8723\n",
            "Epoch 2110/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8777\n",
            "Epoch 2111/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2709 - accuracy: 0.8747\n",
            "Epoch 2112/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8710\n",
            "Epoch 2113/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.8729\n",
            "Epoch 2114/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.8704\n",
            "Epoch 2115/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.8710\n",
            "Epoch 2116/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2624 - accuracy: 0.8808\n",
            "Epoch 2117/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2642 - accuracy: 0.8747\n",
            "Epoch 2118/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.8680\n",
            "Epoch 2119/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8802\n",
            "Epoch 2120/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2649 - accuracy: 0.8759\n",
            "Epoch 2121/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8820\n",
            "Epoch 2122/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2662 - accuracy: 0.8783\n",
            "Epoch 2123/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8771\n",
            "Epoch 2124/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2717 - accuracy: 0.8735\n",
            "Epoch 2125/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8802\n",
            "Epoch 2126/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2625 - accuracy: 0.8771\n",
            "Epoch 2127/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2754 - accuracy: 0.8717\n",
            "Epoch 2128/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8704\n",
            "Epoch 2129/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.8717\n",
            "Epoch 2130/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2671 - accuracy: 0.8735\n",
            "Epoch 2131/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.8777\n",
            "Epoch 2132/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2707 - accuracy: 0.8759\n",
            "Epoch 2133/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.8710\n",
            "Epoch 2134/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2661 - accuracy: 0.8771\n",
            "Epoch 2135/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.8704\n",
            "Epoch 2136/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8771\n",
            "Epoch 2137/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.8777\n",
            "Epoch 2138/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8838\n",
            "Epoch 2139/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8802\n",
            "Epoch 2140/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.8686\n",
            "Epoch 2141/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2707 - accuracy: 0.8680\n",
            "Epoch 2142/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.8723\n",
            "Epoch 2143/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2666 - accuracy: 0.8735\n",
            "Epoch 2144/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.8826\n",
            "Epoch 2145/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2688 - accuracy: 0.8747\n",
            "Epoch 2146/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8698\n",
            "Epoch 2147/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.8710\n",
            "Epoch 2148/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.8747\n",
            "Epoch 2149/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.8844\n",
            "Epoch 2150/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8747\n",
            "Epoch 2151/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8844\n",
            "Epoch 2152/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2633 - accuracy: 0.8698\n",
            "Epoch 2153/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8814\n",
            "Epoch 2154/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2831 - accuracy: 0.8619\n",
            "Epoch 2155/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.8790\n",
            "Epoch 2156/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.8802\n",
            "Epoch 2157/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2635 - accuracy: 0.8796\n",
            "Epoch 2158/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2693 - accuracy: 0.8777\n",
            "Epoch 2159/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.8820\n",
            "Epoch 2160/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2650 - accuracy: 0.8808\n",
            "Epoch 2161/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8741\n",
            "Epoch 2162/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2667 - accuracy: 0.8790\n",
            "Epoch 2163/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.8777\n",
            "Epoch 2164/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2687 - accuracy: 0.8759\n",
            "Epoch 2165/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8710\n",
            "Epoch 2166/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.8771\n",
            "Epoch 2167/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.8796\n",
            "Epoch 2168/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8863\n",
            "Epoch 2169/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2705 - accuracy: 0.8735\n",
            "Epoch 2170/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2640 - accuracy: 0.8704\n",
            "Epoch 2171/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.8832\n",
            "Epoch 2172/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2738 - accuracy: 0.8680\n",
            "Epoch 2173/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8838\n",
            "Epoch 2174/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8844\n",
            "Epoch 2175/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.8802\n",
            "Epoch 2176/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.8802\n",
            "Epoch 2177/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.8814\n",
            "Epoch 2178/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.8802\n",
            "Epoch 2179/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2622 - accuracy: 0.8838\n",
            "Epoch 2180/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8698\n",
            "Epoch 2181/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8662\n",
            "Epoch 2182/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.8783\n",
            "Epoch 2183/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.8759\n",
            "Epoch 2184/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.8771\n",
            "Epoch 2185/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8747\n",
            "Epoch 2186/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.8692\n",
            "Epoch 2187/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2650 - accuracy: 0.8796\n",
            "Epoch 2188/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2605 - accuracy: 0.8850\n",
            "Epoch 2189/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8820\n",
            "Epoch 2190/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8826\n",
            "Epoch 2191/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2593 - accuracy: 0.8783\n",
            "Epoch 2192/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2646 - accuracy: 0.8759\n",
            "Epoch 2193/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.8820\n",
            "Epoch 2194/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.8844\n",
            "Epoch 2195/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2657 - accuracy: 0.8765\n",
            "Epoch 2196/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2596 - accuracy: 0.8808\n",
            "Epoch 2197/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8808\n",
            "Epoch 2198/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.8717\n",
            "Epoch 2199/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.8856\n",
            "Epoch 2200/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8759\n",
            "Epoch 2201/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2680 - accuracy: 0.8826\n",
            "Epoch 2202/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.8771\n",
            "Epoch 2203/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.8717\n",
            "Epoch 2204/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2635 - accuracy: 0.8759\n",
            "Epoch 2205/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8692\n",
            "Epoch 2206/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8893\n",
            "Epoch 2207/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.8771\n",
            "Epoch 2208/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.8686\n",
            "Epoch 2209/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8753\n",
            "Epoch 2210/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.8808\n",
            "Epoch 2211/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8783\n",
            "Epoch 2212/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.8704\n",
            "Epoch 2213/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8759\n",
            "Epoch 2214/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8777\n",
            "Epoch 2215/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2558 - accuracy: 0.8832\n",
            "Epoch 2216/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8802\n",
            "Epoch 2217/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8723\n",
            "Epoch 2218/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.8796\n",
            "Epoch 2219/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2666 - accuracy: 0.8735\n",
            "Epoch 2220/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2673 - accuracy: 0.8802\n",
            "Epoch 2221/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2595 - accuracy: 0.8838\n",
            "Epoch 2222/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2593 - accuracy: 0.8832\n",
            "Epoch 2223/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.8832\n",
            "Epoch 2224/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8777\n",
            "Epoch 2225/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.8790\n",
            "Epoch 2226/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8765\n",
            "Epoch 2227/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8735\n",
            "Epoch 2228/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.8790\n",
            "Epoch 2229/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.8710\n",
            "Epoch 2230/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2652 - accuracy: 0.8710\n",
            "Epoch 2231/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2577 - accuracy: 0.8856\n",
            "Epoch 2232/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.8820\n",
            "Epoch 2233/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2592 - accuracy: 0.8790\n",
            "Epoch 2234/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8747\n",
            "Epoch 2235/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.8783\n",
            "Epoch 2236/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2665 - accuracy: 0.8686\n",
            "Epoch 2237/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8759\n",
            "Epoch 2238/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8783\n",
            "Epoch 2239/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2559 - accuracy: 0.8850\n",
            "Epoch 2240/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.8783\n",
            "Epoch 2241/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8814\n",
            "Epoch 2242/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2596 - accuracy: 0.8796\n",
            "Epoch 2243/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8753\n",
            "Epoch 2244/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.8826\n",
            "Epoch 2245/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.8814\n",
            "Epoch 2246/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8783\n",
            "Epoch 2247/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8820\n",
            "Epoch 2248/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.8753\n",
            "Epoch 2249/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.8710\n",
            "Epoch 2250/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.8771\n",
            "Epoch 2251/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8838\n",
            "Epoch 2252/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8765\n",
            "Epoch 2253/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2626 - accuracy: 0.8777\n",
            "Epoch 2254/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2667 - accuracy: 0.8723\n",
            "Epoch 2255/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8656\n",
            "Epoch 2256/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8765\n",
            "Epoch 2257/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8753\n",
            "Epoch 2258/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8777\n",
            "Epoch 2259/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8899\n",
            "Epoch 2260/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2592 - accuracy: 0.8759\n",
            "Epoch 2261/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8814\n",
            "Epoch 2262/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2784 - accuracy: 0.8644\n",
            "Epoch 2263/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.8723\n",
            "Epoch 2264/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.8826\n",
            "Epoch 2265/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.8814\n",
            "Epoch 2266/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8826\n",
            "Epoch 2267/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.8729\n",
            "Epoch 2268/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.8777\n",
            "Epoch 2269/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.8753\n",
            "Epoch 2270/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.8759\n",
            "Epoch 2271/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2596 - accuracy: 0.8826\n",
            "Epoch 2272/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.8796\n",
            "Epoch 2273/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8802\n",
            "Epoch 2274/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8893\n",
            "Epoch 2275/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.8875\n",
            "Epoch 2276/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.8747\n",
            "Epoch 2277/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2607 - accuracy: 0.8850\n",
            "Epoch 2278/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2583 - accuracy: 0.8777\n",
            "Epoch 2279/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2588 - accuracy: 0.8777\n",
            "Epoch 2280/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2689 - accuracy: 0.8729\n",
            "Epoch 2281/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2671 - accuracy: 0.8741\n",
            "Epoch 2282/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.8790\n",
            "Epoch 2283/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.8838\n",
            "Epoch 2284/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.8753\n",
            "Epoch 2285/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.8802\n",
            "Epoch 2286/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8759\n",
            "Epoch 2287/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2670 - accuracy: 0.8777\n",
            "Epoch 2288/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8796\n",
            "Epoch 2289/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.8753\n",
            "Epoch 2290/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8856\n",
            "Epoch 2291/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.8783\n",
            "Epoch 2292/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2576 - accuracy: 0.8771\n",
            "Epoch 2293/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2558 - accuracy: 0.8856\n",
            "Epoch 2294/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2592 - accuracy: 0.8790\n",
            "Epoch 2295/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.8771\n",
            "Epoch 2296/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8826\n",
            "Epoch 2297/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.8802\n",
            "Epoch 2298/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8881\n",
            "Epoch 2299/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2584 - accuracy: 0.8838\n",
            "Epoch 2300/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.8826\n",
            "Epoch 2301/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8802\n",
            "Epoch 2302/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2600 - accuracy: 0.8777\n",
            "Epoch 2303/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2608 - accuracy: 0.8759\n",
            "Epoch 2304/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8808\n",
            "Epoch 2305/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.8777\n",
            "Epoch 2306/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2633 - accuracy: 0.8765\n",
            "Epoch 2307/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8899\n",
            "Epoch 2308/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8826\n",
            "Epoch 2309/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2720 - accuracy: 0.8753\n",
            "Epoch 2310/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8826\n",
            "Epoch 2311/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.8619\n",
            "Epoch 2312/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8777\n",
            "Epoch 2313/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8832\n",
            "Epoch 2314/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2573 - accuracy: 0.8850\n",
            "Epoch 2315/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.8656\n",
            "Epoch 2316/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2577 - accuracy: 0.8759\n",
            "Epoch 2317/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.8698\n",
            "Epoch 2318/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.8814\n",
            "Epoch 2319/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2554 - accuracy: 0.8838\n",
            "Epoch 2320/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2534 - accuracy: 0.8826\n",
            "Epoch 2321/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8771\n",
            "Epoch 2322/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.8905\n",
            "Epoch 2323/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2612 - accuracy: 0.8832\n",
            "Epoch 2324/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8869\n",
            "Epoch 2325/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8796\n",
            "Epoch 2326/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.8820\n",
            "Epoch 2327/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.8863\n",
            "Epoch 2328/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2669 - accuracy: 0.8759\n",
            "Epoch 2329/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2561 - accuracy: 0.8838\n",
            "Epoch 2330/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8808\n",
            "Epoch 2331/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2590 - accuracy: 0.8814\n",
            "Epoch 2332/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8832\n",
            "Epoch 2333/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8863\n",
            "Epoch 2334/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8790\n",
            "Epoch 2335/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.8887\n",
            "Epoch 2336/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.8820\n",
            "Epoch 2337/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8802\n",
            "Epoch 2338/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8777\n",
            "Epoch 2339/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.8881\n",
            "Epoch 2340/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8698\n",
            "Epoch 2341/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2609 - accuracy: 0.8723\n",
            "Epoch 2342/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.8771\n",
            "Epoch 2343/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2540 - accuracy: 0.8893\n",
            "Epoch 2344/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8771\n",
            "Epoch 2345/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.8747\n",
            "Epoch 2346/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.8838\n",
            "Epoch 2347/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.8820\n",
            "Epoch 2348/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8844\n",
            "Epoch 2349/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.8832\n",
            "Epoch 2350/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.8802\n",
            "Epoch 2351/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8808\n",
            "Epoch 2352/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.8747\n",
            "Epoch 2353/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2555 - accuracy: 0.8820\n",
            "Epoch 2354/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.8790\n",
            "Epoch 2355/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2552 - accuracy: 0.8863\n",
            "Epoch 2356/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2596 - accuracy: 0.8850\n",
            "Epoch 2357/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8692\n",
            "Epoch 2358/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8796\n",
            "Epoch 2359/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2551 - accuracy: 0.8777\n",
            "Epoch 2360/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2552 - accuracy: 0.8844\n",
            "Epoch 2361/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.8802\n",
            "Epoch 2362/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2713 - accuracy: 0.8796\n",
            "Epoch 2363/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2588 - accuracy: 0.8802\n",
            "Epoch 2364/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2696 - accuracy: 0.8771\n",
            "Epoch 2365/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8844\n",
            "Epoch 2366/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8765\n",
            "Epoch 2367/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2522 - accuracy: 0.8802\n",
            "Epoch 2368/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2596 - accuracy: 0.8765\n",
            "Epoch 2369/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8802\n",
            "Epoch 2370/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8917\n",
            "Epoch 2371/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.8856\n",
            "Epoch 2372/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.8802\n",
            "Epoch 2373/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.8838\n",
            "Epoch 2374/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8820\n",
            "Epoch 2375/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.8783\n",
            "Epoch 2376/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8832\n",
            "Epoch 2377/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.8686\n",
            "Epoch 2378/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2554 - accuracy: 0.8863\n",
            "Epoch 2379/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8826\n",
            "Epoch 2380/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.8929\n",
            "Epoch 2381/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8656\n",
            "Epoch 2382/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.8741\n",
            "Epoch 2383/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8838\n",
            "Epoch 2384/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.8717\n",
            "Epoch 2385/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8790\n",
            "Epoch 2386/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.8735\n",
            "Epoch 2387/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.8826\n",
            "Epoch 2388/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.8765\n",
            "Epoch 2389/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.8875\n",
            "Epoch 2390/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.8863\n",
            "Epoch 2391/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.8814\n",
            "Epoch 2392/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8759\n",
            "Epoch 2393/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.8844\n",
            "Epoch 2394/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2554 - accuracy: 0.8814\n",
            "Epoch 2395/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8759\n",
            "Epoch 2396/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8796\n",
            "Epoch 2397/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2561 - accuracy: 0.8863\n",
            "Epoch 2398/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.8777\n",
            "Epoch 2399/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2507 - accuracy: 0.8881\n",
            "Epoch 2400/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.8814\n",
            "Epoch 2401/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2506 - accuracy: 0.8893\n",
            "Epoch 2402/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8723\n",
            "Epoch 2403/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.8814\n",
            "Epoch 2404/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.8796\n",
            "Epoch 2405/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2516 - accuracy: 0.8893\n",
            "Epoch 2406/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2646 - accuracy: 0.8808\n",
            "Epoch 2407/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2612 - accuracy: 0.8790\n",
            "Epoch 2408/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2525 - accuracy: 0.8783\n",
            "Epoch 2409/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2640 - accuracy: 0.8753\n",
            "Epoch 2410/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.8790\n",
            "Epoch 2411/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.8875\n",
            "Epoch 2412/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8814\n",
            "Epoch 2413/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8741\n",
            "Epoch 2414/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8869\n",
            "Epoch 2415/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2471 - accuracy: 0.8850\n",
            "Epoch 2416/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2552 - accuracy: 0.8863\n",
            "Epoch 2417/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2632 - accuracy: 0.8802\n",
            "Epoch 2418/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.8869\n",
            "Epoch 2419/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.8869\n",
            "Epoch 2420/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.8850\n",
            "Epoch 2421/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.8808\n",
            "Epoch 2422/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.8899\n",
            "Epoch 2423/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8826\n",
            "Epoch 2424/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.8777\n",
            "Epoch 2425/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.8826\n",
            "Epoch 2426/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2584 - accuracy: 0.8777\n",
            "Epoch 2427/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.8802\n",
            "Epoch 2428/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2563 - accuracy: 0.8802\n",
            "Epoch 2429/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.8808\n",
            "Epoch 2430/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.8747\n",
            "Epoch 2431/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.8820\n",
            "Epoch 2432/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8826\n",
            "Epoch 2433/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2535 - accuracy: 0.8850\n",
            "Epoch 2434/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.8844\n",
            "Epoch 2435/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.8832\n",
            "Epoch 2436/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2532 - accuracy: 0.8844\n",
            "Epoch 2437/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.8887\n",
            "Epoch 2438/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8832\n",
            "Epoch 2439/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.8808\n",
            "Epoch 2440/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2561 - accuracy: 0.8832\n",
            "Epoch 2441/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.8911\n",
            "Epoch 2442/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2654 - accuracy: 0.8723\n",
            "Epoch 2443/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2533 - accuracy: 0.8869\n",
            "Epoch 2444/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.8844\n",
            "Epoch 2445/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8814\n",
            "Epoch 2446/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2584 - accuracy: 0.8881\n",
            "Epoch 2447/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8820\n",
            "Epoch 2448/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2553 - accuracy: 0.8802\n",
            "Epoch 2449/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2531 - accuracy: 0.8923\n",
            "Epoch 2450/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.8863\n",
            "Epoch 2451/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.8875\n",
            "Epoch 2452/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.8838\n",
            "Epoch 2453/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.8887\n",
            "Epoch 2454/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2506 - accuracy: 0.8838\n",
            "Epoch 2455/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2600 - accuracy: 0.8838\n",
            "Epoch 2456/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2537 - accuracy: 0.8790\n",
            "Epoch 2457/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8899\n",
            "Epoch 2458/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.8826\n",
            "Epoch 2459/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2532 - accuracy: 0.8863\n",
            "Epoch 2460/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.8832\n",
            "Epoch 2461/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8771\n",
            "Epoch 2462/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8893\n",
            "Epoch 2463/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8869\n",
            "Epoch 2464/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.8850\n",
            "Epoch 2465/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.8911\n",
            "Epoch 2466/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.8905\n",
            "Epoch 2467/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2515 - accuracy: 0.8802\n",
            "Epoch 2468/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2669 - accuracy: 0.8753\n",
            "Epoch 2469/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2495 - accuracy: 0.8863\n",
            "Epoch 2470/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2469 - accuracy: 0.8826\n",
            "Epoch 2471/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2521 - accuracy: 0.8765\n",
            "Epoch 2472/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2635 - accuracy: 0.8747\n",
            "Epoch 2473/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.8735\n",
            "Epoch 2474/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8765\n",
            "Epoch 2475/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2527 - accuracy: 0.8844\n",
            "Epoch 2476/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2507 - accuracy: 0.8923\n",
            "Epoch 2477/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2632 - accuracy: 0.8814\n",
            "Epoch 2478/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8783\n",
            "Epoch 2479/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2623 - accuracy: 0.8814\n",
            "Epoch 2480/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.8838\n",
            "Epoch 2481/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.8777\n",
            "Epoch 2482/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.8820\n",
            "Epoch 2483/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.8856\n",
            "Epoch 2484/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8899\n",
            "Epoch 2485/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8856\n",
            "Epoch 2486/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.8875\n",
            "Epoch 2487/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8869\n",
            "Epoch 2488/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8863\n",
            "Epoch 2489/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2558 - accuracy: 0.8875\n",
            "Epoch 2490/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.8820\n",
            "Epoch 2491/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.8826\n",
            "Epoch 2492/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2497 - accuracy: 0.8875\n",
            "Epoch 2493/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.8856\n",
            "Epoch 2494/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2527 - accuracy: 0.8881\n",
            "Epoch 2495/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.8796\n",
            "Epoch 2496/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8850\n",
            "Epoch 2497/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2533 - accuracy: 0.8844\n",
            "Epoch 2498/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2484 - accuracy: 0.8899\n",
            "Epoch 2499/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8832\n",
            "Epoch 2500/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.8814\n",
            "Epoch 2501/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2515 - accuracy: 0.8850\n",
            "Epoch 2502/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8814\n",
            "Epoch 2503/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.8759\n",
            "Epoch 2504/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8814\n",
            "Epoch 2505/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2494 - accuracy: 0.8826\n",
            "Epoch 2506/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8869\n",
            "Epoch 2507/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2506 - accuracy: 0.8844\n",
            "Epoch 2508/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2471 - accuracy: 0.8911\n",
            "Epoch 2509/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.8856\n",
            "Epoch 2510/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.8905\n",
            "Epoch 2511/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2507 - accuracy: 0.8826\n",
            "Epoch 2512/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.8790\n",
            "Epoch 2513/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2537 - accuracy: 0.8814\n",
            "Epoch 2514/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.8887\n",
            "Epoch 2515/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2459 - accuracy: 0.8875\n",
            "Epoch 2516/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.8820\n",
            "Epoch 2517/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.8802\n",
            "Epoch 2518/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2507 - accuracy: 0.8856\n",
            "Epoch 2519/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2535 - accuracy: 0.8832\n",
            "Epoch 2520/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2495 - accuracy: 0.8844\n",
            "Epoch 2521/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.8869\n",
            "Epoch 2522/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.8777\n",
            "Epoch 2523/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2491 - accuracy: 0.8856\n",
            "Epoch 2524/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.8790\n",
            "Epoch 2525/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.8875\n",
            "Epoch 2526/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2520 - accuracy: 0.8869\n",
            "Epoch 2527/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2537 - accuracy: 0.8820\n",
            "Epoch 2528/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.8814\n",
            "Epoch 2529/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8893\n",
            "Epoch 2530/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8838\n",
            "Epoch 2531/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2520 - accuracy: 0.8899\n",
            "Epoch 2532/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.8753\n",
            "Epoch 2533/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.8850\n",
            "Epoch 2534/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2482 - accuracy: 0.8875\n",
            "Epoch 2535/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2505 - accuracy: 0.8820\n",
            "Epoch 2536/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2595 - accuracy: 0.8692\n",
            "Epoch 2537/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.8771\n",
            "Epoch 2538/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2534 - accuracy: 0.8832\n",
            "Epoch 2539/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.8832\n",
            "Epoch 2540/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.8893\n",
            "Epoch 2541/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.8838\n",
            "Epoch 2542/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2499 - accuracy: 0.8844\n",
            "Epoch 2543/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2544 - accuracy: 0.8820\n",
            "Epoch 2544/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8923\n",
            "Epoch 2545/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2515 - accuracy: 0.8826\n",
            "Epoch 2546/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.8856\n",
            "Epoch 2547/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.8777\n",
            "Epoch 2548/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.8850\n",
            "Epoch 2549/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2593 - accuracy: 0.8747\n",
            "Epoch 2550/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.8929\n",
            "Epoch 2551/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.8875\n",
            "Epoch 2552/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8905\n",
            "Epoch 2553/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.8796\n",
            "Epoch 2554/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2494 - accuracy: 0.8808\n",
            "Epoch 2555/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8887\n",
            "Epoch 2556/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.8881\n",
            "Epoch 2557/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8838\n",
            "Epoch 2558/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2525 - accuracy: 0.8802\n",
            "Epoch 2559/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2399 - accuracy: 0.8948\n",
            "Epoch 2560/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2458 - accuracy: 0.8850\n",
            "Epoch 2561/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8850\n",
            "Epoch 2562/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8954\n",
            "Epoch 2563/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8790\n",
            "Epoch 2564/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2585 - accuracy: 0.8759\n",
            "Epoch 2565/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2482 - accuracy: 0.8869\n",
            "Epoch 2566/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8802\n",
            "Epoch 2567/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.8832\n",
            "Epoch 2568/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8893\n",
            "Epoch 2569/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.8869\n",
            "Epoch 2570/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.8863\n",
            "Epoch 2571/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.8984\n",
            "Epoch 2572/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2460 - accuracy: 0.8929\n",
            "Epoch 2573/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.8887\n",
            "Epoch 2574/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2510 - accuracy: 0.8856\n",
            "Epoch 2575/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.8917\n",
            "Epoch 2576/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8832\n",
            "Epoch 2577/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.8911\n",
            "Epoch 2578/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8948\n",
            "Epoch 2579/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8911\n",
            "Epoch 2580/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8856\n",
            "Epoch 2581/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.8820\n",
            "Epoch 2582/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.8856\n",
            "Epoch 2583/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.8936\n",
            "Epoch 2584/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2466 - accuracy: 0.8917\n",
            "Epoch 2585/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.8820\n",
            "Epoch 2586/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8923\n",
            "Epoch 2587/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8875\n",
            "Epoch 2588/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8856\n",
            "Epoch 2589/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.8838\n",
            "Epoch 2590/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.8808\n",
            "Epoch 2591/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.8875\n",
            "Epoch 2592/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2559 - accuracy: 0.8826\n",
            "Epoch 2593/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.8802\n",
            "Epoch 2594/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8984\n",
            "Epoch 2595/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2405 - accuracy: 0.8929\n",
            "Epoch 2596/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.8869\n",
            "Epoch 2597/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2586 - accuracy: 0.8796\n",
            "Epoch 2598/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.8856\n",
            "Epoch 2599/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8929\n",
            "Epoch 2600/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.8948\n",
            "Epoch 2601/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.8911\n",
            "Epoch 2602/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.8887\n",
            "Epoch 2603/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2505 - accuracy: 0.8765\n",
            "Epoch 2604/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2560 - accuracy: 0.8771\n",
            "Epoch 2605/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.8869\n",
            "Epoch 2606/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8850\n",
            "Epoch 2607/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2588 - accuracy: 0.8783\n",
            "Epoch 2608/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8802\n",
            "Epoch 2609/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.8911\n",
            "Epoch 2610/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.8838\n",
            "Epoch 2611/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.8710\n",
            "Epoch 2612/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8972\n",
            "Epoch 2613/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2435 - accuracy: 0.8929\n",
            "Epoch 2614/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.8863\n",
            "Epoch 2615/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2511 - accuracy: 0.8844\n",
            "Epoch 2616/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.8917\n",
            "Epoch 2617/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8844\n",
            "Epoch 2618/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2532 - accuracy: 0.8814\n",
            "Epoch 2619/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2540 - accuracy: 0.8856\n",
            "Epoch 2620/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2449 - accuracy: 0.8917\n",
            "Epoch 2621/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2437 - accuracy: 0.8911\n",
            "Epoch 2622/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8820\n",
            "Epoch 2623/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2393 - accuracy: 0.8899\n",
            "Epoch 2624/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2502 - accuracy: 0.8808\n",
            "Epoch 2625/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8887\n",
            "Epoch 2626/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8820\n",
            "Epoch 2627/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8881\n",
            "Epoch 2628/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.8844\n",
            "Epoch 2629/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2459 - accuracy: 0.8887\n",
            "Epoch 2630/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.8856\n",
            "Epoch 2631/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8814\n",
            "Epoch 2632/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2470 - accuracy: 0.8899\n",
            "Epoch 2633/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.8881\n",
            "Epoch 2634/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2549 - accuracy: 0.8783\n",
            "Epoch 2635/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8838\n",
            "Epoch 2636/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.8905\n",
            "Epoch 2637/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2403 - accuracy: 0.8942\n",
            "Epoch 2638/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.8911\n",
            "Epoch 2639/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2413 - accuracy: 0.8923\n",
            "Epoch 2640/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2447 - accuracy: 0.8832\n",
            "Epoch 2641/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.8881\n",
            "Epoch 2642/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2483 - accuracy: 0.8826\n",
            "Epoch 2643/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2456 - accuracy: 0.8826\n",
            "Epoch 2644/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8802\n",
            "Epoch 2645/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.8929\n",
            "Epoch 2646/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2516 - accuracy: 0.8905\n",
            "Epoch 2647/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8826\n",
            "Epoch 2648/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8863\n",
            "Epoch 2649/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2517 - accuracy: 0.8905\n",
            "Epoch 2650/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.8838\n",
            "Epoch 2651/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8942\n",
            "Epoch 2652/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.8838\n",
            "Epoch 2653/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2443 - accuracy: 0.8844\n",
            "Epoch 2654/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.8850\n",
            "Epoch 2655/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.8850\n",
            "Epoch 2656/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8863\n",
            "Epoch 2657/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.8832\n",
            "Epoch 2658/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8802\n",
            "Epoch 2659/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2415 - accuracy: 0.8887\n",
            "Epoch 2660/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.8893\n",
            "Epoch 2661/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.8923\n",
            "Epoch 2662/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2449 - accuracy: 0.8929\n",
            "Epoch 2663/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.8826\n",
            "Epoch 2664/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2415 - accuracy: 0.8875\n",
            "Epoch 2665/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.8887\n",
            "Epoch 2666/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.8893\n",
            "Epoch 2667/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.8948\n",
            "Epoch 2668/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.8869\n",
            "Epoch 2669/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.8875\n",
            "Epoch 2670/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.8911\n",
            "Epoch 2671/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.8887\n",
            "Epoch 2672/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2552 - accuracy: 0.8777\n",
            "Epoch 2673/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2466 - accuracy: 0.8911\n",
            "Epoch 2674/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.8966\n",
            "Epoch 2675/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.8936\n",
            "Epoch 2676/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8820\n",
            "Epoch 2677/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.8838\n",
            "Epoch 2678/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8899\n",
            "Epoch 2679/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.8856\n",
            "Epoch 2680/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2552 - accuracy: 0.8856\n",
            "Epoch 2681/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2471 - accuracy: 0.8893\n",
            "Epoch 2682/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2495 - accuracy: 0.8887\n",
            "Epoch 2683/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.8869\n",
            "Epoch 2684/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2449 - accuracy: 0.8869\n",
            "Epoch 2685/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.8893\n",
            "Epoch 2686/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2465 - accuracy: 0.8838\n",
            "Epoch 2687/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2458 - accuracy: 0.8850\n",
            "Epoch 2688/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.8936\n",
            "Epoch 2689/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.8917\n",
            "Epoch 2690/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2605 - accuracy: 0.8790\n",
            "Epoch 2691/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.8893\n",
            "Epoch 2692/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2499 - accuracy: 0.8753\n",
            "Epoch 2693/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8850\n",
            "Epoch 2694/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8875\n",
            "Epoch 2695/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.8881\n",
            "Epoch 2696/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8832\n",
            "Epoch 2697/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2495 - accuracy: 0.8881\n",
            "Epoch 2698/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8777\n",
            "Epoch 2699/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.8838\n",
            "Epoch 2700/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2384 - accuracy: 0.8881\n",
            "Epoch 2701/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.8893\n",
            "Epoch 2702/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.8850\n",
            "Epoch 2703/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8905\n",
            "Epoch 2704/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2399 - accuracy: 0.8917\n",
            "Epoch 2705/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.8856\n",
            "Epoch 2706/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2473 - accuracy: 0.8881\n",
            "Epoch 2707/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8875\n",
            "Epoch 2708/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8838\n",
            "Epoch 2709/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2513 - accuracy: 0.8863\n",
            "Epoch 2710/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.8948\n",
            "Epoch 2711/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.8948\n",
            "Epoch 2712/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2353 - accuracy: 0.8966\n",
            "Epoch 2713/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2409 - accuracy: 0.8899\n",
            "Epoch 2714/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.8948\n",
            "Epoch 2715/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8948\n",
            "Epoch 2716/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2481 - accuracy: 0.8960\n",
            "Epoch 2717/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.8960\n",
            "Epoch 2718/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8820\n",
            "Epoch 2719/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8850\n",
            "Epoch 2720/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2525 - accuracy: 0.8790\n",
            "Epoch 2721/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8850\n",
            "Epoch 2722/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2411 - accuracy: 0.8893\n",
            "Epoch 2723/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8875\n",
            "Epoch 2724/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2449 - accuracy: 0.8899\n",
            "Epoch 2725/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8893\n",
            "Epoch 2726/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.8911\n",
            "Epoch 2727/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2367 - accuracy: 0.8942\n",
            "Epoch 2728/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.8881\n",
            "Epoch 2729/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.8850\n",
            "Epoch 2730/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.8850\n",
            "Epoch 2731/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.8911\n",
            "Epoch 2732/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.8887\n",
            "Epoch 2733/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8881\n",
            "Epoch 2734/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2469 - accuracy: 0.8796\n",
            "Epoch 2735/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2430 - accuracy: 0.8917\n",
            "Epoch 2736/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8850\n",
            "Epoch 2737/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.8881\n",
            "Epoch 2738/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.8893\n",
            "Epoch 2739/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2403 - accuracy: 0.8923\n",
            "Epoch 2740/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.8905\n",
            "Epoch 2741/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2364 - accuracy: 0.8948\n",
            "Epoch 2742/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8905\n",
            "Epoch 2743/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.8936\n",
            "Epoch 2744/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.8911\n",
            "Epoch 2745/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.8893\n",
            "Epoch 2746/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.8869\n",
            "Epoch 2747/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8936\n",
            "Epoch 2748/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2404 - accuracy: 0.8917\n",
            "Epoch 2749/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2460 - accuracy: 0.8881\n",
            "Epoch 2750/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.8936\n",
            "Epoch 2751/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.8875\n",
            "Epoch 2752/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.8802\n",
            "Epoch 2753/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.8899\n",
            "Epoch 2754/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2398 - accuracy: 0.8936\n",
            "Epoch 2755/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.8802\n",
            "Epoch 2756/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2532 - accuracy: 0.8844\n",
            "Epoch 2757/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2488 - accuracy: 0.8832\n",
            "Epoch 2758/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2449 - accuracy: 0.8887\n",
            "Epoch 2759/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.8869\n",
            "Epoch 2760/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2415 - accuracy: 0.8905\n",
            "Epoch 2761/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.8856\n",
            "Epoch 2762/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.8899\n",
            "Epoch 2763/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.8917\n",
            "Epoch 2764/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2398 - accuracy: 0.8948\n",
            "Epoch 2765/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.9002\n",
            "Epoch 2766/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8832\n",
            "Epoch 2767/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2411 - accuracy: 0.8929\n",
            "Epoch 2768/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.8881\n",
            "Epoch 2769/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.8881\n",
            "Epoch 2770/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2505 - accuracy: 0.8783\n",
            "Epoch 2771/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.8832\n",
            "Epoch 2772/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.8820\n",
            "Epoch 2773/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2437 - accuracy: 0.8832\n",
            "Epoch 2774/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2415 - accuracy: 0.8856\n",
            "Epoch 2775/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.8905\n",
            "Epoch 2776/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.8905\n",
            "Epoch 2777/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2515 - accuracy: 0.8881\n",
            "Epoch 2778/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.8923\n",
            "Epoch 2779/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.8869\n",
            "Epoch 2780/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2400 - accuracy: 0.8948\n",
            "Epoch 2781/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2356 - accuracy: 0.8887\n",
            "Epoch 2782/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2356 - accuracy: 0.9009\n",
            "Epoch 2783/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2404 - accuracy: 0.8960\n",
            "Epoch 2784/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.8881\n",
            "Epoch 2785/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.8850\n",
            "Epoch 2786/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.8863\n",
            "Epoch 2787/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.8863\n",
            "Epoch 2788/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.8863\n",
            "Epoch 2789/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.8978\n",
            "Epoch 2790/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.8863\n",
            "Epoch 2791/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2298 - accuracy: 0.8990\n",
            "Epoch 2792/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.8960\n",
            "Epoch 2793/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2499 - accuracy: 0.8869\n",
            "Epoch 2794/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8850\n",
            "Epoch 2795/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8893\n",
            "Epoch 2796/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.8984\n",
            "Epoch 2797/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2458 - accuracy: 0.8875\n",
            "Epoch 2798/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.8899\n",
            "Epoch 2799/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.8936\n",
            "Epoch 2800/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2413 - accuracy: 0.8893\n",
            "Epoch 2801/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2440 - accuracy: 0.8929\n",
            "Epoch 2802/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2426 - accuracy: 0.8863\n",
            "Epoch 2803/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.8893\n",
            "Epoch 2804/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.8850\n",
            "Epoch 2805/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2353 - accuracy: 0.8948\n",
            "Epoch 2806/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2348 - accuracy: 0.8905\n",
            "Epoch 2807/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.8942\n",
            "Epoch 2808/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.8893\n",
            "Epoch 2809/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2482 - accuracy: 0.8832\n",
            "Epoch 2810/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.8905\n",
            "Epoch 2811/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.8832\n",
            "Epoch 2812/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.8911\n",
            "Epoch 2813/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2371 - accuracy: 0.8954\n",
            "Epoch 2814/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.8905\n",
            "Epoch 2815/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.8905\n",
            "Epoch 2816/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.8887\n",
            "Epoch 2817/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2386 - accuracy: 0.8972\n",
            "Epoch 2818/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2405 - accuracy: 0.8869\n",
            "Epoch 2819/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.8905\n",
            "Epoch 2820/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.8911\n",
            "Epoch 2821/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.8942\n",
            "Epoch 2822/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2386 - accuracy: 0.8917\n",
            "Epoch 2823/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.8948\n",
            "Epoch 2824/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8899\n",
            "Epoch 2825/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8802\n",
            "Epoch 2826/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2339 - accuracy: 0.8984\n",
            "Epoch 2827/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.8972\n",
            "Epoch 2828/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8942\n",
            "Epoch 2829/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.8905\n",
            "Epoch 2830/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.8838\n",
            "Epoch 2831/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.8765\n",
            "Epoch 2832/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.8844\n",
            "Epoch 2833/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.8838\n",
            "Epoch 2834/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8869\n",
            "Epoch 2835/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.8899\n",
            "Epoch 2836/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.8905\n",
            "Epoch 2837/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.8905\n",
            "Epoch 2838/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2313 - accuracy: 0.8923\n",
            "Epoch 2839/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.8911\n",
            "Epoch 2840/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2411 - accuracy: 0.8905\n",
            "Epoch 2841/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.8893\n",
            "Epoch 2842/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.8917\n",
            "Epoch 2843/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.8917\n",
            "Epoch 2844/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2386 - accuracy: 0.8917\n",
            "Epoch 2845/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.8875\n",
            "Epoch 2846/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2349 - accuracy: 0.9015\n",
            "Epoch 2847/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2404 - accuracy: 0.8929\n",
            "Epoch 2848/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.8863\n",
            "Epoch 2849/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2346 - accuracy: 0.8990\n",
            "Epoch 2850/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.8960\n",
            "Epoch 2851/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.8905\n",
            "Epoch 2852/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.8899\n",
            "Epoch 2853/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2359 - accuracy: 0.8954\n",
            "Epoch 2854/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8869\n",
            "Epoch 2855/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.8966\n",
            "Epoch 2856/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8893\n",
            "Epoch 2857/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8948\n",
            "Epoch 2858/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.8875\n",
            "Epoch 2859/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2366 - accuracy: 0.8838\n",
            "Epoch 2860/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2380 - accuracy: 0.8936\n",
            "Epoch 2861/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2331 - accuracy: 0.8996\n",
            "Epoch 2862/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.8808\n",
            "Epoch 2863/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.8881\n",
            "Epoch 2864/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2336 - accuracy: 0.8905\n",
            "Epoch 2865/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.8905\n",
            "Epoch 2866/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.8960\n",
            "Epoch 2867/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.8869\n",
            "Epoch 2868/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2330 - accuracy: 0.8954\n",
            "Epoch 2869/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.8887\n",
            "Epoch 2870/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.8960\n",
            "Epoch 2871/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.8917\n",
            "Epoch 2872/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2482 - accuracy: 0.8838\n",
            "Epoch 2873/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2376 - accuracy: 0.8893\n",
            "Epoch 2874/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2302 - accuracy: 0.8936\n",
            "Epoch 2875/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2426 - accuracy: 0.8887\n",
            "Epoch 2876/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2339 - accuracy: 0.8923\n",
            "Epoch 2877/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.8881\n",
            "Epoch 2878/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.8960\n",
            "Epoch 2879/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2539 - accuracy: 0.8729\n",
            "Epoch 2880/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8826\n",
            "Epoch 2881/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.8966\n",
            "Epoch 2882/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.8917\n",
            "Epoch 2883/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.8899\n",
            "Epoch 2884/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.8856\n",
            "Epoch 2885/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.8856\n",
            "Epoch 2886/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.8929\n",
            "Epoch 2887/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2347 - accuracy: 0.8844\n",
            "Epoch 2888/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.8936\n",
            "Epoch 2889/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.8905\n",
            "Epoch 2890/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2293 - accuracy: 0.8972\n",
            "Epoch 2891/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.8954\n",
            "Epoch 2892/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2362 - accuracy: 0.8978\n",
            "Epoch 2893/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2354 - accuracy: 0.8972\n",
            "Epoch 2894/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.8929\n",
            "Epoch 2895/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.8893\n",
            "Epoch 2896/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.8844\n",
            "Epoch 2897/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2358 - accuracy: 0.8984\n",
            "Epoch 2898/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8954\n",
            "Epoch 2899/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9021\n",
            "Epoch 2900/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.8990\n",
            "Epoch 2901/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2393 - accuracy: 0.8917\n",
            "Epoch 2902/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2366 - accuracy: 0.8899\n",
            "Epoch 2903/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2443 - accuracy: 0.8875\n",
            "Epoch 2904/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.8972\n",
            "Epoch 2905/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.8948\n",
            "Epoch 2906/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2347 - accuracy: 0.8929\n",
            "Epoch 2907/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2460 - accuracy: 0.8850\n",
            "Epoch 2908/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.8844\n",
            "Epoch 2909/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8838\n",
            "Epoch 2910/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2343 - accuracy: 0.8929\n",
            "Epoch 2911/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.8929\n",
            "Epoch 2912/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2346 - accuracy: 0.8917\n",
            "Epoch 2913/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.8911\n",
            "Epoch 2914/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2399 - accuracy: 0.8911\n",
            "Epoch 2915/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.8960\n",
            "Epoch 2916/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.8875\n",
            "Epoch 2917/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.8899\n",
            "Epoch 2918/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.8978\n",
            "Epoch 2919/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.8869\n",
            "Epoch 2920/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.8905\n",
            "Epoch 2921/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.8936\n",
            "Epoch 2922/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2432 - accuracy: 0.8948\n",
            "Epoch 2923/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2407 - accuracy: 0.8929\n",
            "Epoch 2924/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.8899\n",
            "Epoch 2925/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2339 - accuracy: 0.8905\n",
            "Epoch 2926/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.8905\n",
            "Epoch 2927/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.8917\n",
            "Epoch 2928/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2459 - accuracy: 0.8856\n",
            "Epoch 2929/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.8936\n",
            "Epoch 2930/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.8905\n",
            "Epoch 2931/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2318 - accuracy: 0.8911\n",
            "Epoch 2932/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.8856\n",
            "Epoch 2933/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.8917\n",
            "Epoch 2934/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.8929\n",
            "Epoch 2935/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.8948\n",
            "Epoch 2936/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.8850\n",
            "Epoch 2937/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2447 - accuracy: 0.8923\n",
            "Epoch 2938/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8826\n",
            "Epoch 2939/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8899\n",
            "Epoch 2940/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2316 - accuracy: 0.8893\n",
            "Epoch 2941/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2340 - accuracy: 0.8954\n",
            "Epoch 2942/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2346 - accuracy: 0.8923\n",
            "Epoch 2943/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.8875\n",
            "Epoch 2944/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.8881\n",
            "Epoch 2945/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.8917\n",
            "Epoch 2946/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8905\n",
            "Epoch 2947/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.8936\n",
            "Epoch 2948/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2540 - accuracy: 0.8802\n",
            "Epoch 2949/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8832\n",
            "Epoch 2950/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8899\n",
            "Epoch 2951/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2303 - accuracy: 0.8936\n",
            "Epoch 2952/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.8948\n",
            "Epoch 2953/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.8814\n",
            "Epoch 2954/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.8863\n",
            "Epoch 2955/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8917\n",
            "Epoch 2956/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2342 - accuracy: 0.8905\n",
            "Epoch 2957/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2430 - accuracy: 0.8899\n",
            "Epoch 2958/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.8966\n",
            "Epoch 2959/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2358 - accuracy: 0.8966\n",
            "Epoch 2960/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2399 - accuracy: 0.8929\n",
            "Epoch 2961/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.8936\n",
            "Epoch 2962/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8875\n",
            "Epoch 2963/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.8899\n",
            "Epoch 2964/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.8814\n",
            "Epoch 2965/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.8929\n",
            "Epoch 2966/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.8936\n",
            "Epoch 2967/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.8948\n",
            "Epoch 2968/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2348 - accuracy: 0.8960\n",
            "Epoch 2969/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.8869\n",
            "Epoch 2970/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.8929\n",
            "Epoch 2971/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.8893\n",
            "Epoch 2972/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.8917\n",
            "Epoch 2973/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.8954\n",
            "Epoch 2974/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.9015\n",
            "Epoch 2975/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8905\n",
            "Epoch 2976/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2334 - accuracy: 0.8917\n",
            "Epoch 2977/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.8936\n",
            "Epoch 2978/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.8966\n",
            "Epoch 2979/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2278 - accuracy: 0.8972\n",
            "Epoch 2980/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2331 - accuracy: 0.8905\n",
            "Epoch 2981/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2386 - accuracy: 0.8869\n",
            "Epoch 2982/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8802\n",
            "Epoch 2983/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8960\n",
            "Epoch 2984/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.8942\n",
            "Epoch 2985/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.8899\n",
            "Epoch 2986/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.8954\n",
            "Epoch 2987/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2364 - accuracy: 0.8856\n",
            "Epoch 2988/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2389 - accuracy: 0.8893\n",
            "Epoch 2989/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2362 - accuracy: 0.8984\n",
            "Epoch 2990/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2376 - accuracy: 0.8917\n",
            "Epoch 2991/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2428 - accuracy: 0.8899\n",
            "Epoch 2992/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2356 - accuracy: 0.8905\n",
            "Epoch 2993/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2404 - accuracy: 0.8948\n",
            "Epoch 2994/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.8881\n",
            "Epoch 2995/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.8960\n",
            "Epoch 2996/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9021\n",
            "Epoch 2997/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.8996\n",
            "Epoch 2998/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2319 - accuracy: 0.8978\n",
            "Epoch 2999/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2244 - accuracy: 0.8960\n",
            "Epoch 3000/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.8954\n",
            "Epoch 3001/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.8929\n",
            "Epoch 3002/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.8936\n",
            "Epoch 3003/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.9027\n",
            "Epoch 3004/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.8917\n",
            "Epoch 3005/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.8881\n",
            "Epoch 3006/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8972\n",
            "Epoch 3007/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2311 - accuracy: 0.8899\n",
            "Epoch 3008/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.8923\n",
            "Epoch 3009/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2380 - accuracy: 0.8917\n",
            "Epoch 3010/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.8844\n",
            "Epoch 3011/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.8887\n",
            "Epoch 3012/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.8960\n",
            "Epoch 3013/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.9002\n",
            "Epoch 3014/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.8850\n",
            "Epoch 3015/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2298 - accuracy: 0.8954\n",
            "Epoch 3016/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2325 - accuracy: 0.8966\n",
            "Epoch 3017/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.8966\n",
            "Epoch 3018/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.8960\n",
            "Epoch 3019/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.8917\n",
            "Epoch 3020/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.8978\n",
            "Epoch 3021/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.8978\n",
            "Epoch 3022/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2377 - accuracy: 0.9002\n",
            "Epoch 3023/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.8911\n",
            "Epoch 3024/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.8899\n",
            "Epoch 3025/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.8954\n",
            "Epoch 3026/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2318 - accuracy: 0.8948\n",
            "Epoch 3027/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8960\n",
            "Epoch 3028/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.8929\n",
            "Epoch 3029/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2364 - accuracy: 0.8911\n",
            "Epoch 3030/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.8984\n",
            "Epoch 3031/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.8972\n",
            "Epoch 3032/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.8972\n",
            "Epoch 3033/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2383 - accuracy: 0.8869\n",
            "Epoch 3034/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2302 - accuracy: 0.8936\n",
            "Epoch 3035/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.8972\n",
            "Epoch 3036/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2350 - accuracy: 0.8911\n",
            "Epoch 3037/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2298 - accuracy: 0.8978\n",
            "Epoch 3038/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.8765\n",
            "Epoch 3039/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2354 - accuracy: 0.8917\n",
            "Epoch 3040/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2358 - accuracy: 0.8996\n",
            "Epoch 3041/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8850\n",
            "Epoch 3042/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.8923\n",
            "Epoch 3043/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2308 - accuracy: 0.8948\n",
            "Epoch 3044/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.8899\n",
            "Epoch 3045/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2307 - accuracy: 0.8923\n",
            "Epoch 3046/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2286 - accuracy: 0.8990\n",
            "Epoch 3047/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.8972\n",
            "Epoch 3048/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.8942\n",
            "Epoch 3049/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9015\n",
            "Epoch 3050/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.8911\n",
            "Epoch 3051/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2274 - accuracy: 0.8972\n",
            "Epoch 3052/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.8875\n",
            "Epoch 3053/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.8850\n",
            "Epoch 3054/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.8996\n",
            "Epoch 3055/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.8972\n",
            "Epoch 3056/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.8984\n",
            "Epoch 3057/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.8978\n",
            "Epoch 3058/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2282 - accuracy: 0.8966\n",
            "Epoch 3059/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.8911\n",
            "Epoch 3060/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2305 - accuracy: 0.9015\n",
            "Epoch 3061/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2306 - accuracy: 0.9015\n",
            "Epoch 3062/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2284 - accuracy: 0.8978\n",
            "Epoch 3063/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.8850\n",
            "Epoch 3064/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2278 - accuracy: 0.8942\n",
            "Epoch 3065/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2677 - accuracy: 0.8826\n",
            "Epoch 3066/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2319 - accuracy: 0.8923\n",
            "Epoch 3067/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.8923\n",
            "Epoch 3068/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9009\n",
            "Epoch 3069/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.8942\n",
            "Epoch 3070/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2325 - accuracy: 0.8996\n",
            "Epoch 3071/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.8911\n",
            "Epoch 3072/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9009\n",
            "Epoch 3073/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.8972\n",
            "Epoch 3074/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2383 - accuracy: 0.8917\n",
            "Epoch 3075/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.8942\n",
            "Epoch 3076/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9002\n",
            "Epoch 3077/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.8942\n",
            "Epoch 3078/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.8844\n",
            "Epoch 3079/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2302 - accuracy: 0.9033\n",
            "Epoch 3080/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.8887\n",
            "Epoch 3081/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.8917\n",
            "Epoch 3082/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.8942\n",
            "Epoch 3083/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.8850\n",
            "Epoch 3084/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.8954\n",
            "Epoch 3085/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2334 - accuracy: 0.8936\n",
            "Epoch 3086/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2380 - accuracy: 0.8911\n",
            "Epoch 3087/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.8942\n",
            "Epoch 3088/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.8966\n",
            "Epoch 3089/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2274 - accuracy: 0.8893\n",
            "Epoch 3090/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2351 - accuracy: 0.8917\n",
            "Epoch 3091/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2336 - accuracy: 0.8923\n",
            "Epoch 3092/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.8954\n",
            "Epoch 3093/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2312 - accuracy: 0.8899\n",
            "Epoch 3094/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2346 - accuracy: 0.8905\n",
            "Epoch 3095/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8966\n",
            "Epoch 3096/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 0.8960\n",
            "Epoch 3097/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.8954\n",
            "Epoch 3098/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.8966\n",
            "Epoch 3099/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.8942\n",
            "Epoch 3100/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.8942\n",
            "Epoch 3101/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2303 - accuracy: 0.8996\n",
            "Epoch 3102/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.8966\n",
            "Epoch 3103/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2282 - accuracy: 0.8954\n",
            "Epoch 3104/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2259 - accuracy: 0.8917\n",
            "Epoch 3105/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2349 - accuracy: 0.8887\n",
            "Epoch 3106/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.8917\n",
            "Epoch 3107/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.9021\n",
            "Epoch 3108/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2364 - accuracy: 0.8917\n",
            "Epoch 3109/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.8960\n",
            "Epoch 3110/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.8954\n",
            "Epoch 3111/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.9051\n",
            "Epoch 3112/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.8960\n",
            "Epoch 3113/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2282 - accuracy: 0.8942\n",
            "Epoch 3114/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2282 - accuracy: 0.8972\n",
            "Epoch 3115/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.8984\n",
            "Epoch 3116/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.8917\n",
            "Epoch 3117/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.8923\n",
            "Epoch 3118/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.8875\n",
            "Epoch 3119/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2303 - accuracy: 0.8972\n",
            "Epoch 3120/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2383 - accuracy: 0.8923\n",
            "Epoch 3121/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.8954\n",
            "Epoch 3122/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9015\n",
            "Epoch 3123/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2366 - accuracy: 0.8917\n",
            "Epoch 3124/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.8960\n",
            "Epoch 3125/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8942\n",
            "Epoch 3126/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.9033\n",
            "Epoch 3127/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.8936\n",
            "Epoch 3128/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.8899\n",
            "Epoch 3129/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.8960\n",
            "Epoch 3130/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.9015\n",
            "Epoch 3131/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.8923\n",
            "Epoch 3132/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.8893\n",
            "Epoch 3133/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9027\n",
            "Epoch 3134/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.8875\n",
            "Epoch 3135/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.8996\n",
            "Epoch 3136/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2338 - accuracy: 0.8923\n",
            "Epoch 3137/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2261 - accuracy: 0.8929\n",
            "Epoch 3138/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.8954\n",
            "Epoch 3139/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2277 - accuracy: 0.8990\n",
            "Epoch 3140/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.8978\n",
            "Epoch 3141/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.8893\n",
            "Epoch 3142/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.9002\n",
            "Epoch 3143/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2214 - accuracy: 0.8966\n",
            "Epoch 3144/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.8948\n",
            "Epoch 3145/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.9009\n",
            "Epoch 3146/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.8984\n",
            "Epoch 3147/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.8948\n",
            "Epoch 3148/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.9002\n",
            "Epoch 3149/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.8966\n",
            "Epoch 3150/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.8911\n",
            "Epoch 3151/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2304 - accuracy: 0.8893\n",
            "Epoch 3152/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.9009\n",
            "Epoch 3153/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.8929\n",
            "Epoch 3154/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.8984\n",
            "Epoch 3155/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.9069\n",
            "Epoch 3156/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2295 - accuracy: 0.8954\n",
            "Epoch 3157/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.9021\n",
            "Epoch 3158/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.8923\n",
            "Epoch 3159/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.8978\n",
            "Epoch 3160/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.8978\n",
            "Epoch 3161/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.8990\n",
            "Epoch 3162/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2313 - accuracy: 0.8905\n",
            "Epoch 3163/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2304 - accuracy: 0.8899\n",
            "Epoch 3164/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2293 - accuracy: 0.8984\n",
            "Epoch 3165/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2319 - accuracy: 0.8990\n",
            "Epoch 3166/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.8942\n",
            "Epoch 3167/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.8887\n",
            "Epoch 3168/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9015\n",
            "Epoch 3169/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.8936\n",
            "Epoch 3170/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.8966\n",
            "Epoch 3171/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.8923\n",
            "Epoch 3172/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.8960\n",
            "Epoch 3173/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.8936\n",
            "Epoch 3174/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.8978\n",
            "Epoch 3175/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9015\n",
            "Epoch 3176/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2225 - accuracy: 0.9009\n",
            "Epoch 3177/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.9009\n",
            "Epoch 3178/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.8990\n",
            "Epoch 3179/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2293 - accuracy: 0.8966\n",
            "Epoch 3180/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.8923\n",
            "Epoch 3181/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2415 - accuracy: 0.8942\n",
            "Epoch 3182/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.8917\n",
            "Epoch 3183/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.8978\n",
            "Epoch 3184/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2212 - accuracy: 0.8966\n",
            "Epoch 3185/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.8960\n",
            "Epoch 3186/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.9045\n",
            "Epoch 3187/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.9015\n",
            "Epoch 3188/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.8929\n",
            "Epoch 3189/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2223 - accuracy: 0.9015\n",
            "Epoch 3190/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.8978\n",
            "Epoch 3191/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.8923\n",
            "Epoch 3192/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2303 - accuracy: 0.8966\n",
            "Epoch 3193/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2307 - accuracy: 0.8893\n",
            "Epoch 3194/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2206 - accuracy: 0.9051\n",
            "Epoch 3195/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.9021\n",
            "Epoch 3196/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2269 - accuracy: 0.8948\n",
            "Epoch 3197/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2276 - accuracy: 0.8936\n",
            "Epoch 3198/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.9051\n",
            "Epoch 3199/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.8984\n",
            "Epoch 3200/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2414 - accuracy: 0.8863\n",
            "Epoch 3201/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9051\n",
            "Epoch 3202/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.8942\n",
            "Epoch 3203/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.8990\n",
            "Epoch 3204/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.8984\n",
            "Epoch 3205/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2292 - accuracy: 0.8966\n",
            "Epoch 3206/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.8966\n",
            "Epoch 3207/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9009\n",
            "Epoch 3208/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.8996\n",
            "Epoch 3209/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2316 - accuracy: 0.8929\n",
            "Epoch 3210/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.9027\n",
            "Epoch 3211/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.8936\n",
            "Epoch 3212/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2307 - accuracy: 0.8881\n",
            "Epoch 3213/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.8978\n",
            "Epoch 3214/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2312 - accuracy: 0.8978\n",
            "Epoch 3215/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.8954\n",
            "Epoch 3216/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.8966\n",
            "Epoch 3217/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2321 - accuracy: 0.8923\n",
            "Epoch 3218/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.8948\n",
            "Epoch 3219/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.8960\n",
            "Epoch 3220/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.8911\n",
            "Epoch 3221/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9009\n",
            "Epoch 3222/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.8972\n",
            "Epoch 3223/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.9002\n",
            "Epoch 3224/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.8972\n",
            "Epoch 3225/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2318 - accuracy: 0.8948\n",
            "Epoch 3226/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.8990\n",
            "Epoch 3227/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9002\n",
            "Epoch 3228/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2224 - accuracy: 0.9021\n",
            "Epoch 3229/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2227 - accuracy: 0.9057\n",
            "Epoch 3230/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.8996\n",
            "Epoch 3231/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.8972\n",
            "Epoch 3232/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.8996\n",
            "Epoch 3233/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.8972\n",
            "Epoch 3234/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.8942\n",
            "Epoch 3235/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.8875\n",
            "Epoch 3236/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.8972\n",
            "Epoch 3237/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.9033\n",
            "Epoch 3238/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.9063\n",
            "Epoch 3239/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.8960\n",
            "Epoch 3240/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.8954\n",
            "Epoch 3241/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2239 - accuracy: 0.8923\n",
            "Epoch 3242/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.9015\n",
            "Epoch 3243/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.8948\n",
            "Epoch 3244/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.9002\n",
            "Epoch 3245/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9039\n",
            "Epoch 3246/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.9009\n",
            "Epoch 3247/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.8887\n",
            "Epoch 3248/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.8917\n",
            "Epoch 3249/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.9021\n",
            "Epoch 3250/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.8978\n",
            "Epoch 3251/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2231 - accuracy: 0.9002\n",
            "Epoch 3252/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9009\n",
            "Epoch 3253/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2384 - accuracy: 0.8942\n",
            "Epoch 3254/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2302 - accuracy: 0.8942\n",
            "Epoch 3255/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.8911\n",
            "Epoch 3256/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.8996\n",
            "Epoch 3257/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2277 - accuracy: 0.8948\n",
            "Epoch 3258/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2293 - accuracy: 0.8942\n",
            "Epoch 3259/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.8966\n",
            "Epoch 3260/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2278 - accuracy: 0.8948\n",
            "Epoch 3261/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.8893\n",
            "Epoch 3262/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.8996\n",
            "Epoch 3263/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.8954\n",
            "Epoch 3264/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2335 - accuracy: 0.9027\n",
            "Epoch 3265/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2330 - accuracy: 0.8954\n",
            "Epoch 3266/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.8887\n",
            "Epoch 3267/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.8911\n",
            "Epoch 3268/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.8923\n",
            "Epoch 3269/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.8966\n",
            "Epoch 3270/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9027\n",
            "Epoch 3271/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.8966\n",
            "Epoch 3272/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.9027\n",
            "Epoch 3273/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2223 - accuracy: 0.8984\n",
            "Epoch 3274/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.8936\n",
            "Epoch 3275/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9027\n",
            "Epoch 3276/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9069\n",
            "Epoch 3277/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2336 - accuracy: 0.8917\n",
            "Epoch 3278/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.8948\n",
            "Epoch 3279/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.8911\n",
            "Epoch 3280/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.8960\n",
            "Epoch 3281/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2201 - accuracy: 0.8990\n",
            "Epoch 3282/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.8954\n",
            "Epoch 3283/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.8929\n",
            "Epoch 3284/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.8942\n",
            "Epoch 3285/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2308 - accuracy: 0.8954\n",
            "Epoch 3286/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.8936\n",
            "Epoch 3287/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8966\n",
            "Epoch 3288/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.8966\n",
            "Epoch 3289/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2226 - accuracy: 0.8917\n",
            "Epoch 3290/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2284 - accuracy: 0.8929\n",
            "Epoch 3291/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.8936\n",
            "Epoch 3292/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.8972\n",
            "Epoch 3293/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.9088\n",
            "Epoch 3294/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.8954\n",
            "Epoch 3295/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2278 - accuracy: 0.8960\n",
            "Epoch 3296/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.8917\n",
            "Epoch 3297/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8929\n",
            "Epoch 3298/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2415 - accuracy: 0.8893\n",
            "Epoch 3299/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2231 - accuracy: 0.8960\n",
            "Epoch 3300/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.8978\n",
            "Epoch 3301/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.8948\n",
            "Epoch 3302/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.9015\n",
            "Epoch 3303/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9009\n",
            "Epoch 3304/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.9009\n",
            "Epoch 3305/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.8942\n",
            "Epoch 3306/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.8996\n",
            "Epoch 3307/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9021\n",
            "Epoch 3308/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.8936\n",
            "Epoch 3309/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.8887\n",
            "Epoch 3310/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.8990\n",
            "Epoch 3311/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.9088\n",
            "Epoch 3312/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9063\n",
            "Epoch 3313/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.9021\n",
            "Epoch 3314/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2192 - accuracy: 0.8936\n",
            "Epoch 3315/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.8911\n",
            "Epoch 3316/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9002\n",
            "Epoch 3317/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9021\n",
            "Epoch 3318/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.8929\n",
            "Epoch 3319/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.8948\n",
            "Epoch 3320/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9002\n",
            "Epoch 3321/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9057\n",
            "Epoch 3322/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9045\n",
            "Epoch 3323/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.8996\n",
            "Epoch 3324/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.8984\n",
            "Epoch 3325/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2206 - accuracy: 0.9039\n",
            "Epoch 3326/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.8978\n",
            "Epoch 3327/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2244 - accuracy: 0.8954\n",
            "Epoch 3328/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.8936\n",
            "Epoch 3329/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9009\n",
            "Epoch 3330/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2278 - accuracy: 0.8893\n",
            "Epoch 3331/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.8923\n",
            "Epoch 3332/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2204 - accuracy: 0.8954\n",
            "Epoch 3333/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.8978\n",
            "Epoch 3334/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.8990\n",
            "Epoch 3335/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9009\n",
            "Epoch 3336/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.8923\n",
            "Epoch 3337/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.8905\n",
            "Epoch 3338/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.8917\n",
            "Epoch 3339/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2261 - accuracy: 0.8942\n",
            "Epoch 3340/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.8960\n",
            "Epoch 3341/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.9033\n",
            "Epoch 3342/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9039\n",
            "Epoch 3343/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9015\n",
            "Epoch 3344/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2331 - accuracy: 0.8856\n",
            "Epoch 3345/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.8984\n",
            "Epoch 3346/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2244 - accuracy: 0.9015\n",
            "Epoch 3347/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.8972\n",
            "Epoch 3348/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2204 - accuracy: 0.9009\n",
            "Epoch 3349/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.8929\n",
            "Epoch 3350/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.8875\n",
            "Epoch 3351/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.9021\n",
            "Epoch 3352/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.9027\n",
            "Epoch 3353/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.8978\n",
            "Epoch 3354/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.8954\n",
            "Epoch 3355/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.8960\n",
            "Epoch 3356/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2284 - accuracy: 0.8960\n",
            "Epoch 3357/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.8996\n",
            "Epoch 3358/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.8972\n",
            "Epoch 3359/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2468 - accuracy: 0.8893\n",
            "Epoch 3360/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.8911\n",
            "Epoch 3361/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.8996\n",
            "Epoch 3362/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.8954\n",
            "Epoch 3363/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.8966\n",
            "Epoch 3364/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.8978\n",
            "Epoch 3365/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9051\n",
            "Epoch 3366/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9027\n",
            "Epoch 3367/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.9021\n",
            "Epoch 3368/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.8966\n",
            "Epoch 3369/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2257 - accuracy: 0.8960\n",
            "Epoch 3370/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.8929\n",
            "Epoch 3371/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.8960\n",
            "Epoch 3372/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2270 - accuracy: 0.8954\n",
            "Epoch 3373/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.9051\n",
            "Epoch 3374/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9015\n",
            "Epoch 3375/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.8984\n",
            "Epoch 3376/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2126 - accuracy: 0.9057\n",
            "Epoch 3377/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2231 - accuracy: 0.8923\n",
            "Epoch 3378/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.8984\n",
            "Epoch 3379/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.8972\n",
            "Epoch 3380/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.9027\n",
            "Epoch 3381/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2192 - accuracy: 0.8954\n",
            "Epoch 3382/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2332 - accuracy: 0.8887\n",
            "Epoch 3383/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2212 - accuracy: 0.8990\n",
            "Epoch 3384/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.8948\n",
            "Epoch 3385/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.8960\n",
            "Epoch 3386/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2200 - accuracy: 0.9063\n",
            "Epoch 3387/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.8996\n",
            "Epoch 3388/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2208 - accuracy: 0.8960\n",
            "Epoch 3389/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.8996\n",
            "Epoch 3390/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.8929\n",
            "Epoch 3391/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2186 - accuracy: 0.8948\n",
            "Epoch 3392/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2197 - accuracy: 0.9015\n",
            "Epoch 3393/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9015\n",
            "Epoch 3394/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2156 - accuracy: 0.9063\n",
            "Epoch 3395/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.9039\n",
            "Epoch 3396/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9027\n",
            "Epoch 3397/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9057\n",
            "Epoch 3398/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2158 - accuracy: 0.9027\n",
            "Epoch 3399/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.8966\n",
            "Epoch 3400/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.8966\n",
            "Epoch 3401/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.8966\n",
            "Epoch 3402/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9002\n",
            "Epoch 3403/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.8984\n",
            "Epoch 3404/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2193 - accuracy: 0.9039\n",
            "Epoch 3405/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2348 - accuracy: 0.8875\n",
            "Epoch 3406/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.8984\n",
            "Epoch 3407/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2288 - accuracy: 0.8905\n",
            "Epoch 3408/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9045\n",
            "Epoch 3409/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2293 - accuracy: 0.8911\n",
            "Epoch 3410/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.8996\n",
            "Epoch 3411/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.8960\n",
            "Epoch 3412/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.8996\n",
            "Epoch 3413/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9015\n",
            "Epoch 3414/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9027\n",
            "Epoch 3415/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.8984\n",
            "Epoch 3416/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9027\n",
            "Epoch 3417/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.8990\n",
            "Epoch 3418/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9009\n",
            "Epoch 3419/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.8996\n",
            "Epoch 3420/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.9009\n",
            "Epoch 3421/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2211 - accuracy: 0.8978\n",
            "Epoch 3422/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.8996\n",
            "Epoch 3423/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9002\n",
            "Epoch 3424/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9063\n",
            "Epoch 3425/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.8936\n",
            "Epoch 3426/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9027\n",
            "Epoch 3427/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.8966\n",
            "Epoch 3428/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.8990\n",
            "Epoch 3429/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.8960\n",
            "Epoch 3430/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.8954\n",
            "Epoch 3431/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9051\n",
            "Epoch 3432/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.8984\n",
            "Epoch 3433/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.8954\n",
            "Epoch 3434/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9039\n",
            "Epoch 3435/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9069\n",
            "Epoch 3436/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.9027\n",
            "Epoch 3437/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.8948\n",
            "Epoch 3438/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.8893\n",
            "Epoch 3439/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.8923\n",
            "Epoch 3440/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2324 - accuracy: 0.8990\n",
            "Epoch 3441/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.8972\n",
            "Epoch 3442/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2197 - accuracy: 0.8996\n",
            "Epoch 3443/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.9002\n",
            "Epoch 3444/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.8996\n",
            "Epoch 3445/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.8936\n",
            "Epoch 3446/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9082\n",
            "Epoch 3447/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9015\n",
            "Epoch 3448/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.8942\n",
            "Epoch 3449/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9002\n",
            "Epoch 3450/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.8966\n",
            "Epoch 3451/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.8929\n",
            "Epoch 3452/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9021\n",
            "Epoch 3453/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.9002\n",
            "Epoch 3454/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9021\n",
            "Epoch 3455/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2158 - accuracy: 0.9009\n",
            "Epoch 3456/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.8850\n",
            "Epoch 3457/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9051\n",
            "Epoch 3458/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.8984\n",
            "Epoch 3459/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.8978\n",
            "Epoch 3460/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9021\n",
            "Epoch 3461/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.8960\n",
            "Epoch 3462/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.9021\n",
            "Epoch 3463/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9039\n",
            "Epoch 3464/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.9027\n",
            "Epoch 3465/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.9009\n",
            "Epoch 3466/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9039\n",
            "Epoch 3467/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9027\n",
            "Epoch 3468/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2216 - accuracy: 0.8984\n",
            "Epoch 3469/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.8960\n",
            "Epoch 3470/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9051\n",
            "Epoch 3471/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9033\n",
            "Epoch 3472/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9027\n",
            "Epoch 3473/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9015\n",
            "Epoch 3474/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.8905\n",
            "Epoch 3475/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.8972\n",
            "Epoch 3476/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9009\n",
            "Epoch 3477/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2196 - accuracy: 0.8972\n",
            "Epoch 3478/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2204 - accuracy: 0.8954\n",
            "Epoch 3479/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9057\n",
            "Epoch 3480/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.8960\n",
            "Epoch 3481/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9021\n",
            "Epoch 3482/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.8984\n",
            "Epoch 3483/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.8984\n",
            "Epoch 3484/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2207 - accuracy: 0.8954\n",
            "Epoch 3485/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9009\n",
            "Epoch 3486/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8948\n",
            "Epoch 3487/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9082\n",
            "Epoch 3488/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.8990\n",
            "Epoch 3489/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2189 - accuracy: 0.8966\n",
            "Epoch 3490/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9039\n",
            "Epoch 3491/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2148 - accuracy: 0.9033\n",
            "Epoch 3492/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.8960\n",
            "Epoch 3493/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9045\n",
            "Epoch 3494/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9039\n",
            "Epoch 3495/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2197 - accuracy: 0.9082\n",
            "Epoch 3496/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2215 - accuracy: 0.8905\n",
            "Epoch 3497/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9002\n",
            "Epoch 3498/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2220 - accuracy: 0.8984\n",
            "Epoch 3499/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.9021\n",
            "Epoch 3500/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.8996\n",
            "Epoch 3501/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.8984\n",
            "Epoch 3502/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 0.8984\n",
            "Epoch 3503/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.8984\n",
            "Epoch 3504/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.8893\n",
            "Epoch 3505/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9088\n",
            "Epoch 3506/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.9033\n",
            "Epoch 3507/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2101 - accuracy: 0.9069\n",
            "Epoch 3508/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.9033\n",
            "Epoch 3509/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.8978\n",
            "Epoch 3510/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9027\n",
            "Epoch 3511/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9075\n",
            "Epoch 3512/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9015\n",
            "Epoch 3513/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9027\n",
            "Epoch 3514/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2114 - accuracy: 0.9039\n",
            "Epoch 3515/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9009\n",
            "Epoch 3516/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9075\n",
            "Epoch 3517/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.9033\n",
            "Epoch 3518/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.8929\n",
            "Epoch 3519/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.9045\n",
            "Epoch 3520/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.8984\n",
            "Epoch 3521/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9051\n",
            "Epoch 3522/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.8960\n",
            "Epoch 3523/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2211 - accuracy: 0.8996\n",
            "Epoch 3524/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.9057\n",
            "Epoch 3525/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.8936\n",
            "Epoch 3526/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2160 - accuracy: 0.9039\n",
            "Epoch 3527/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2258 - accuracy: 0.8960\n",
            "Epoch 3528/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9088\n",
            "Epoch 3529/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2212 - accuracy: 0.8972\n",
            "Epoch 3530/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.8966\n",
            "Epoch 3531/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2151 - accuracy: 0.9009\n",
            "Epoch 3532/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.8996\n",
            "Epoch 3533/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.9021\n",
            "Epoch 3534/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9045\n",
            "Epoch 3535/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2114 - accuracy: 0.9057\n",
            "Epoch 3536/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9002\n",
            "Epoch 3537/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9027\n",
            "Epoch 3538/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.8954\n",
            "Epoch 3539/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.8923\n",
            "Epoch 3540/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.8984\n",
            "Epoch 3541/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2124 - accuracy: 0.9027\n",
            "Epoch 3542/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2193 - accuracy: 0.9027\n",
            "Epoch 3543/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2225 - accuracy: 0.9015\n",
            "Epoch 3544/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.8929\n",
            "Epoch 3545/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.8972\n",
            "Epoch 3546/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9088\n",
            "Epoch 3547/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2074 - accuracy: 0.9021\n",
            "Epoch 3548/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9021\n",
            "Epoch 3549/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.8984\n",
            "Epoch 3550/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2106 - accuracy: 0.9063\n",
            "Epoch 3551/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9075\n",
            "Epoch 3552/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.9021\n",
            "Epoch 3553/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9088\n",
            "Epoch 3554/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9082\n",
            "Epoch 3555/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2078 - accuracy: 0.9118\n",
            "Epoch 3556/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.9057\n",
            "Epoch 3557/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9045\n",
            "Epoch 3558/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2196 - accuracy: 0.9002\n",
            "Epoch 3559/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.8978\n",
            "Epoch 3560/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9033\n",
            "Epoch 3561/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.9082\n",
            "Epoch 3562/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2197 - accuracy: 0.8996\n",
            "Epoch 3563/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.9015\n",
            "Epoch 3564/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.8990\n",
            "Epoch 3565/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.8984\n",
            "Epoch 3566/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9051\n",
            "Epoch 3567/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2106 - accuracy: 0.9045\n",
            "Epoch 3568/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.8996\n",
            "Epoch 3569/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9033\n",
            "Epoch 3570/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9015\n",
            "Epoch 3571/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9051\n",
            "Epoch 3572/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.8972\n",
            "Epoch 3573/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9112\n",
            "Epoch 3574/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9039\n",
            "Epoch 3575/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.8948\n",
            "Epoch 3576/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9033\n",
            "Epoch 3577/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9015\n",
            "Epoch 3578/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.9051\n",
            "Epoch 3579/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9051\n",
            "Epoch 3580/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.8990\n",
            "Epoch 3581/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9112\n",
            "Epoch 3582/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2117 - accuracy: 0.9015\n",
            "Epoch 3583/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.8893\n",
            "Epoch 3584/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.8990\n",
            "Epoch 3585/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9057\n",
            "Epoch 3586/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9051\n",
            "Epoch 3587/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.8990\n",
            "Epoch 3588/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9057\n",
            "Epoch 3589/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9002\n",
            "Epoch 3590/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.8978\n",
            "Epoch 3591/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.9002\n",
            "Epoch 3592/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9045\n",
            "Epoch 3593/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2105 - accuracy: 0.9051\n",
            "Epoch 3594/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9009\n",
            "Epoch 3595/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9009\n",
            "Epoch 3596/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.8990\n",
            "Epoch 3597/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9027\n",
            "Epoch 3598/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.8972\n",
            "Epoch 3599/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2124 - accuracy: 0.9069\n",
            "Epoch 3600/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9118\n",
            "Epoch 3601/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9051\n",
            "Epoch 3602/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9015\n",
            "Epoch 3603/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9027\n",
            "Epoch 3604/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9082\n",
            "Epoch 3605/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.9039\n",
            "Epoch 3606/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2128 - accuracy: 0.9009\n",
            "Epoch 3607/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9063\n",
            "Epoch 3608/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2223 - accuracy: 0.8966\n",
            "Epoch 3609/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.8978\n",
            "Epoch 3610/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9094\n",
            "Epoch 3611/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9057\n",
            "Epoch 3612/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.9015\n",
            "Epoch 3613/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9033\n",
            "Epoch 3614/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9051\n",
            "Epoch 3615/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9051\n",
            "Epoch 3616/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.8984\n",
            "Epoch 3617/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9088\n",
            "Epoch 3618/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2177 - accuracy: 0.8972\n",
            "Epoch 3619/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9021\n",
            "Epoch 3620/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9057\n",
            "Epoch 3621/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9033\n",
            "Epoch 3622/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9063\n",
            "Epoch 3623/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.8990\n",
            "Epoch 3624/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9015\n",
            "Epoch 3625/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9082\n",
            "Epoch 3626/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.8960\n",
            "Epoch 3627/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9057\n",
            "Epoch 3628/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9094\n",
            "Epoch 3629/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9027\n",
            "Epoch 3630/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2156 - accuracy: 0.8996\n",
            "Epoch 3631/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2265 - accuracy: 0.8942\n",
            "Epoch 3632/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2224 - accuracy: 0.8942\n",
            "Epoch 3633/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.8996\n",
            "Epoch 3634/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9009\n",
            "Epoch 3635/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9057\n",
            "Epoch 3636/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2114 - accuracy: 0.9027\n",
            "Epoch 3637/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.9051\n",
            "Epoch 3638/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9015\n",
            "Epoch 3639/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9015\n",
            "Epoch 3640/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.8990\n",
            "Epoch 3641/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9033\n",
            "Epoch 3642/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2212 - accuracy: 0.9002\n",
            "Epoch 3643/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.8984\n",
            "Epoch 3644/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9002\n",
            "Epoch 3645/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9069\n",
            "Epoch 3646/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.8972\n",
            "Epoch 3647/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2094 - accuracy: 0.9045\n",
            "Epoch 3648/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9136\n",
            "Epoch 3649/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.8978\n",
            "Epoch 3650/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.8990\n",
            "Epoch 3651/4000\n",
            "52/52 [==============================] - 0s 4ms/step - loss: 0.2139 - accuracy: 0.9009\n",
            "Epoch 3652/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9069\n",
            "Epoch 3653/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9045\n",
            "Epoch 3654/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9082\n",
            "Epoch 3655/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2145 - accuracy: 0.9063\n",
            "Epoch 3656/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.8996\n",
            "Epoch 3657/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9075\n",
            "Epoch 3658/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.8966\n",
            "Epoch 3659/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2078 - accuracy: 0.9027\n",
            "Epoch 3660/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9057\n",
            "Epoch 3661/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9069\n",
            "Epoch 3662/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.9021\n",
            "Epoch 3663/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.9002\n",
            "Epoch 3664/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9045\n",
            "Epoch 3665/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.8972\n",
            "Epoch 3666/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9106\n",
            "Epoch 3667/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2108 - accuracy: 0.9033\n",
            "Epoch 3668/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9094\n",
            "Epoch 3669/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9015\n",
            "Epoch 3670/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9039\n",
            "Epoch 3671/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9002\n",
            "Epoch 3672/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2093 - accuracy: 0.9075\n",
            "Epoch 3673/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.9002\n",
            "Epoch 3674/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9039\n",
            "Epoch 3675/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2076 - accuracy: 0.9088\n",
            "Epoch 3676/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9075\n",
            "Epoch 3677/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9057\n",
            "Epoch 3678/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.8966\n",
            "Epoch 3679/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9100\n",
            "Epoch 3680/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9009\n",
            "Epoch 3681/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9051\n",
            "Epoch 3682/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9063\n",
            "Epoch 3683/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2167 - accuracy: 0.9015\n",
            "Epoch 3684/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2102 - accuracy: 0.8972\n",
            "Epoch 3685/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9021\n",
            "Epoch 3686/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2137 - accuracy: 0.9045\n",
            "Epoch 3687/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9039\n",
            "Epoch 3688/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9106\n",
            "Epoch 3689/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2160 - accuracy: 0.8990\n",
            "Epoch 3690/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2169 - accuracy: 0.8978\n",
            "Epoch 3691/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.9051\n",
            "Epoch 3692/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9142\n",
            "Epoch 3693/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9051\n",
            "Epoch 3694/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.8978\n",
            "Epoch 3695/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9069\n",
            "Epoch 3696/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.9039\n",
            "Epoch 3697/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9051\n",
            "Epoch 3698/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9069\n",
            "Epoch 3699/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9088\n",
            "Epoch 3700/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9069\n",
            "Epoch 3701/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.8942\n",
            "Epoch 3702/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9033\n",
            "Epoch 3703/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9039\n",
            "Epoch 3704/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9075\n",
            "Epoch 3705/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2224 - accuracy: 0.8948\n",
            "Epoch 3706/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2114 - accuracy: 0.9075\n",
            "Epoch 3707/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9075\n",
            "Epoch 3708/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9021\n",
            "Epoch 3709/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.9063\n",
            "Epoch 3710/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2112 - accuracy: 0.9075\n",
            "Epoch 3711/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9021\n",
            "Epoch 3712/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9009\n",
            "Epoch 3713/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.8966\n",
            "Epoch 3714/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9082\n",
            "Epoch 3715/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9063\n",
            "Epoch 3716/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.8990\n",
            "Epoch 3717/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9088\n",
            "Epoch 3718/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9015\n",
            "Epoch 3719/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.9033\n",
            "Epoch 3720/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.8972\n",
            "Epoch 3721/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.8960\n",
            "Epoch 3722/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9130\n",
            "Epoch 3723/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9155\n",
            "Epoch 3724/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9130\n",
            "Epoch 3725/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9057\n",
            "Epoch 3726/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.8942\n",
            "Epoch 3727/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.9057\n",
            "Epoch 3728/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9100\n",
            "Epoch 3729/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.9027\n",
            "Epoch 3730/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2156 - accuracy: 0.9002\n",
            "Epoch 3731/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9063\n",
            "Epoch 3732/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9112\n",
            "Epoch 3733/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.9039\n",
            "Epoch 3734/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9124\n",
            "Epoch 3735/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9082\n",
            "Epoch 3736/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2160 - accuracy: 0.8990\n",
            "Epoch 3737/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9082\n",
            "Epoch 3738/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9021\n",
            "Epoch 3739/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9106\n",
            "Epoch 3740/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9045\n",
            "Epoch 3741/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9021\n",
            "Epoch 3742/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9057\n",
            "Epoch 3743/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.9075\n",
            "Epoch 3744/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.8984\n",
            "Epoch 3745/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9106\n",
            "Epoch 3746/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9082\n",
            "Epoch 3747/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9088\n",
            "Epoch 3748/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.8929\n",
            "Epoch 3749/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9100\n",
            "Epoch 3750/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9051\n",
            "Epoch 3751/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.9063\n",
            "Epoch 3752/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2101 - accuracy: 0.9051\n",
            "Epoch 3753/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.9063\n",
            "Epoch 3754/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2009 - accuracy: 0.9069\n",
            "Epoch 3755/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9136\n",
            "Epoch 3756/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9094\n",
            "Epoch 3757/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.9075\n",
            "Epoch 3758/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9069\n",
            "Epoch 3759/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2215 - accuracy: 0.8923\n",
            "Epoch 3760/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2068 - accuracy: 0.9027\n",
            "Epoch 3761/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2118 - accuracy: 0.9015\n",
            "Epoch 3762/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9015\n",
            "Epoch 3763/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.8984\n",
            "Epoch 3764/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9051\n",
            "Epoch 3765/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9051\n",
            "Epoch 3766/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9106\n",
            "Epoch 3767/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9051\n",
            "Epoch 3768/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2076 - accuracy: 0.9021\n",
            "Epoch 3769/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.8917\n",
            "Epoch 3770/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2105 - accuracy: 0.9063\n",
            "Epoch 3771/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9118\n",
            "Epoch 3772/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.8960\n",
            "Epoch 3773/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9002\n",
            "Epoch 3774/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9094\n",
            "Epoch 3775/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2158 - accuracy: 0.8978\n",
            "Epoch 3776/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2177 - accuracy: 0.8978\n",
            "Epoch 3777/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.8978\n",
            "Epoch 3778/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9088\n",
            "Epoch 3779/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9027\n",
            "Epoch 3780/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9039\n",
            "Epoch 3781/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9045\n",
            "Epoch 3782/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2088 - accuracy: 0.9021\n",
            "Epoch 3783/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2227 - accuracy: 0.9002\n",
            "Epoch 3784/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2198 - accuracy: 0.8984\n",
            "Epoch 3785/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9063\n",
            "Epoch 3786/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2226 - accuracy: 0.9015\n",
            "Epoch 3787/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.8978\n",
            "Epoch 3788/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9033\n",
            "Epoch 3789/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9021\n",
            "Epoch 3790/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.8990\n",
            "Epoch 3791/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9045\n",
            "Epoch 3792/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9069\n",
            "Epoch 3793/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.9088\n",
            "Epoch 3794/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2074 - accuracy: 0.9021\n",
            "Epoch 3795/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9075\n",
            "Epoch 3796/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9051\n",
            "Epoch 3797/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2078 - accuracy: 0.9015\n",
            "Epoch 3798/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.9124\n",
            "Epoch 3799/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.8966\n",
            "Epoch 3800/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2002 - accuracy: 0.9100\n",
            "Epoch 3801/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.8917\n",
            "Epoch 3802/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9021\n",
            "Epoch 3803/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.9027\n",
            "Epoch 3804/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2020 - accuracy: 0.9082\n",
            "Epoch 3805/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9009\n",
            "Epoch 3806/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2114 - accuracy: 0.9051\n",
            "Epoch 3807/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9118\n",
            "Epoch 3808/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9069\n",
            "Epoch 3809/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2054 - accuracy: 0.9057\n",
            "Epoch 3810/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9136\n",
            "Epoch 3811/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9057\n",
            "Epoch 3812/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2177 - accuracy: 0.8960\n",
            "Epoch 3813/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9057\n",
            "Epoch 3814/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.8990\n",
            "Epoch 3815/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2144 - accuracy: 0.9039\n",
            "Epoch 3816/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.9027\n",
            "Epoch 3817/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9100\n",
            "Epoch 3818/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9057\n",
            "Epoch 3819/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9082\n",
            "Epoch 3820/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9075\n",
            "Epoch 3821/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9051\n",
            "Epoch 3822/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2009 - accuracy: 0.9033\n",
            "Epoch 3823/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9075\n",
            "Epoch 3824/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2100 - accuracy: 0.9057\n",
            "Epoch 3825/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1988 - accuracy: 0.9057\n",
            "Epoch 3826/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9039\n",
            "Epoch 3827/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9051\n",
            "Epoch 3828/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2004 - accuracy: 0.9057\n",
            "Epoch 3829/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2028 - accuracy: 0.9136\n",
            "Epoch 3830/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.8960\n",
            "Epoch 3831/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2269 - accuracy: 0.8936\n",
            "Epoch 3832/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9045\n",
            "Epoch 3833/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9094\n",
            "Epoch 3834/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2033 - accuracy: 0.9063\n",
            "Epoch 3835/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9088\n",
            "Epoch 3836/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9063\n",
            "Epoch 3837/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9069\n",
            "Epoch 3838/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9027\n",
            "Epoch 3839/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9063\n",
            "Epoch 3840/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9075\n",
            "Epoch 3841/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.8990\n",
            "Epoch 3842/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9057\n",
            "Epoch 3843/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.9082\n",
            "Epoch 3844/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.9051\n",
            "Epoch 3845/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9069\n",
            "Epoch 3846/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.8996\n",
            "Epoch 3847/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.9106\n",
            "Epoch 3848/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9088\n",
            "Epoch 3849/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9063\n",
            "Epoch 3850/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9045\n",
            "Epoch 3851/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1995 - accuracy: 0.9142\n",
            "Epoch 3852/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9075\n",
            "Epoch 3853/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2094 - accuracy: 0.9027\n",
            "Epoch 3854/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9069\n",
            "Epoch 3855/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.9088\n",
            "Epoch 3856/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9106\n",
            "Epoch 3857/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2040 - accuracy: 0.9094\n",
            "Epoch 3858/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9130\n",
            "Epoch 3859/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9069\n",
            "Epoch 3860/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9063\n",
            "Epoch 3861/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9088\n",
            "Epoch 3862/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9075\n",
            "Epoch 3863/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9021\n",
            "Epoch 3864/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9051\n",
            "Epoch 3865/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9027\n",
            "Epoch 3866/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9033\n",
            "Epoch 3867/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9033\n",
            "Epoch 3868/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.9094\n",
            "Epoch 3869/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9075\n",
            "Epoch 3870/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.9142\n",
            "Epoch 3871/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9100\n",
            "Epoch 3872/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.9002\n",
            "Epoch 3873/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2109 - accuracy: 0.9051\n",
            "Epoch 3874/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9088\n",
            "Epoch 3875/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2259 - accuracy: 0.8954\n",
            "Epoch 3876/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9069\n",
            "Epoch 3877/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9069\n",
            "Epoch 3878/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9063\n",
            "Epoch 3879/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9082\n",
            "Epoch 3880/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.8984\n",
            "Epoch 3881/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2117 - accuracy: 0.9033\n",
            "Epoch 3882/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9033\n",
            "Epoch 3883/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.9039\n",
            "Epoch 3884/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.8984\n",
            "Epoch 3885/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9075\n",
            "Epoch 3886/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2025 - accuracy: 0.9075\n",
            "Epoch 3887/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9112\n",
            "Epoch 3888/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9002\n",
            "Epoch 3889/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9094\n",
            "Epoch 3890/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2068 - accuracy: 0.9063\n",
            "Epoch 3891/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9088\n",
            "Epoch 3892/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9136\n",
            "Epoch 3893/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9039\n",
            "Epoch 3894/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1967 - accuracy: 0.9161\n",
            "Epoch 3895/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.8990\n",
            "Epoch 3896/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9021\n",
            "Epoch 3897/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9082\n",
            "Epoch 3898/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9094\n",
            "Epoch 3899/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9082\n",
            "Epoch 3900/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2082 - accuracy: 0.9002\n",
            "Epoch 3901/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2038 - accuracy: 0.9051\n",
            "Epoch 3902/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2025 - accuracy: 0.9094\n",
            "Epoch 3903/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.1993 - accuracy: 0.9142\n",
            "Epoch 3904/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2036 - accuracy: 0.9075\n",
            "Epoch 3905/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2019 - accuracy: 0.9063\n",
            "Epoch 3906/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.8978\n",
            "Epoch 3907/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9063\n",
            "Epoch 3908/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9027\n",
            "Epoch 3909/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2177 - accuracy: 0.8984\n",
            "Epoch 3910/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.2083 - accuracy: 0.9094\n",
            "Epoch 3911/4000\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.1986 - accuracy: 0.9112\n",
            "Epoch 3912/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.9033\n",
            "Epoch 3913/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9045\n",
            "Epoch 3914/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9009\n",
            "Epoch 3915/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9075\n",
            "Epoch 3916/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9148\n",
            "Epoch 3917/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9100\n",
            "Epoch 3918/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9027\n",
            "Epoch 3919/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9155\n",
            "Epoch 3920/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2014 - accuracy: 0.9069\n",
            "Epoch 3921/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.8990\n",
            "Epoch 3922/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9069\n",
            "Epoch 3923/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2002 - accuracy: 0.9082\n",
            "Epoch 3924/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9051\n",
            "Epoch 3925/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.8996\n",
            "Epoch 3926/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2071 - accuracy: 0.8984\n",
            "Epoch 3927/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.9002\n",
            "Epoch 3928/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2154 - accuracy: 0.9015\n",
            "Epoch 3929/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9045\n",
            "Epoch 3930/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.9100\n",
            "Epoch 3931/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9057\n",
            "Epoch 3932/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9088\n",
            "Epoch 3933/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2006 - accuracy: 0.9112\n",
            "Epoch 3934/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1981 - accuracy: 0.9106\n",
            "Epoch 3935/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9027\n",
            "Epoch 3936/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9045\n",
            "Epoch 3937/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9009\n",
            "Epoch 3938/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9094\n",
            "Epoch 3939/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2014 - accuracy: 0.9161\n",
            "Epoch 3940/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.9069\n",
            "Epoch 3941/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9063\n",
            "Epoch 3942/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9094\n",
            "Epoch 3943/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.9136\n",
            "Epoch 3944/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.9039\n",
            "Epoch 3945/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9124\n",
            "Epoch 3946/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2101 - accuracy: 0.9082\n",
            "Epoch 3947/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9088\n",
            "Epoch 3948/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9015\n",
            "Epoch 3949/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9075\n",
            "Epoch 3950/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.8996\n",
            "Epoch 3951/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9021\n",
            "Epoch 3952/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9015\n",
            "Epoch 3953/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2222 - accuracy: 0.8923\n",
            "Epoch 3954/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9002\n",
            "Epoch 3955/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.9027\n",
            "Epoch 3956/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9045\n",
            "Epoch 3957/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9057\n",
            "Epoch 3958/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9069\n",
            "Epoch 3959/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.9063\n",
            "Epoch 3960/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9033\n",
            "Epoch 3961/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9082\n",
            "Epoch 3962/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9088\n",
            "Epoch 3963/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2145 - accuracy: 0.9021\n",
            "Epoch 3964/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.8996\n",
            "Epoch 3965/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9136\n",
            "Epoch 3966/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9045\n",
            "Epoch 3967/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9130\n",
            "Epoch 3968/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9051\n",
            "Epoch 3969/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9051\n",
            "Epoch 3970/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9148\n",
            "Epoch 3971/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2020 - accuracy: 0.9112\n",
            "Epoch 3972/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2020 - accuracy: 0.9094\n",
            "Epoch 3973/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9069\n",
            "Epoch 3974/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2048 - accuracy: 0.9069\n",
            "Epoch 3975/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9100\n",
            "Epoch 3976/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9094\n",
            "Epoch 3977/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9057\n",
            "Epoch 3978/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9094\n",
            "Epoch 3979/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.9088\n",
            "Epoch 3980/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9118\n",
            "Epoch 3981/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9130\n",
            "Epoch 3982/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9082\n",
            "Epoch 3983/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9118\n",
            "Epoch 3984/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9069\n",
            "Epoch 3985/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.8978\n",
            "Epoch 3986/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9094\n",
            "Epoch 3987/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9124\n",
            "Epoch 3988/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.1955 - accuracy: 0.9191\n",
            "Epoch 3989/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2211 - accuracy: 0.9021\n",
            "Epoch 3990/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9027\n",
            "Epoch 3991/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9094\n",
            "Epoch 3992/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9057\n",
            "Epoch 3993/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.8966\n",
            "Epoch 3994/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9027\n",
            "Epoch 3995/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9021\n",
            "Epoch 3996/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2125 - accuracy: 0.9063\n",
            "Epoch 3997/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9051\n",
            "Epoch 3998/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9075\n",
            "Epoch 3999/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9039\n",
            "Epoch 4000/4000\n",
            "52/52 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9136\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb0fd7e7cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "LbUAoGWXbFWw",
        "outputId": "9573642f-5192-4bfe-ef48-aa64ce9502cc"
      },
      "source": [
        "perm = PermutationImportance(model, random_state=1).fit(X_test,y_test)\n",
        "eli5.show_weights(perm, feature_names = X.columns.tolist())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step - loss: 0.9064 - accuracy: 0.7937\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.9311 - accuracy: 0.6966\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.0327 - accuracy: 0.6359\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.2467 - accuracy: 0.7136\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.1112 - accuracy: 0.7791\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.9850 - accuracy: 0.7063\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.1665 - accuracy: 0.8010\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.5165 - accuracy: 0.7330\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.1738 - accuracy: 0.7864\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 1.5693 - accuracy: 0.7306\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.2421 - accuracy: 0.6019\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.0086 - accuracy: 0.7112\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.1524 - accuracy: 0.7646\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.9802 - accuracy: 0.7039\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.2154 - accuracy: 0.8010\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.6065 - accuracy: 0.7136\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.8687 - accuracy: 0.7913\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.5286 - accuracy: 0.7257\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.7673 - accuracy: 0.6505\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.0896 - accuracy: 0.7354\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.1275 - accuracy: 0.7791\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.7720 - accuracy: 0.7087\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.2177 - accuracy: 0.7791\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.6082 - accuracy: 0.7233\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.9829 - accuracy: 0.7816\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.0050 - accuracy: 0.6942\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 3.6675 - accuracy: 0.6189\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.8651 - accuracy: 0.7403\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.2418 - accuracy: 0.7767\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.8632 - accuracy: 0.7354\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.3308 - accuracy: 0.7694\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.0842 - accuracy: 0.7476\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.9668 - accuracy: 0.7816\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.8890 - accuracy: 0.6917\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 4.1020 - accuracy: 0.6214\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.4667 - accuracy: 0.7257\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.1985 - accuracy: 0.7767\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 1.7799 - accuracy: 0.6966\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.9601 - accuracy: 0.8034\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 2.5640 - accuracy: 0.7354\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 0.9224 - accuracy: 0.7888\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1680\n",
              "                \n",
              "                    &plusmn; 0.0328\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 87.49%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0859\n",
              "                \n",
              "                    &plusmn; 0.0336\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 87.74%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0835\n",
              "                \n",
              "                    &plusmn; 0.0265\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 89.33%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0684\n",
              "                \n",
              "                    &plusmn; 0.0231\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 89.92%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0631\n",
              "                \n",
              "                    &plusmn; 0.0230\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 95.74%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0184\n",
              "                \n",
              "                    &plusmn; 0.0109\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.67%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0078\n",
              "                \n",
              "                    &plusmn; 0.0078\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 98.83%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0029\n",
              "                \n",
              "                    &plusmn; 0.0277\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFbZcyungL0V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2b5d3c0d-79a7-4191-de03-37e8306b39d3"
      },
      "source": [
        "eli5.show_weights(perm, feature_names = X.columns.tolist())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
              "    <thead>\n",
              "    <tr style=\"border: none;\">\n",
              "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
              "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "    </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.1680\n",
              "                \n",
              "                    &plusmn; 0.0328\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Eigenvector\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 87.49%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0859\n",
              "                \n",
              "                    &plusmn; 0.0336\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Degree centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 87.74%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0835\n",
              "                \n",
              "                    &plusmn; 0.0265\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Clustering coefficient\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 89.33%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0684\n",
              "                \n",
              "                    &plusmn; 0.0231\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Closness centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 89.92%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0631\n",
              "                \n",
              "                    &plusmn; 0.0230\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Age\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 95.74%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0184\n",
              "                \n",
              "                    &plusmn; 0.0109\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Betweeness Centrality\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 97.67%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0078\n",
              "                \n",
              "                    &plusmn; 0.0078\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Smoking\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "        <tr style=\"background-color: hsl(120, 100.00%, 98.83%); border: none;\">\n",
              "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "                0.0029\n",
              "                \n",
              "                    &plusmn; 0.0277\n",
              "                \n",
              "            </td>\n",
              "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "                Gender\n",
              "            </td>\n",
              "        </tr>\n",
              "    \n",
              "    \n",
              "    </tbody>\n",
              "</table>\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2VUC7eqgWGb"
      },
      "source": [
        "feature_important_nn = list(zip(feature_names, perm.feature_importances_))\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxj_kIT2qIdr"
      },
      "source": [
        "# SNS plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GU8wJQQ0umm"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ychwM_XbqLY6"
      },
      "source": [
        "feature_important_nn = list(zip(feature_names, perm.feature_importances_))\n",
        "feature_important_nn = pd.DataFrame(feature_important_nn,columns=['Feature','Weight'])\n",
        "feature_important_nn['Model'] = 'ANN'"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druhRjVW01OT"
      },
      "source": [
        "## LR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq85Z2aMqcXY"
      },
      "source": [
        "feature_important_lr = list(zip(feature_names, perm_LR.feature_importances_))\n",
        "feature_important_lr = pd.DataFrame(feature_important_lr,columns=['Feature','Weight'])\n",
        "feature_important_lr['Model'] = 'LR'"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwr3L8K05uH"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99BksGuRqoXI"
      },
      "source": [
        "feature_important_knn = list(zip(feature_names, perm_KNN.feature_importances_))\n",
        "feature_important_knn = pd.DataFrame(feature_important_knn,columns=['Feature','Weight'])\n",
        "feature_important_knn['Model'] = 'KNN'"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJWVw_dP1MAs"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGdDqLKy1Ozd"
      },
      "source": [
        "feature_important_svm = list(zip(feature_names, perm_SVM.feature_importances_))\n",
        "feature_important_svm = pd.DataFrame(feature_important_svm,columns=['Feature','Weight'])\n",
        "feature_important_svm['Model'] = 'SVM'"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CugWBE051TIV"
      },
      "source": [
        "## NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR8VQHiv1baH"
      },
      "source": [
        "feature_important_nb = list(zip(feature_names, perm_NB.feature_importances_))\n",
        "feature_important_nb = pd.DataFrame(feature_important_nb,columns=['Feature','Weight'])\n",
        "feature_important_nb['Model'] = 'NB'"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5glVMEI21l9n"
      },
      "source": [
        "## DT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB3rODYv1ohh"
      },
      "source": [
        "feature_important_dt = list(zip(feature_names, perm_DT.feature_importances_))\n",
        "feature_important_dt = pd.DataFrame(feature_important_dt,columns=['Feature','Weight'])\n",
        "feature_important_dt['Model'] = 'DT'"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCT14VrJ1yse"
      },
      "source": [
        "## RF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6j1SHcS11QY"
      },
      "source": [
        "feature_important_rf = list(zip(feature_names, perm_RF.feature_importances_))\n",
        "feature_important_rf = pd.DataFrame(feature_important_rf,columns=['Feature','Weight'])\n",
        "feature_important_rf['Model'] = 'RF'"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uQeHi4C2IOE"
      },
      "source": [
        "XGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQxeriW62HYr"
      },
      "source": [
        "feature_important_xgb = list(zip(feature_names, perm_xgb.feature_importances_))\n",
        "feature_important_xgb = pd.DataFrame(feature_important_xgb,columns=['Feature','Weight'])\n",
        "feature_important_xgb['Model'] = 'XGB'"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjx--Uvzq3m4"
      },
      "source": [
        "frames = [feature_important_lr, feature_important_knn, feature_important_svm, feature_important_nb, feature_important_dt, feature_important_rf, feature_important_xgb, feature_important_nn]\n",
        "result = pd.concat(frames)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "ukYcSeg7lP_M",
        "outputId": "d64ae984-f9ad-4092-c35a-2280a1bf51f2"
      },
      "source": [
        "g = sns.catplot(x=\"Weight\", y=\"Feature\", col=\"Model\",\n",
        "                 data=result,\n",
        "                kind=\"bar\", ci=None, aspect=.45, )\n",
        "(g.set_axis_labels(\"\", \"\", )\n",
        "  .set_titles(\"{col_name}\")\n",
        "  .despine(top=False, right=False, left=False, bottom=False)\n",
        "  .set(xlim=(-0.03, 0.23))\n",
        "  )\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-98-450bd2764b27>:1: FutureWarning: \n",
            "\n",
            "The `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n",
            "\n",
            "  g = sns.catplot(x=\"Weight\", y=\"Feature\", col=\"Model\",\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7fb0fd5902e0>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x500 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jfeDS7qRrtG"
      },
      "source": [
        "#AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "PoAjQ81ZYH1u",
        "outputId": "4041af9b-362b-4f57-d512-de4b188a69b6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "\n",
        "fpr, tpr, thresholds =roc_curve(y_test, y_pred)\n",
        "\n",
        "\n",
        "y_pred_prob1 = KNN.predict_proba(X_test)[:,1]\n",
        "fpr1 , tpr1, thresholds1 = roc_curve(y_test, y_pred_prob1)\n",
        "\n",
        "y_pred_prob2 = SVM.predict_proba(X_test)[:,1]\n",
        "fpr2 , tpr2, thresholds2 = roc_curve(y_test, y_pred_prob2)\n",
        "\n",
        "\n",
        "y_pred_prob3 = NB.predict_proba(X_test)[:,1]\n",
        "fpr3 , tpr3, thresholds3 = roc_curve(y_test, y_pred_prob3)\n",
        "\n",
        "y_pred_prob4 = LR.predict_proba(X_test)[:,1]\n",
        "fpr4 , tpr4, thresholds4 = roc_curve(y_test, y_pred_prob4)\n",
        "\n",
        "y_pred_prob5 = RF.predict_proba(X_test)[:,1]\n",
        "fpr5 , tpr5, thresholds5 = roc_curve(y_test, y_pred_prob5)\n",
        "\n",
        "y_pred_prob6 = DT.predict_proba(X_test)[:,1]\n",
        "fpr6 , tpr6, thresholds6 = roc_curve(y_test, y_pred_prob6)\n",
        "\n",
        "y_pred_prob7 = xgb.predict_proba(X_test)[:,1]\n",
        "fpr7 , tpr7, thresholds7 = roc_curve(y_test, y_pred_prob7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "roc_auc = auc(fpr, tpr)\n",
        "roc_auc_knn = auc(fpr1, tpr1)\n",
        "roc_auc_svm = auc(fpr2, tpr2)\n",
        "roc_auc_nb = auc(fpr3, tpr3)\n",
        "roc_auc_lr = auc(fpr4, tpr4)\n",
        "roc_auc_rf = auc(fpr5, tpr5)\n",
        "roc_auc_dt = auc(fpr6, tpr6)\n",
        "roc_auc_xgb = auc(fpr7, tpr7)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr4, tpr4, label='LR (AUC = %0.4f)' % roc_auc_lr)\n",
        "plt.plot(fpr1, tpr1, label='KNN (AUC = %0.4f)' % roc_auc_knn)\n",
        "plt.plot(fpr2, tpr2, label='SVM (AUC = %0.4f)' % roc_auc_svm)\n",
        "plt.plot(fpr3, tpr3, label='NB (AUC = %0.4f)' % roc_auc_nb)\n",
        "plt.plot(fpr6, tpr6, label='DT (AUC = %0.4f)' % roc_auc_dt)\n",
        "plt.plot(fpr5, tpr5, label='RF (AUC = %0.4f)' % roc_auc_rf)\n",
        "plt.plot(fpr7, tpr7, label='XGBoost (AUC = %0.4f)' % roc_auc_xgb)\n",
        "plt.plot(fpr, tpr, label='ANN (AUC = %0.4f)' % roc_auc)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('ROC.png', dpi=300, bbox_inches = 'tight')\n",
        "plt.show()\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhzXlnKXDUxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}